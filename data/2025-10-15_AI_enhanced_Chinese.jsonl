{"id": "2510.12277", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.12277", "abs": "https://arxiv.org/abs/2510.12277", "authors": ["Thomas Benz", "Axel Vanoni", "Michael Rogenmoser", "Luca Benini"], "title": "A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems", "comment": "6 pages, 5 figures", "summary": "With the ever-growing heterogeneity in computing systems, driven by modern\nmachine learning applications, pressure is increasing on memory systems to\nhandle arbitrary and more demanding transfers efficiently. Descriptor-based\ndirect memory access controllers (DMACs) allow such transfers to be executed by\ndecoupling memory transfers from processing units. Classical descriptor-based\nDMACs are inefficient when handling arbitrary transfers of small unit sizes.\nExcessive descriptor size and the serialized nature of processing descriptors\nemployed by the DMAC lead to large static overheads when setting up transfers.\nTo tackle this inefficiency, we propose a descriptor-based DMAC optimized to\nefficiently handle arbitrary transfers of small unit sizes. We implement a\nlightweight descriptor format in an AXI4-based DMAC. We further increase\nperformance by implementing a low-overhead speculative descriptor prefetching\nscheme without additional latency penalties in the case of a misprediction. Our\nDMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a\nKintex FPGA to evaluate its performance. Compared to an off-the-shelf\ndescriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,\nincrease bus utilization up to 2.5x in an ideal memory system with\n64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer\nflip-flops, and no block RAMs. We can extend our lead in bus utilization to\n3.6x with 64-byte-length transfers in deep memory systems. We synthesized our\nDMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44\nGHz while occupying only 49.5 kGE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\u63a7\u5236\u5668\uff08DMAC\uff09\uff0c\u9488\u5bf9\u5c0f\u5c3a\u5bf8\u4efb\u610f\u5185\u5b58\u4f20\u8f93\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63cf\u8ff0\u7b26\u683c\u5f0f\u548c\u4f4e\u5f00\u9500\u7684\u63a8\u6d4b\u6027\u63cf\u8ff0\u7b26\u9884\u53d6\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u542f\u52a8\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86\u603b\u7ebf\u5229\u7528\u7387\uff0c\u5e76\u5728FPGA\u548c12nm\u5de5\u827a\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6027\u80fd\u4e0e\u4f4e\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5bfc\u81f4\u8ba1\u7b97\u7cfb\u7edf\u5f02\u6784\u6027\u589e\u52a0\uff0c\u5185\u5b58\u7cfb\u7edf\u9700\u9ad8\u6548\u5904\u7406\u4efb\u610f\u4e14\u9ad8\u8981\u6c42\u7684\u6570\u636e\u4f20\u8f93\u3002\u4f20\u7edf\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684DMAC\u5728\u5904\u7406\u5c0f\u5355\u4f4d\u5c3a\u5bf8\u7684\u4efb\u610f\u4f20\u8f93\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5b58\u5728\u63cf\u8ff0\u7b26\u5f00\u9500\u5927\u548c\u5904\u7406\u4e32\u884c\u5316\u7684\u95ee\u9898\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\u51cf\u5c11\u5c0f\u6570\u636e\u4f20\u8f93\u65f6\u7684\u63cf\u8ff0\u7b26\u5f00\u9500\u3001\u907f\u514d\u63cf\u8ff0\u7b26\u5904\u7406\u7684\u4e32\u884c\u5316\u74f6\u9888\u3001\u5b9e\u73b0\u9ad8\u6548\u7684\u63cf\u8ff0\u7b26\u9884\u53d6\u673a\u5236\u4e14\u4e0d\u56e0\u9884\u6d4b\u9519\u8bef\u5f15\u5165\u989d\u5916\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdDMAC\u3002", "contributions": "1\uff09\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63cf\u8ff0\u7b26\u683c\u5f0f\uff1b2\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u989d\u5916\u5ef6\u8fdf\u60e9\u7f5a\u7684\u4f4e\u5f00\u9500\u63a8\u6d4b\u6027\u63cf\u8ff0\u7b26\u9884\u53d6\u65b9\u6848\uff1b3\uff09\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eAXI4\u7684\u9ad8\u6548DMAC\uff0c\u5e76\u96c6\u6210\u523064\u4f4dRISC-V SoC\u4e2d\uff1b4\uff09\u5728FPGA\u548c12nm\u5de5\u827a\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u4f18\u52bf\u3002", "results": "\u76f8\u6bd4\u5546\u7528DMAC IP\uff0c\u4f20\u8f93\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e1.66\u500d\uff0c\u7406\u60f3\u5185\u5b58\u7cfb\u7edf\u4e0b64\u5b57\u8282\u4f20\u8f93\u7684\u603b\u7ebf\u5229\u7528\u7387\u63d0\u5347\u6700\u9ad8\u8fbe2.5\u500d\uff0c\u5728\u6df1\u5b58\u50a8\u7cfb\u7edf\u4e2d\u53ef\u8fbe3.6\u500d\uff1b\u8d44\u6e90\u65b9\u9762\u51cf\u5c1111%\u67e5\u627e\u8868\u300123%\u89e6\u53d1\u5668\uff0c\u4e14\u65e0\u9700\u5757RAM\uff1b\u5728GF12LP+\u5de5\u827a\u4e0b\u8fbe\u52301.44 GHz\u4ee5\u4e0a\u4e3b\u9891\uff0c\u9762\u79ef\u4ec549.5 kGE\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316DMAC\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u5c3a\u5bf8\u4efb\u610f\u4f20\u8f93\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728\u6027\u80fd\u3001\u8d44\u6e90\u5360\u7528\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u73b0\u4ee3\u5185\u5b58\u7cfb\u7edf\u9700\u6c42\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684DMAC\u8bbe\u8ba1\u53ca\u5176\u5728\u5d4c\u5165\u5f0f\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u5df2\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5927\u5757\u6570\u636e\u4f20\u8f93\u4f18\u5316\uff0c\u800c\u5bf9\u5c0f\u5355\u4f4d\u5c3a\u5bf8\u4efb\u610f\u4f20\u8f93\u7684\u6548\u7387\u95ee\u9898\u5173\u6ce8\u4e0d\u8db3\u3002"}}
{"id": "2510.11938", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11938", "abs": "https://arxiv.org/abs/2510.11938", "authors": ["Yanying Lin", "Shijie Peng", "Chengzhi Lu", "Chengzhong Xu", "Kejiang Ye"], "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters", "comment": "EuroSys 26", "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.", "AI": {"tldr": "FlexPipe\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u65b0\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u91cd\u6784\u6d41\u6c34\u7ebf\u67b6\u6784\u6765\u5e94\u5bf9\u8bf7\u6c42\u6a21\u5f0f\u9ad8\u5ea6\u53d8\u5316\u548c\u670d\u52a1\u5668\u96c6\u7fa4\u8d44\u6e90\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8d44\u6e90\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u6d41\u6c34\u7ebf\u914d\u7f6e\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u7684\u89e3\u51b3\u65b9\u6848\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\u9ad8\u5ea6\u53ef\u53d8\u7684\u8bf7\u6c42\u6a21\u5f0f\u3001\u670d\u52a1\u5668\u96c6\u7fa4\u4e2d\u7684\u4e25\u91cd\u8d44\u6e90\u788e\u7247\u5316\uff0c\u4ee5\u53ca\u9759\u6001\u914d\u7f6e\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u8d1f\u8f7d\u53d8\u5316\u3002", "contributions": "\u63d0\u51fa\u4e86FlexPipe\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u4fdd\u6301\u8ba1\u7b97\u56fe\u7ea6\u675f\u7684\u7ec6\u7c92\u5ea6\u6a21\u578b\u5212\u5206\u3001\u5177\u6709\u7f13\u5b58\u4e00\u81f4\u6027\u7684\u8fd0\u884c\u65f6\u6d41\u6c34\u7ebf\u91cd\u6784\uff0c\u4ee5\u53ca\u611f\u77e5\u62d3\u6251\u7684\u8d44\u6e90\u5206\u914d\u673a\u5236\u3002", "results": "\u572882-GPU\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u6bd4\uff0cFlexPipe\u7684\u8d44\u6e90\u6548\u7387\u63d0\u5347\u9ad8\u8fbe8.5\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e38.3%\uff0cGPU\u9884\u7559\u9700\u6c42\u4ece\u5cf0\u503c\u768475%\u964d\u81f330%\u3002", "conclusion": "FlexPipe\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6d41\u6c34\u7ebf\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u670d\u52a1\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u8d44\u6e90\u788e\u7247\u5316\u548c\u8d1f\u8f7d\u6ce2\u52a8\u95ee\u9898\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u9759\u6001\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u56fa\u5b9a\u6a21\u578b\u5212\u5206\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u800cFlexPipe\u9996\u6b21\u5b9e\u73b0\u4e86\u8fd0\u884c\u65f6\u52a8\u6001\u91cd\u6784\u6d41\u6c34\u7ebf\u7ed3\u6784\u3002"}}
{"id": "2510.12166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12166", "abs": "https://arxiv.org/abs/2510.12166", "authors": ["Kenneth Weiss", "Thomas M. Stitt", "Daryl Hawkins", "Olga Pearce", "Stephanie Brink", "Robert N. Rieben"], "title": "Comparing Cross-Platform Performance via Node-to-Node Scaling Studies", "comment": "16 pages; accepted to the International Journal of High Performance\n  Computing Applications (IJHPCA)", "summary": "Due to the increasing diversity of high-performance computing architectures,\nresearchers and practitioners are increasingly interested in comparing a code's\nperformance and scalability across different platforms. However, there is a\nlack of available guidance on how to actually set up and analyze such\ncross-platform studies. In this paper, we contend that the natural base unit of\ncomputing for such studies is a single compute node on each platform and offer\nguidance in setting up, running, and analyzing node-to-node scaling studies. We\npropose templates for presenting scaling results of these studies and provide\nseveral case studies highlighting the benefits of this approach.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u4e0d\u540c\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e4b\u95f4\u8fdb\u884c\u4ee3\u7801\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u6bd4\u8f83\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u63d0\u51fa\u4ee5\u5355\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e3a\u57fa\u672c\u5355\u4f4d\u8fdb\u884c\u8de8\u5e73\u53f0\u6027\u80fd\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u7f6e\u3001\u8fd0\u884c\u548c\u5206\u6790\u8282\u70b9\u95f4\u6269\u5c55\u6027\u7814\u7a76\u7684\u6307\u5bfc\uff0c\u4ee5\u53ca\u7ed3\u679c\u5c55\u793a\u6a21\u677f\u548c\u5b9e\u9645\u6848\u4f8b\u3002", "motivation": "\u7531\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u65e5\u76ca\u591a\u6837\u5316\uff0c\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u5bf9\u8de8\u5e73\u53f0\u6027\u80fd\u6bd4\u8f83\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u65b9\u6cd5\u3002", "challenges": "\u5982\u4f55\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u5408\u7406\u8bbe\u7f6e\u548c\u5206\u6790\u8de8\u5e73\u53f0\u6027\u80fd\u7814\u7a76\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u5355\u4f4d\u548c\u5206\u6790\u6846\u67b6\u3002", "contributions": "\u63d0\u51fa\u4ee5\u5355\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e3a\u57fa\u672c\u5355\u4f4d\u8fdb\u884c\u8de8\u5e73\u53f0\u6bd4\u8f83\uff0c\u63d0\u4f9b\u8282\u70b9\u95f4\u6269\u5c55\u6027\u7814\u7a76\u7684\u5b9e\u65bd\u6307\u5357\u3001\u7ed3\u679c\u5c55\u793a\u6a21\u677f\u548c\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "results": "\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u8de8\u5e73\u53f0\u6027\u80fd\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u4ee5\u5355\u8282\u70b9\u4e3a\u57fa\u51c6\u7684\u8de8\u5e73\u53f0\u6027\u80fd\u7814\u7a76\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u652f\u6301\u4ee3\u7801\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u7684\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u6bd4\u8f83\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u53ef\u64cd\u4f5c\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002", "related_work": "\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u5355\u4e00\u5e73\u53f0\u5185\u7684\u6027\u80fd\u4f18\u5316\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u5e73\u53f0\u53ef\u6269\u5c55\u6027\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u548c\u6807\u51c6\u5316\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.12196", "categories": ["cs.DC", "8W10"], "pdf": "https://arxiv.org/pdf/2510.12196", "abs": "https://arxiv.org/abs/2510.12196", "authors": ["Petr Samoldekin", "Christian Schulz", "Henning Woydt"], "title": "GPU-Accelerated Algorithms for Process Mapping", "comment": null, "summary": "Process mapping asks to assign vertices of a task graph to processing\nelements of a supercomputer such that the computational workload is balanced\nwhile the communication cost is minimized. Motivated by the recent success of\nGPU-based graph partitioners, we propose two GPU-accelerated algorithms for\nthis optimization problem. The first algorithm employs hierarchical\nmultisection, which partitions the task graph alongside the hierarchy of the\nsupercomputer. The method utilizes GPU-based graph partitioners to accelerate\nthe mapping process. The second algorithm integrates process mapping directly\ninto the modern multilevel graph partitioning pipeline. Vital phases like\ncoarsening and refinement are accelerated by exploiting the parallelism of\nGPUs. In our experiments, both methods achieve speedups exceeding 300 when\ncompared to state-of-the-art CPU-based algorithms. The first algorithm has, on\naverage, about 10 percent greater communication costs and thus remains\ncompetitive to CPU algorithms. The second approach is much faster, with a\ngeometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower\nsolution quality. To our knowledge, these are the first GPU-based algorithms\nfor process mapping.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u8fdb\u7a0b\u6620\u5c04\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u4f18\u5316\u4efb\u52a1\u56fe\u7684\u5206\u914d\uff0c\u4ee5\u5e73\u8861\u8ba1\u7b97\u8d1f\u8f7d\u5e76\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u53d7\u57fa\u4e8eGPU\u7684\u56fe\u5212\u5206\u5668\u8fd1\u671f\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u52a0\u901f\u8fdb\u7a0b\u6620\u5c04\u8fc7\u7a0b\u3002", "challenges": "\u5982\u4f55\u5728\u4fdd\u6301\u901a\u4fe1\u6210\u672c\u8f83\u4f4e\u7684\u540c\u65f6\uff0c\u9ad8\u6548\u5730\u5c06\u4efb\u52a1\u56fe\u6620\u5c04\u5230\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u5904\u7406\u5355\u5143\u4e0a\uff0c\u5e76\u5145\u5206\u5229\u7528GPU\u7684\u5e76\u884c\u80fd\u529b\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684GPU\u52a0\u901f\u7b97\u6cd5\uff1a\u4e00\u79cd\u91c7\u7528\u5206\u5c42\u591a\u5206\u533a\u7b56\u7565\uff0c\u53e6\u4e00\u79cd\u5c06\u8fdb\u7a0b\u6620\u5c04\u96c6\u6210\u5230\u73b0\u4ee3\u591a\u7ea7\u56fe\u5212\u5206\u6d41\u7a0b\u4e2d\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684CPU\u7b97\u6cd5\uff0c\u4e24\u79cd\u65b9\u6cd5\u7684\u901f\u5ea6\u63d0\u5347\u8d85\u8fc7300\u500d\uff1b\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u7684\u51e0\u4f55\u5e73\u5747\u901f\u5ea6\u63d0\u5347\u4e3a77.6\uff0c\u5cf0\u503c\u8fbe598\uff0c\u4f46\u89e3\u7684\u8d28\u91cf\u7565\u6709\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u63d0\u51fa\u57fa\u4e8eGPU\u7684\u8fdb\u7a0b\u6620\u5c04\u7b97\u6cd5\uff0c\u5c3d\u7ba1\u5728\u89e3\u7684\u8d28\u91cf\u4e0a\u6709\u6743\u8861\uff0c\u4f46\u5728\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u4f18\u52bf\u3002", "related_work": "\u57fa\u4e8eGPU\u7684\u56fe\u5212\u5206\u5668\u53ca\u76f8\u5173CPU\u4e0a\u7684\u8fdb\u7a0b\u6620\u5c04\u4e0e\u56fe\u5212\u5206\u7b97\u6cd5\u3002"}}
{"id": "2510.12354", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.12354", "abs": "https://arxiv.org/abs/2510.12354", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel", "Stefan Tai"], "title": "A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures gain traction in federated environments,\norganizations are increasingly building consumer-specific data-sharing\npipelines using modular, cloud-native transformation services. Prior work has\nshown that structuring these pipelines with reusable transformation stages\nenhances both scalability and energy efficiency. However, integrating\ntraditional cloud design patterns into such pipelines poses a challenge:\npredefining and embedding patterns can compromise modularity, reduce\nreusability, and conflict with the pipelines dynamic, consumer-driven nature.\nTo address this, we introduce a Kubernetes-based tool that enables the deferred\nand non-intrusive application of selected cloud design patterns without\nrequiring changes to service source code. The tool supports automated pattern\ninjection and collects energy consumption metrics, allowing developers to make\nenergy-aware decisions while preserving the flexible, composable structure of\nreusable data-sharing pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u6e90\u7801\u7684\u524d\u63d0\u4e0b\uff0c\u5ef6\u8fdf\u4e14\u975e\u4fb5\u5165\u5f0f\u5730\u6ce8\u5165\u4e91\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u63d0\u5347\u6570\u636e\u5171\u4eab\u7ba1\u9053\u7684\u80fd\u6e90\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u3002", "motivation": "\u5728\u6570\u636e\u7f51\u683c\u67b6\u6784\u4e2d\uff0c\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u7684\u9884\u5b9a\u4e49\u548c\u5d4c\u5165\u4f1a\u7834\u574f\u6a21\u5757\u5316\u3001\u964d\u4f4e\u53ef\u91cd\u7528\u6027\uff0c\u5e76\u4e0e\u6d88\u8d39\u8005\u9a71\u52a8\u7684\u52a8\u6001\u7ba1\u9053\u7279\u6027\u51b2\u7a81\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u96c6\u6210\u65b9\u5f0f\u3002", "challenges": "\u5982\u4f55\u5728\u4e0d\u5f71\u54cd\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u96c6\u6210\u5230\u6d88\u8d39\u8005\u9a71\u52a8\u7684\u52a8\u6001\u6570\u636e\u5171\u4eab\u7ba1\u9053\u4e2d\uff0c\u5e76\u517c\u987e\u80fd\u6e90\u6548\u7387\u3002", "contributions": "1. \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u5de5\u5177\uff0c\u652f\u6301\u5ef6\u8fdf\u548c\u975e\u4fb5\u5165\u5f0f\u7684\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u6ce8\u5165\uff1b2. \u5b9e\u73b0\u65e0\u9700\u4fee\u6539\u6e90\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u6a21\u5f0f\u96c6\u6210\uff1b3. \u63d0\u4f9b\u80fd\u8017\u76d1\u63a7\u529f\u80fd\uff0c\u652f\u6301\u80fd\u6e90\u611f\u77e5\u7684\u5f00\u53d1\u51b3\u7b56\u3002", "results": "\u8be5\u5de5\u5177\u80fd\u591f\u5728\u4fdd\u6301\u6570\u636e\u5171\u4eab\u7ba1\u9053\u7075\u6d3b\u6027\u548c\u53ef\u7ec4\u5408\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u96c6\u6210\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u5e76\u6536\u96c6\u80fd\u8017\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5de5\u5177\u5728\u4e0d\u727a\u7272\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u7684\u7075\u6d3b\u96c6\u6210\uff0c\u5e76\u4e3a\u80fd\u6e90\u611f\u77e5\u7684\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "related_work": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u53ef\u91cd\u7528\u7684\u8f6c\u6362\u9636\u6bb5\u6784\u5efa\u6570\u636e\u7ba1\u9053\u53ef\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u80fd\u6e90\u6548\u7387\uff0c\u4f46\u672a\u89e3\u51b3\u4f20\u7edf\u4e91\u8bbe\u8ba1\u6a21\u5f0f\u96c6\u6210\u5bf9\u6a21\u5757\u5316\u9020\u6210\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2510.12436", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.12436", "abs": "https://arxiv.org/abs/2510.12436", "authors": ["Valentin Seitz", "Jordy Trilaksono", "Marta Garcia-Gasulla"], "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework", "comment": null, "summary": "Ensuring good performance is a key aspect in the development of codes that\ntarget HPC machines. As these codes are under active development, the necessity\nto detect performance degradation early in the development process becomes\napparent. In addition, having meaningful insight into application scaling\nbehavior tightly coupled to the development workflow is helpful. In this paper,\nwe introduce TALP-Pages, an easy-to-integrate framework that enables developers\nto get fast and in-repository feedback about their code performance using\nestablished fundamental performance and scaling factors. The framework relies\non TALP, which enables the on-the-fly collection of these metrics. Based on a\nfolder structure suited for CI which contains the files generated by TALP,\nTALP-Pages generates an HTML report with visualizations of the performance\nfactor regression as well as scaling-efficiency tables. We compare TALP-Pages\nto tracing-based tools in terms of overhead and post-processing requirements\nand find that TALP-Pages can produce the scaling-efficiency tables faster and\nunder tighter resource constraints. To showcase the ease of use and\neffectiveness of this approach, we extend the current CI setup of GENE-X with\nonly minimal changes required and showcase the ability to detect and explain a\nperformance improvement.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TALP-Pages\uff0c\u4e00\u4e2a\u6613\u4e8e\u96c6\u6210\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728HPC\u4ee3\u7801\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u5feb\u901f\u83b7\u5f97\u6027\u80fd\u548c\u6269\u5c55\u6027\u53cd\u9988\uff0c\u76f8\u6bd4\u57fa\u4e8e\u8ffd\u8e2a\u7684\u5de5\u5177\uff0c\u5176\u5177\u6709\u66f4\u4f4e\u7684\u5f00\u9500\u548c\u66f4\u5c11\u7684\u540e\u5904\u7406\u9700\u6c42\u3002", "motivation": "\u5728HPC\u4ee3\u7801\u6301\u7eed\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5c3d\u65e9\u68c0\u6d4b\u6027\u80fd\u9000\u5316\uff0c\u5e76\u5728\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7d27\u5bc6\u96c6\u6210\u6027\u80fd\u5206\u6790\uff0c\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "challenges": "\u5982\u4f55\u5728\u5f00\u53d1\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u4f4e\u5f00\u9500\u3001\u5feb\u901f\u53cd\u9988\u7684\u6027\u80fd\u76d1\u63a7\uff0c\u5e76\u751f\u6210\u6709\u610f\u4e49\u7684\u6269\u5c55\u6027\u5206\u6790\u7ed3\u679c\uff0c\u540c\u65f6\u9002\u5e94CI\u73af\u5883\u7684\u8d44\u6e90\u9650\u5236\u3002", "contributions": "\u63d0\u51fa\u4e86TALP-Pages\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4ee3\u7801\u4ed3\u5e93\u96c6\u6210\u7684\u81ea\u52a8\u5316\u6027\u80fd\u62a5\u544a\u751f\u6210\uff0c\u652f\u6301\u6027\u80fd\u56e0\u5b50\u56de\u5f52\u53ef\u89c6\u5316\u548c\u6269\u5c55\u6548\u7387\u8868\u683c\u8f93\u51fa\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728GENE-X\u9879\u76ee\u4e2d\u96c6\u6210\u7684\u7b80\u4fbf\u6027\u548c\u6709\u6548\u6027\u3002", "results": "TALP-Pages\u76f8\u6bd4\u8ffd\u8e2a\u5de5\u5177\u80fd\u66f4\u5feb\u751f\u6210\u6269\u5c55\u6548\u7387\u8868\uff0c\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u4f18\uff1b\u5728GENE-X\u7684CI\u4e2d\u6210\u529f\u96c6\u6210\uff0c\u80fd\u591f\u68c0\u6d4b\u5e76\u89e3\u91ca\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "TALP-Pages\u4e3aHPC\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u3001\u9ad8\u6548\u7684\u6027\u80fd\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u4ee3\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0c\u6709\u52a9\u4e8e\u6301\u7eed\u6027\u80fd\u4f18\u5316\u3002", "related_work": "\u57fa\u4e8e\u8ffd\u8e2a\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u4ee5\u53ca\u73b0\u6709\u7684CI\u96c6\u6210\u6027\u80fd\u76d1\u63a7\u65b9\u6cd5\u3002"}}
