<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding](https://arxiv.org/abs/2601.10953)
*Junming Zhang,Qinyan Zhang,Huajun Sun,Feiyang Gao,Sheng Hu,Rui Nie,Xiangshui Miao*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.

</details>


### [2] [RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs](https://arxiv.org/abs/2601.11057)
*Hongshi Tan,Yao Chen,Xinyu Chen,Qizhen Zhang,Cheng Chen,Weng-Fai Wong,Bingsheng He*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.

</details>


### [3] [OpenACM: An Open-Source SRAM-Based Approximate CiM Compiler](https://arxiv.org/abs/2601.11292)
*Yiqi Zhou,JunHao Ma,Xingyang Li,Yule Sheng,Yue Yuan,Yikai Wang,Bochang Wang,Yiheng Wu,Shan Shen,Wei Xing,Daying Sun,Li Li,Zhiqiang Xiao*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The rise of data-intensive AI workloads has exacerbated the ``memory wall'' bottleneck. Digital Compute-in-Memory (DCiM) using SRAM offers a scalable solution, but its vast design space makes manual design impractical, creating a need for automated compilers. A key opportunity lies in approximate computing, which leverages the error tolerance of AI applications for significant energy savings. However, existing DCiM compilers focus on exact arithmetic, failing to exploit this optimization. This paper introduces OpenACM, the first open-source, accuracy-aware compiler for SRAM-based approximate DCiM architectures. OpenACM bridges the gap between application error tolerance and hardware automation. Its key contribution is an integrated library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic), enabling designers to make fine-grained accuracy-energy trade-offs. The compiler automates the generation of the DCiM architecture, integrating a transistor-level customizable SRAM macro with variation-aware characterization into a complete, open-source physical design flow based on OpenROAD and the FreePDK45 library. This ensures full reproducibility and accessibility, removing dependencies on proprietary tools. Experimental results on representative convolutional neural networks (CNNs) demonstrate that OpenACM achieves energy savings of up to 64\% with negligible loss in application accuracy. The framework is available on \href{https://github.com/ShenShan123/OpenACM}{OpenACM:URL}

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [X-raying the arXiv: A Large-Scale Analysis of arXiv Submissions' Source Files](https://arxiv.org/abs/2601.11385)
*Giovanni Apruzzese,Aurore Fass*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: arXiv is the largest open-access repository for scientific literature. When submitting a paper, authors upload the manuscript's source files, from which the final PDF is compiled. These source files are also publicly downloadable, potentially exposing data unrelated to the published paper -- such as figures, documents, or comments -- that may unintentionally reveal confidential information or simply waste storage space. We thus ask ourselves: "What can be found within the source files of arXiv submissions?"
  We present a longitudinal analysis of ~600,000 submissions appeared on arXiv between 2015--2025. For each submission, we examine the uploaded source files to quantify and characterize data not required for producing the respective PDF. On average, 27% of the data in each submission are unnecessary, totaling >580 GB of redundant content across our dataset. Qualitative inspection reveals the presence of offensive/inappropriate text (e.g., "WTF does this mean?") and experimental details that could disclose ongoing research. We have contacted arXiv's leadership team, as well as the authors of affected papers to alert them of these issues. Finally, we propose recommendations and an automated tool to detect and analyze arXiv submissions residual data at scale, aiming to improve data hygiene in the arXiv's ecosystem.

</details>


### [5] [Indoor Neutral-Host Networks Over Shared Spectrum and Shared Infrastructure: A Comparison Study of Real-World Deployments](https://arxiv.org/abs/2601.11457)
*Joshua Roy Palathinkal,Muhammad Iqbal Rochman,Vanlin Sathya,Mehmet Yavuz,Monisha Ghosh*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Indoor high-capacity connectivity is frequently constrained by significant building penetration loss and the inherent uplink power limitations of a typical outdoor macro-cell deployment. While Mobile Network Operators (MNOs) must optimize spectrum across low-band (<1 GHz) and mid-band (1-7 GHz) frequencies, uplink performance remains disproportionately degraded due to link budget asymmetry. Neutral-host (NH) networking provides a scalable alternative by transparently offloading MNO subscribers via spectrum sharing and shared infrastructure. We present a multi-site measurement study comparing Citizens Broadband Radio Service (CBRS)-enabled NH networks against public MNO 4G/5G macro deployments and Wi-Fi. Our results show: (i) significant building penetration loss with up to 15.5 dB in low-bands and 17.9 dB in mid-bands, resulting in a ~10 dB RSRP deficit for MNO mid-bands compared to low-bands; (ii) NH networks provide a 30 dB higher median indoor RSRP with indoor NH normalized downlink throughput matches MNO outdoor performance, while its uplink performance exceeds MNO levels in both indoor and outdoor settings; (iii) NH proximity enables superior uplink efficiency, utilizing 64-QAM for 56% of transmissions (versus <6% for MNOs) and reducing median UE transmit power by 5 dB; (iv) MNOs rely on low-band spectrum for indoor uplink transmissions, while the NH deployment maintains high-performance mid-band connectivity; and (v) NH outperforms MNOs in end-to-end throughput but trails Wi-Fi in uplink throughput and latency due to packet routing overhead to the MNO core.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [7] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [8] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo SÃ©rgio Almeida*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>
