<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis](https://arxiv.org/abs/2602.10246)
*Mayur Akewar,Sandeep Madireddy,Dongsheng Luo,Janki Bhimani*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL

</details>


### [2] [Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A](https://arxiv.org/abs/2602.10262)
*Aaron Jarmusch,Connor Vitz,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.

</details>


### [3] [Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores](https://arxiv.org/abs/2602.10378)
*Elliot L. Epstein,Rajat Vadiraj Dwaraknath,John Winnicki*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.

</details>


### [4] [Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems](https://arxiv.org/abs/2602.10486)
*Vijay K. Garg,Rohan Garg*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

</details>


### [5] [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)
*Youhe Jiang,Fangcheng Fu,Eiko Yoneki*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.

</details>


### [6] [Fine-Tuning GPT-5 for GPU Kernel Generation](https://arxiv.org/abs/2602.11000)
*Ali Tehrani,Yahya Emara,Essam Wissam,Wojciech Paluch,Waleed Atallah,Łukasz Dudziak,Mohamed S. Abdelfattah*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.

</details>


### [7] [Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots](https://arxiv.org/abs/2602.11125)
*Animesh Maiti,Abhinav Chakraborty,Bibhuti Das,Subhash Bhagat,Krishnendu Mukhopadhyaya*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [Bring Your Own Objective: Inter-operability of Network Objectives in Datacenters](https://arxiv.org/abs/2602.10252)
*Sanjoli Narang,Anup Agarwal,Venkat Arun,Manya Ghobadi*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Datacenter networks are currently locked in a "tyranny of the single objective". While modern workloads demand diverse performance goals, ranging from coflow completion times, per-flow fairness, short-flow latencies, existing fabrics are typically hardcoded for a single metric. This rigid coupling ensures peak performance when application and network objectives align, but results in abysmal performance when they diverge.
  We propose DMart, a decentralized scheduling framework that treats network bandwidth as a competitive marketplace. In DMart, applications independently encode the urgency and importance of their network traffic into autonomous bids, allowing diverse objectives to co-exist natively on the same fabric. To meet the extreme scale and sub-microsecond requirements of modern datacenters, DMart implements distributed, per-link, per-RTT auctions, without relying on ILPs, centralized schedulers, or complex priority queues.
  We evaluate DMart using packet-level simulations and compare it against network schedulers designed for individual metrics, e.g., pFabric and Sincronia. DMart matches the performance of specialized schedulers on their own "home turf" while simultaneously optimizing secondary metrics. Compared to pFabric and Sincronia, DMart reduces deadline misses by 2x and coflow completion times by 1.6x respectively, while matching pFabric short-flow completion times.

</details>


### [9] [To Reconfigure or Not to Reconfigure: Optimizing All-to-All Collectives in Circuit-Switched Photonic Interconnects](https://arxiv.org/abs/2602.10468)
*Anchengcheng Zhou,Vamsi Addanki,Maria Apostolaki*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: All-to-all collective communication is a core primitive in distributed machine learning and high-performance computing. At the server scale, the communication demands of these workloads are increasingly outstripping the bandwidth and energy limits of electrical interconnects, driving a growing interest in photonic interconnects. However, leveraging these interconnects for all-to-all communication is nontrivial. The core challenge lies in jointly optimizing a sequence of topologies and flow schedules, reconfiguring only when the transmission savings from traversing shorter paths outweigh the reconfiguration cost. Yet the search space of this joint optimization is enormous. Existing work sidesteps this challenge by making unrealistic assumptions on reconfiguration costs so that it is never or always worthwhile to reconfigure. In this paper, we show that any candidate sequence of topologies and flow schedules can be expressed as a sum of adjacency matrices and their powers. This abstraction captures the entire solution space and yields a lower bound on all-to-all completion time. Building on this formulation, we identify a family of topology sequences with strong symmetry and high expansion that admits bandwidth-efficient schedules, which our algorithm constructs with low computational overhead. Together, these insights allow us to efficiently construct near-optimal solutions, effectively avoiding enumeration of the combinatorial design space. Evaluation shows that our approach reduces all-to-all completion time by up to 44% on average across a wide range of network parameters, message sizes and workload types.

</details>


### [10] [Scaling Routers with In-Package Optics and High-Bandwidth Memories](https://arxiv.org/abs/2602.10505)
*Isaac Keslassy,Ilay Yavlovich,Jose Yallouz,Tzu-Chien Hsueh,Yeshaiahu Fainman,Bill Lin*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: This paper aims to apply two major scaling transformations from the computing packaging industry to internet routers: the heterogeneous integration of high-bandwidth memories (HBMs) and chiplets, as well as in-package optics. We propose a novel internet router architecture that employs these technologies to achieve a petabit/sec router within a single integrated package. At the top-level, we introduce a novel split-parallel switch architecture that spatially divides (without processing) the incoming fibers and distributes them across smaller independent switches without intermediate OEO conversions or fine-tuned per-packet load-balancing. This passive spatial division enables scaling at the cost of a coarser traffic load balancing. Yet, through extensive evaluations of backbone network traffic, we demonstrate that differences with fine-tuned approaches are small. In addition, we propose a novel HBM-based shared-memory architecture for the implementation of the smaller independent switches, and we introduce a novel parallel frame interleaving algorithm that packs traffic into frames so that HBM banks are accessed at peak HBM data rates in a cyclical interleaving manner. We further discuss why these new technologies represent a paradigm shift in the design of future internet routers. Finally, we emphasize that power consumption may constitute the primary bottleneck to scaling.

</details>


### [11] [SplitCom: Communication-efficient Split Federated Fine-tuning of LLMs via Temporal Compression](https://arxiv.org/abs/2602.10564)
*Tao Li,Yulin Tang,Yiyang Song,Cong Wu,Xihui Liu,Pan Li,Xianhao Chen*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Federated fine-tuning of on-device large language models (LLMs) mitigates privacy concerns by preventing raw data sharing. However, the intensive computational and memory demands pose significant challenges for resource-constrained edge devices. To overcome these limitations, split federated learning (SFL) emerges as a promising solution that partitions the model into lightweight client-side and compute-intensive server-side sub-models, thus offloading the primary training workload to a powerful server. Nevertheless, high-dimensional activation exchanges in SFL lead to excessive communication overhead. To overcome this, we propose SplitCom, a communication-efficient SFL framework for LLMs that exploits temporal redundancy in activations across consecutive training epochs. Inspired by video compression, the core innovation of our framework lies in selective activation uploading only when a noticeable deviation from previous epochs occurs. To balance communication efficiency and learning performance, we introduce two adaptive threshold control schemes based on 1) bang-bang control or 2) deep deterministic policy gradient (DDPG)-based reinforcement learning. Moreover, we implement dimensionality reduction techniques to alleviate client-side memory requirements. Furthermore, we extend SplitCom to the U-shape architecture, ensuring the server never accesses clients' labels. Extensive simulations and laboratory experiments demonstrate that SplitCom reduces uplink communication costs by up to 98.6\,\% in its standard configuration and total communication costs by up to 95.8\,\% in its U-shape variant without noticeably compromising model performance.

</details>


### [12] [Supercharging Packet-level Network Simulation of Large Model Training via Memoization and Fast-Forwarding](https://arxiv.org/abs/2602.10615)
*Fei Long,Kaihui Gao,Li Chen,Dan Li,Yiwei Zhang,Fei Gui,Yitao Xing,Wenjia Wei,Bingyang Liu*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Packet-level discrete-event simulation (PLDES) is a prevalent tool for evaluating detailed performance of large model training. Although PLDES offers high fidelity and generality, its slow performance has plagued networking practitioners. Existing optimization techniques either simplify the network model, resulting in large errors; or execute it in parallel using multiple processors, with an upper bound on speedup. This paper explores an alternative optimization direction that reduces the computational loads of PLDES while maintaining high fidelity. Our key insight is that, in distributed LLM training, packet-level traffic behaviors often exhibit repetitive contention patterns and steady-states where flow rates stabilize, ignoring these redundant discrete events speeds up the simulation considerably and the error is negligible. We realize this idea by proposing Wormhole, a user-transparent PLDES kernel capable of automatically memoization for unsteady-states and skipping for steady-states. Wormhole adopts network partitioning, state memoization and reuse, and rate-based steady-state identification to accurately determine the periods of each flow's steady-state, while maintaining simulation consistency after fast-forwarding. Experiments demonstrate that Wormhole can achieve a 744x speedup over the original ns-3 (510x for MoE workload), with a bounded error of <1%. Applying current multithreading parallel techniques and Wormhole together allows a 1012x speedup, reducing the simulation time for one GPT-13B training under 128 GPUs from 9 hours to 5 minutes.

</details>


### [13] [Security, Privacy and System-Level Resillience of 6G End-to-End System: Hexa-X-II Perspective](https://arxiv.org/abs/2602.10734)
*Pawani Porambage,Diego Lopez,Antonio Pastor,Bin Han,José María Jorquera Valero,Manuel Gil Pérez,Noelia Pérez Palma,Antonio Skarmeta,Prajnamaya Dass,Stefan Köpsell,Sonika Ujjwal,Javier José Díaz Rivera,Pol Alemany,Raul Muñoz,Jafar Mohammadi,Chaitanya Aggarwal,Betul Guvenc Paltun,Ferhat Karakoc*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The sixth generation (6G) of mobile networks are being developed to overcome limitations in previous generations and meet emerging user demands. As a European project, the Smart Networks and Services Joint Undertaking (SNS JU) 6G Flagship project Hexa-X-II has a leading role for developing technologies and anchoring 6G end-to-end system. This paper summarizes the security, privacy and resilient (SPR) controls identified by Hexa-X-II project and their validation frameworks.

</details>


### [14] [Less is More: The Dilution Effect in Multi-Link Wireless Sensing](https://arxiv.org/abs/2602.10823)
*Bruno Rodrigues,Karim Khamaisi*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Wireless sensing approaches promise to transform smart infrastructures into privacy-preserving motion detectors, yet commercial adoption remains limited. A common assumption may explain this gap: that denser sensor deployments yield better accuracy. We tested this assumption with a 12-day naturalistic study using a 9-node ESP32-C3 mesh (72 sensing links) in a residential environment. Our results show that a single well-placed link outperformed the full 72-link mesh (AUC 0.541 vs. 0.489, Cohen's $d$=0.86). Even a random link selection matched optimized selection ($p$=0.35). The benefit comes from avoiding multi-link fusion, not from choosing the right link. We attribute this to a "dilution effect": links whose Fresnel zones miss activity regions contribute noise that overwhelms signal from informative links. In our deployment, strategic link placement mattered 2.7$\times$ more than classifier choice. We release 312 hours of labeled CSI data, firmware, and analysis code to enable validation across diverse environments.

</details>


### [15] [AI Infrastructure Sovereignty](https://arxiv.org/abs/2602.10900)
*Sergio Cruzes*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Artificial intelligence has shifted from a software-centric discipline to an infrastructure-driven system. Large-scale training and inference increasingly depend on tightly coupled data centers, high-capacity optical networks, and energy systems operating close to physical and environmental limits. As a result, control over data and algorithms alone is no longer sufficient to achieve meaningful AI sovereignty. Practical sovereignty now depends on who can deploy, operate, and adapt AI infrastructure under constraints imposed by energy availability, sustainability targets, and network reach. This tutorial-survey introduces the concept of AI infrastructure sovereignty, defined as the ability of a region, operator, or nation to exercise operational control over AI systems within physical and environmental limits. The paper argues that sovereignty emerges from the co-design of three layers: AI-oriented data centers, optical transport networks, and automation frameworks that provide real-time visibility and control. We analyze how AI workloads reshape data center design, driving extreme power densities, advanced cooling requirements, and tighter coupling to local energy systems, with sustainability metrics such as carbon intensity and water usage acting as hard deployment boundaries. We then examine optical networks as the backbone of distributed AI, showing how latency, capacity, failure domains, and jurisdictional control define practical sovereignty limits. Building on this foundation, the paper positions telemetry, agentic AI, and digital twins as enablers of operational sovereignty through validated, closed-loop control across compute, network, and energy domains. The tutorial concludes with a reference architecture for sovereign AI infrastructure that integrates telemetry pipelines, agent-based control, and digital twins, framing sustainability as a first-order design constraint.

</details>


### [16] [A Robust Optimization Approach for Regenerator Placement in Fault-Tolerant Networks Under Discrete Cost Uncertainty](https://arxiv.org/abs/2602.11058)
*Mohammad Khosravi,Setareh Maghsudi*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: We focus on robust, survivable communication networks, where network links and nodes are affected by an uncertainty set. In this sense, any network links might fail. Besides, a signal can only travel a maximum distance before its quality falls below a certain threshold, necessitating its regeneration by regenerators installed at network nodes. In addition, the price of installing and maintaining regenerators belongs to a discrete uncertainty set. Robust optimization seeks a solution with guaranteed performance against all scenarios modeled in an uncertainty set. Thus, the problem is to find a subset of nodes with minimum cost for the placement of the regenerator, ensuring that all nodes can communicate even if a subset of network links fails. To solve the problem optimally, we propose two solution approaches, including one flow-based and one cut-based integer programming formulation, as well as their iterative exact method. Our theoretical and experimental results show the effectiveness of our methods.

</details>


### [17] [WHEREIS: IP Address Registration Geo-Consistency](https://arxiv.org/abs/2602.11102)
*Robert Beverly,Amreesh Phokeer,Oliver Gasser*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The five Regional Internet Registries (RIRs) provide the critical function of IP address resource del egation and registration. The accuracy of registration data directly impacts Internet operation, management, security, and optimization. In addition, the scarcity of IP addresses has brought into focus conflicts between RIR policy and IP registration ownership and use. The tension between a free-market based approach to address allocation versus policies to promote fairness and regional equity has resulted in court litigation that threatens the very existence of the RIR system.
  We develop WHEREIS, a measurement-based approach to geolocate delegated IPv4 and IPv6 prefixes at an RIR-region granularity and systematically study where addresses are used post-allocation and the extent to which registration information is accurate. We define a taxonomy of registration ``geo-consistency'' that compares a prefix's measured geolocation to the allocating RIR's coverage region as well as the registered organization's location. While in aggregate over 98% of the prefixes we examine are consistent with our geolocation inferences, there is substantial variation across RIRs and we focus on AFRINIC as a case study. IPv6 registrations are no more consistent than IPv4, suggesting that structural, rather than technical, issues play an important role in allocations. We solicit additional information on inconsistent prefixes from network operators, IP leasing providers, and collaborate with three RIRs to obtain validation. We further show that the inconsistencies we discover manifest in three commercial geolocation databases. By improving the transparency around post-allocation prefix use, we hope to improve applications that use IP registration data and inform ongoing discussions over in-region address use and policy.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs](https://arxiv.org/abs/2602.10218)
*Chenhui Deng,Zhongzhi Yu,Guan-Ting Liu,Nathaniel Pinckney,Haoxing Ren*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.

</details>


### [19] [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](https://arxiv.org/abs/2602.10254)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

</details>


### [20] [DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets](https://arxiv.org/abs/2602.10654)
*Derek Christ,Thomas Zimmermann,Philippe Barbie,Dmitri Saberi,Yao Yin,Matthias Jung*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.
  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.

</details>


### [21] [Fault Tolerant Design of IGZO-based Binary Search ADCs](https://arxiv.org/abs/2602.10790)
*Paula Carolina Lozano Duarte,Sule Ozev,Mehdi Tahoori*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.

</details>


### [22] [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016)
*Jinxin Yu,Yudong Pan,Mengdi Wang,Huawei Li,Yinhe Han,Xiaowei Li,Ying Wang*

Main category: cs.AR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Challenges: Challenges extraction failed

Contributions: Contributions extraction failed

Results: Results analysis unavailable

Conclusion: Conclusion extraction failed

Related Work: Related work extraction failed

Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.

</details>
