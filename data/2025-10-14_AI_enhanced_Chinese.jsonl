{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper.", "AI": {"tldr": "ISAAC \u5c06 LLM \u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u523a\u6fc0\u751f\u6210\u4e0e\u57fa\u4e8e FPGA \u7684\u5e76\u884c\u5171\u4eff\u771f\u7ed3\u5408\uff0c\u901a\u8fc7\u5fae\u7ed3\u6784\u611f\u77e5\u7684\u6d4b\u8bd5\u751f\u6210\u4e0e\u524d\u5411\u5feb\u7167+\u89e3\u8026\u67b6\u6784\uff0c\u5927\u5e45\u52a0\u901f CPU \u5dee\u5206\u9a8c\u8bc1\u5e76\u6355\u83b7\u96be\u89e6\u53d1\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u5dee\u5206\u6d4b\u8bd5\u5728 CPU \u9a8c\u8bc1\u4e2d\u53d7\u524d\u7aef\u523a\u6fc0\u8d28\u91cf\u4f4e\u3001\u5197\u4f59\u9ad8\u548c\u540e\u7aef\u4eff\u771f\u74f6\u9888\u9650\u5236\uff0c\u5bfc\u81f4\u8986\u76d6\u96be\u4ee5\u95ed\u5408\u3001\u89d2\u843d\u60c5\u51b5\u6f0f\u68c0\u4ee5\u53ca\u8c03\u8bd5\u5468\u671f\u957f\u3002", "method": "\u524d\u7aef\u91c7\u7528\u591a\u667a\u80fd\u4f53\u523a\u6fc0\u5f15\u64ce\uff0c\u878d\u5408\u5fae\u7ed3\u6784\u77e5\u8bc6\u4e0e\u5386\u53f2\u7f3a\u9677\u6a21\u5f0f\u751f\u6210\u9ad8\u9488\u5bf9\u6027\u6d4b\u8bd5\uff1b\u540e\u7aef\u5f15\u5165\u8f7b\u91cf\u7ea7\u524d\u5411\u5feb\u7167\u673a\u5236\u4ee5\u53ca ISS \u4e0e DUT \u89e3\u8026\u5171\u4eff\u771f\uff0c\u4f7f\u5355\u4e2a ISS \u53ef\u9a71\u52a8\u591a FPGA \u4e0a\u7684 DUT \u5e76\u884c\u8fd0\u884c\uff0c\u6d88\u9664\u957f\u5c3e\u6d4b\u8bd5\u74f6\u9888\u3002", "result": "\u5728\u5bf9\u4e00\u4e2a\u5df2\u6210\u719f\u4e14\u591a\u6b21\u6d41\u7247\u7684 CPU \u9a8c\u8bc1\u4e2d\uff0cISAAC \u5728\u4eff\u771f\u541e\u5410\u4e0a\u76f8\u8f83\u8f6f\u4ef6 RTL \u4eff\u771f\u6700\u9ad8\u8fbe 17,536x \u63d0\u901f\uff0c\u5e76\u53d1\u73b0\u82e5\u5e72\u5148\u524d\u672a\u77e5\u7684\u7f3a\u9677\uff08\u8bba\u6587\u4e2d\u62a5\u544a\u4e86\u4e24\u4f8b\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a ISAAC \u7684\u7aef\u5230\u7aef LLM \u8f85\u52a9 CPU \u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u7aef\u591a\u667a\u80fd\u4f53\u5fae\u7ed3\u6784\u611f\u77e5\u7684\u523a\u6fc0\u751f\u6210\u5668\u548c\u540e\u7aef\u89e3\u8026\u7684\u5e76\u884c\u5171\u4eff\u771f\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u6548\u7387\u5e76\u53d1\u73b0\u771f\u5b9e\u7f3a\u9677\u3002"}}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u53d8\u7cbe\u5ea6systolic\u9635\u5217ADiP\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5904\u7406\u5355\u5143\u548c\u5171\u4eab\u7d2f\u52a0\u5668\u63d0\u9ad8\u77e9\u9635\u4e58\u6cd5\u7684\u541e\u5410\u4e0e\u80fd\u6548\uff0c\u572822nm\u5b9e\u73b0\u4e0a\u8fbe\u5230\u6700\u9ad84\u00d7\u541e\u5410\u63d0\u5347\uff0c\u5e76\u5728GPT-2/BERT/BitNet\u4efb\u52a1\u4e0a\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u4e0e\u80fd\u8017\u3002", "motivation": "Reduce memory and computation demands of transformers by using quantization and reconfigurable architectures to dynamically adjust precision, improving data reuse and PE utilization.", "method": "Adaptive-precision systolic array (ADiP)", "result": "ADiP supports multiple computation modes (symmetric single-matrix and asymmetric multi-matrix with shared input), adapts to 8\u00d78, 8\u00d74, 8\u00d72 precisions, achieves up to 4\u00d7 higher computational throughput in 22nm, and shows latency improvement up to 53.6% and energy improvement up to 24.4% on transformer workloads; peak throughputs of 8.192, 16.384, 32.768 TOPS for respective precisions at 64\u00d764 size with 4096 PEs.", "conclusion": "ADiP\u901a\u8fc7\u591a\u6a21\u8ba1\u7b97\u652f\u6301\u4e0e\u7cbe\u5ea6\u81ea\u9002\u5e94\uff0c\u5728\u4fdd\u8bc1\u6570\u636e\u91cd\u7528\u548c\u9ad8PE\u5229\u7528\u7387\u60c5\u51b5\u4e0b\uff0c\u4f7f\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5728\u541e\u5410\u548c\u80fd\u6548\u4e0a\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u9002\u914dTransformer\u7c7b\u6a21\u578b\u7684\u63a8\u7406\u9700\u6c42\u3002"}}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9762\u5411\u8fb9\u7f18FPGA\u7684\u91cf\u5316\u611f\u77e5\u591a\u8bed\u79cd\u7ffb\u8bd1\u7cfb\u7edf\uff0cFP4\u91cf\u5316\u5b9e\u73b0\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\u4e0e\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u8d44\u6e90\u5360\u7528\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u5728\u7269\u8054\u7f51\u4e0e\u8fb9\u7f18\u8bbe\u5907\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\uff0c\u9700\u8981\u5728\u53d7\u9650\u8ba1\u7b97\u4e0e\u5b58\u50a8\u8d44\u6e90\u4e0a\u5b9e\u65f6\u5f00\u5c55\u591a\u8bed\u79cd\u7ffb\u8bd1\uff0c\u5c24\u5176\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u5b50\u5b57\u8282\u7cbe\u5ea6\uff08FP8\u3001INT8\u3001INT4\u3001FP4\uff09\u4e0a\u8fdb\u884c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u9488\u5bf9FPGA\u7684\u786c\u4ef6\u4f18\u5316\u4ee5\u51cf\u5c11\u903b\u8f91\u8d44\u6e90\uff08LUT/FF\uff09\u5360\u7528\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7ffb\u8bd1\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u548c\u63d0\u9ad8\u63a8\u7406\u541e\u5410\u91cf\u3002", "result": "\u5728\u6700\u4f4e\u7cbe\u5ea6FP4\u4e0b\u6a21\u578b\u4f53\u79ef\u51cf\u5c0f\u7ea64.1\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u901f\u7ea64.2\u500d\uff08\u541e\u5410\u91cf\u63d0\u9ad8\u81f366 token/s\uff0c\u7ea64.8\u500d\u63d0\u5347\uff09\uff1bFPGA\u90e8\u7f72\u76f8\u6bd4OPU\u548cHPTA\u5206\u522b\u5b9e\u73b0\u7ea62.2\u500d\u548c4.6\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u5728LUT\u548cFF\u5360\u7528\u4e0a\u5206\u522b\u964d\u4f4e\u7ea61.96\u500d\u548c1.65\u500d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8d44\u6e90\u53d7\u9650\u573a\u666f\u7684\u8f7b\u91cf\u591a\u8bed\u79cd\u7ffb\u8bd1\u7cfb\u7edfBhasha-Rupantarika\uff0c\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u5b9e\u73b0\u53ef\u90e8\u7f72\u6027\u3002"}}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy.", "AI": {"tldr": "Use high-density 3D FeNAND and hyperdimensional computing plus D-BAM to parallelize MS spectral library search in-storage, yielding major speed and energy gains while keeping accuracy similar.", "motivation": "Mass spectrometry data growth makes large-scale spectral library search computationally challenging; ISP offers promise but NAND flash limitations hinder throughput; leverage FeNAND advantages and HDC parallelism to overcome serial-read bottleneck.", "method": "ISP-based FeNAND HDC for MS library search", "result": "Proposed ISP architecture using 3D FeNAND combined with hyperdimensional computing and a dual-bound approximate matching (D-BAM) metric achieves 43x speedup and 21x energy efficiency improvement over prior 3D NAND ISP methods, with comparable accuracy.", "conclusion": "Integrating HDC with FeNAND ISP and a tailored D-BAM distance metric enables efficient, parallel MS spectral library search, overcoming traditional NAND throughput limits and significantly improving performance and energy efficiency without sacrificing accuracy."}}
{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.", "AI": {"tldr": "THEAS\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u9002\u914d\uff0c\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u5bf9\u975e\u56fa\u5b9a\u4eb2\u548c\u4efb\u52a1\u63d0\u4f9b\u80fd\u6548\u4e0e\u5b9e\u65f6\u6027\u517c\u987e\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u5e76\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u6216\u6301\u5e73\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347\u7cfb\u7edf\u5728\u8d1f\u8f7d\u6ce2\u52a8\u660e\u663e\u60c5\u5883\u4e0b\u7684\u80fd\u6548\u4e0e\u6027\u80fd\u5e73\u8861\uff0c\u5c24\u5176\u9488\u5bf9\u5f02\u6784\u7cfb\u7edf\u4e0e\u975e\u56fa\u5b9aCPU\u4eb2\u548c\u6027\u7684\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u52a8\u6001\u8d44\u6e90\u6c34\u5e73\u8c03\u6574\u4e0e\u5f02\u6784\u6838\u5fc3\u9009\u62e9\u7b56\u7565\uff0c\u7efc\u5408\u6027\u80fd\u4e0e\u529f\u8017\u76ee\u6807\u5e76\u8fdb\u884c\u591a\u7ef4\u6307\u6807\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u63d0\u51faTHEAS\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u6c34\u5e73\u4ee5\u5728\u6027\u80fd\u4e0e\u80fd\u8017\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u4e0eCFS\u3001EAS\u3001HeteroSched\u548cUtility-Based Scheduling\u8fdb\u884c\u591a\u7ef4\u6bd4\u8f83\u3002", "conclusion": "THEAS\u5728\u9002\u5e94\u6027\u3001\u6838\u5fc3\u9009\u62e9\u3001\u6027\u80fd\u6269\u5c55\u548c\u5b9e\u65f6\u9002\u7528\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u9002\u5408\u5e7f\u6cdb\u5b9e\u65f6\u5e94\u7528\u3002\u540c\u65f6\u9700\u8981\u5173\u6ce8\u5f00\u9500\u548c\u7f13\u5b58\u611f\u77e5\u7b49\u5b9e\u73b0\u7ec6\u8282\u3002"}}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["Jo\u00e3o Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u4f18\u5316\u7a00\u758f\u77e9\u9635\u5728\u8ba1\u7b97\u5185\u5b58\u9635\u5217\u4e0a\u7684\u6620\u5c04\u548c\u8c03\u5ea6\uff0c\u4f7fCIM\u9635\u5217\u5229\u7528\u7387\u63d0\u534750%\u4ee5\u4e0a\uff0c\u5185\u5b58\u5360\u7528\u4e0e\u8ba1\u7b97\u91cf\u51cf\u5c114\u500d\u4ee5\u4e0a\uff0c\u4ece\u800c\u52a0\u901f\u7a00\u758fLLM\u63a8\u7406\u3002", "motivation": "Reduce inference cost of large language models on resource-constrained systems by combining structured sparsity and compute-in-memory architectures to eliminate off-chip data movement and improve efficiency.", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u6620\u5c04\u4e0e\u8c03\u5ea6\u7b56\u7565\uff0c\u5c06\u7a00\u758f\u77e9\u9635\u91cd\u6392\u4e3a\u5757\u5bf9\u89d2\u7ed3\u6784\u5e76\u5408\u7406\u5206\u914d\u5230CIM\u9635\u5217\u4ee5\u63d0\u5347\u5229\u7528\u7387\uff0c\u540c\u65f6\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u7684\u8c03\u5ea6\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u4e0e\u6570\u636e\u79fb\u52a8\u3002", "result": "An automated framework with mapping and scheduling strategies exploiting block-diagonal sparsity to improve CIM array utilization by >50% and reduce memory footprint and FLOPs by >4x for sparse LLM inference on CIM accelerators.", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u7684\u6620\u5c04\u4e0e\u8c03\u5ea6\u4f18\u5316\uff0cCIM\u52a0\u901f\u5668\u80fd\u66f4\u9ad8\u6548\u5730\u6267\u884c\u7a00\u758fLLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u9635\u5217\u5229\u7528\u7387\u5e76\u964d\u4f4e\u5b58\u50a8\u4e0e\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.09983", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.09983", "abs": "https://arxiv.org/abs/2510.09983", "authors": ["Ethan Thompson", "Ali Sadeghi Jahromi", "AbdelRahman Abdou"], "title": "Fine-grained CDN Delegation", "comment": "13 pages, 2 figures", "summary": "The use of Content Delivery Networks (CDNs) has significantly increased over\nthe past decade, with approximately 55 million websites currently relying on\nCDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),\nlack fine-grained definitions of many critical aspects of delegation, such as\nthe length of delegation chains, revocation mechanism, permitted operations,\nand a well-defined scope for said delegation. We present Delegation\nCertificates (DeCerts), which modify X.509 certificate standard and add new\nextensions to enable fine-grained CDN delegation. DeCerts allow domain owners\nto specify delegated and non-delegated subdomains, and control the depth of\ndelegation extended by CDNs, which provides flexibility in delegation\nmanagement. But more importantly, DeCerts are built on a new principle which\nprovides full autonomy to domain owners-domain owners can issue DeCerts fully\nindependent of Certificate Authorities (CAs), and thus have greater flexibility\nin policy control, including revocation methods. Such level of flexibility\nwould be hard to match if CAs where to issue such certificates. Revoking a\nDeCert revokes delegation. We discuss multiple revocation mechanisms for a\nDeCerts balancing security, performance, and delegator control. We modify\nFirefox to support DeCert (i.e., proper validation) as a proof-of-concept, and\ntest it to demonstrate the feasibility, compatibility of DeCerts with browsers\nand TLS/HTTPS protocols. DeCerts enhance the security, scalability, and\nmanageability of CDN delegation, offering a practical solution for Internet\nservices.", "AI": {"tldr": "DeCerts extend X.509 to give domain owners autonomous, fine-grained CDN delegation control (scope, depth, revocation). Implemented in Firefox as proof-of-concept; balances security and performance.", "motivation": "CDN delegation lacks fine-grained controls and autonomy for domain owners; existing solutions like Delegated Credentials don't define delegation length, revocation, operations, or scope well. Need a mechanism to allow domain owners fine-grained delegation and independent issuance.", "method": "Design X.509 extensions for delegation, define semantics for subdomain scope and delegation depth, propose multiple revocation mechanisms, implement validation in Firefox, test compatibility and feasibility.", "result": "Propose DeCerts: X.509 modifications and extensions enabling domain owners to issue delegation certificates independent of CAs, specify delegated/non-delegated subdomains, control delegation depth, and support revocation methods. Implemented Firefox modification proving feasibility and TLS compatibility.", "conclusion": "DeCerts improve security, scalability, and manageability of CDN delegation by allowing domain-owner-issued certificates with controlled scope and revocation, compatible with browsers and TLS."}}
{"id": "2510.09851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09851", "abs": "https://arxiv.org/abs/2510.09851", "authors": ["Haci Ismail Aslan", "Syed Muhammad Mahmudul Haque", "Joel Witzke", "Odej Kao"], "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters", "comment": "Accepted at the International Conference on Service-Oriented\n  Computing (ICSOC) 2025", "summary": "Modern applications increasingly span across cloud, fog, and edge\nenvironments, demanding orchestration systems that can adapt to diverse\ndeployment contexts while meeting Quality-of-Service (QoS) requirements.\nStandard Kubernetes schedulers do not account for user-defined objectives such\nas energy efficiency, cost optimization, and global performance, often leaving\noperators to make manual, cluster-by-cluster placement decisions. To address\nthis need, we present QONNECT, a vendor-agnostic orchestration framework that\nenables declarative, QoS-driven application deployment across heterogeneous\nKubernetes and K3s clusters. QONNECT introduces a distributed architecture\ncomposed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and\nlightweight Resource Agents in each cluster. Through a minimal YAML-based\ninterface, users specify high-level QoS goals, which the system translates into\nconcrete placement and migration actions. Our implementation is evaluated on a\nfederated testbed of up to nine cloud-fog-edge clusters using the Istio\nBookinfo microservice application. The system demonstrates dynamic,\npolicy-driven microservice placement, automated failover, QoS-compliant\nrescheduling, and leader re-election after node failure, all without manual\nintervention. By bridging the gap between declarative deployment models and\noperational QoS goals, QONNECT transforms the cloud-edge continuum into a\nunified, self-optimizing platform.", "AI": {"tldr": "QONNECT enables declarative QoS-driven deployment across heterogeneous Kubernetes/K3s clusters via a distributed agent-based architecture, automating placement and migration to meet goals like energy, cost, performance.", "motivation": "Provide QoS-driven, vendor-agnostic orchestration across heterogeneous cloud-fog-edge Kubernetes clusters to automate placement and migration decisions and avoid manual operator work.", "method": "Design and implement distributed system: central Knowledge Base, Raft-replicated Resource Lead Agents, lightweight Resource Agents; minimal YAML QoS interface; translation of goals into placement/migration actions; evaluation on federated testbed with Istio Bookinfo microservices across up to nine clusters measuring placement, failover, rescheduling, leader re-election.", "result": "QONNECT: distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, lightweight Resource Agents per cluster, YAML QoS specification, evaluated on up to nine clusters with Bookinfo app showing dynamic placement, failover, rescheduling, leader re-election without manual intervention.", "conclusion": "QONNECT bridges declarative deployment and operational QoS, enabling self-optimizing cross-cluster orchestration and robust fault tolerance in cloud-fog-edge environments."}}
{"id": "2510.10040", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.10040", "abs": "https://arxiv.org/abs/2510.10040", "authors": ["Shafi Ullah Khan", "Michel Kulhandjian", "Debashri Roy"], "title": "Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference", "comment": null, "summary": "Spectrum sharing is a critical strategy for meeting escalating user demands\nvia commercial wireless services, yet its effective regulation and\ntechnological enablement, particularly concerning coexistence with incumbent\nsystems, remain significant challenges. Federal organizations have established\nregulatory frameworks to manage shared commercial use alongside\nmission-critical operations, such as military communications. This paper\ninvestigates the potential of machine learning (ML)-based approaches to enhance\nspectrum sharing capabilities within the Citizens Broadband Radio Service\n(CBRS) band, specifically focusing on the coexistence of commercial signals\n(e.g., 5G) and military radar systems. We demonstrate that ML techniques can\npotentially extend the Federal Communications Commission (FCC)-recommended\nsignal-to-interference-plus-noise ratio (SINR) boundaries by improving radar\ndetection and waveform identification in high-interference environments.\nThrough rigorous evaluation using both synthetic and real-world signals, our\nfindings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)\ndata and spectrograms, can achieve the FCC-recommended $99\\%$ radar detection\naccuracy even when subjected to high interference from 5G signals upto -5dB\nSINR, exceeding the required limits of $20$ SINR. Our experimental studies\ndistinguish this work from the state-of-the-art by significantly extending the\nSINR limit for $99\\%$ radar detection accuracy from approximately $12$ dB down\nto $-5$ dB. Subsequent to detection, we further apply ML to analyze and\nidentify radar waveforms. The proposed models also demonstrate the capability\nto classify six distinct radar waveform types with $93\\%$ accuracy.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5728CBRS\u9891\u6bb5\u4e2d\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u63d0\u5347\u5546\u4e1a\u65e0\u7ebf\u4e0e\u519b\u7528\u96f7\u8fbe\u5171\u5b58\u7684\u80fd\u529b\uff0c\u9488\u5bf9\u9ad8\u5e72\u6270\u73af\u5883\u4e0b\u7684\u96f7\u8fbe\u68c0\u6d4b\u4e0e\u6ce2\u5f62\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eIQ\u6570\u636e\u548c\u9891\u8c31\u56fe\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u9ad8\u8fbe-5 dB SINR \u7684\u5e72\u6270\u4e0b\u4ecd\u53ef\u8fbe\u5230FCC\u5efa\u8bae\u768499%\u68c0\u6d4b\u7387\uff0c\u5e76\u80fd\u4ee593%\u51c6\u786e\u7387\u533a\u52066\u79cd\u96f7\u8fbe\u6ce2\u5f62\uff0c\u663e\u8457\u5c0699%\u68c0\u6d4b\u6240\u9700\u7684SINR\u4e0b\u9650\u4ece\u7ea612 dB\u6269\u5c55\u5230-5 dB\u3002", "motivation": "\u9762\u5bf9\u5546\u4e1a\u65e0\u7ebf\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u9891\u8c31\u5171\u4eab\u6210\u4e3a\u5173\u952e\u7b56\u7565\uff0c\u4f46\u9700\u5728\u4e0d\u5f71\u54cd\u5173\u952e\u4f7f\u547d\u7cfb\u7edf\uff08\u5982\u519b\u7528\u96f7\u8fbe\uff09\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5171\u5b58\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63d0\u9ad8\u96f7\u8fbe\u5728\u9ad8\u5e72\u6270\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u548c\u6ce2\u5f62\u8bc6\u522b\u80fd\u529b\uff0c\u4ece\u800c\u63a8\u52a8\u66f4\u7075\u6d3b\u7684\u9891\u8c31\u7ba1\u7406\u4e0e\u6280\u672f\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eIQ\u6837\u672c\u548c\u9891\u8c31\u56fe\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u96f7\u8fbe\u68c0\u6d4b\u4e0e\u6ce2\u5f62\u5206\u7c7b\u3002\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u65f6\u91c7\u7528\u5408\u6210\u4fe1\u53f7\u4e0e\u771f\u5b9e\u4e16\u754c\u4fe1\u53f7\u6570\u636e\u96c6\uff0c\u6d4b\u91cf\u4e0d\u540cSINR\u6761\u4ef6\u4e0b\u7684\u68c0\u6d4b\u7387\u548c\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6bd4\u8f83\u6a21\u578b\u6027\u80fd\u4e0eFCC\u63a8\u8350\u9608\u503c\u53ca\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u5728IQ\u4e0e\u9891\u8c31\u56fe\u8f93\u5165\u4e0b\uff0c\u5728-5 dB SINR\u6761\u4ef6\u4ecd\u8fbe\u523099%\u96f7\u8fbe\u68c0\u6d4b\u7387\uff08\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u9700\u7ea612 dB\uff09\uff0c\u5e76\u5728\u96f7\u8fbe\u6ce2\u5f62\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b093%\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86ML\u65b9\u6cd5\u6269\u5c55SINR\u5bb9\u9650\u4e0e\u533a\u5206\u591a\u79cd\u96f7\u8fbe\u6ce2\u5f62\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ML\u6a21\u578b\u80fd\u5728CBRS\u573a\u666f\u4e2d\u663e\u8457\u63d0\u9ad8\u96f7\u8fbe\u68c0\u6d4b\u548c\u6ce2\u5f62\u8bc6\u522b\u6027\u80fd\uff0c\u4f7f\u5728\u9ad8\u8fbe-5 dB SINR\u76845G\u5e72\u6270\u4e0b\u4ecd\u5b9e\u73b099%\u68c0\u6d4b\u7387\uff0c\u5e76\u80fd\u4ee593%\u51c6\u786e\u7387\u5206\u7c7b\u516d\u79cd\u96f7\u8fbe\u6ce2\u5f62\uff0c\u4ece\u800c\u6709\u671b\u6269\u5c55\u73b0\u6709\u76d1\u7ba1\u548c\u5171\u5b58\u7b56\u7565\u7684\u8fb9\u754c\u3002"}}
{"id": "2510.10126", "categories": ["cs.DC", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10126", "abs": "https://arxiv.org/abs/2510.10126", "authors": ["Sehar Zehra", "Hassan Jamil Syed", "Ummay Faseeha"], "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments", "comment": "7 pages , 6 figures , 1 table and it is a conference paper", "summary": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving\nanomaly detection. Existing eBPF-based monitors provide low-overhead system and\nnetwork visibility but are limited to single clusters, while centralized\napproaches incur bandwidth, privacy, and heterogeneity challenges. We propose\nFedMon, a federated eBPF framework that unifies kernel-level telemetry with\nfederated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF\nagents capture syscalls and network events, extract local statistical and\nsequence features, and share only model updates with a global server. A hybrid\ndetection engine combining Variational Autoencoders (VAEs) with Isolation\nForests enables both temporal pattern modeling and outlier detection. Deployed\nacross three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,\nand an F1-score of 0.92, while cutting bandwidth usage by 60% relative to\ncentralized baselines. Results demonstrate that FedMon enhances accuracy,\nscalability, and privacy, providing an effective defense for large-scale,\nmulti-tenant cloud-native environments.", "AI": {"tldr": "FedMon combines lightweight eBPF agents with federated learning (VAE + Isolation Forest) to detect anomalies across multiple Kubernetes clusters while preserving privacy and reducing bandwidth", "motivation": "Enable scalable, privacy-preserving cross-cluster telemetry and anomaly detection without centralized raw data collection", "method": "Federated anomaly detection using eBPF across Kubernetes clusters", "result": "FedMon achieves 94% precision, 91% recall, F1 0.92, and 60% bandwidth reduction compared to centralized baselines", "conclusion": "FedMon improves accuracy, scalability, and privacy for multi-cluster cloud-native anomaly detection"}}
{"id": "2510.10044", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.10044", "abs": "https://arxiv.org/abs/2510.10044", "authors": ["Rahul Vanukuri", "Shafi Ullah Khan", "Talip Tolga Sar\u0131", "Gokhan Secinti", "Diego Pati\u00f1o", "Debashri Roy"], "title": "Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures", "comment": null, "summary": "The growing demand for effective spectrum management and interference\nmitigation in shared bands, such as the Citizens Broadband Radio Service\n(CBRS), requires robust radar detection algorithms to protect the military\ntransmission from interference due to commercial wireless transmission. These\nalgorithms, in turn, depend on large, diverse, and carefully labeled\nspectrogram datasets. However, collecting and annotating real-world radio\nfrequency (RF) spectrogram data remains a significant challenge, as radar\nsignals are rare, and their occurrences are infrequent. This challenge makes\nthe creation of balanced datasets difficult, limiting the performance and\ngeneralizability of AI models in this domain.\n  To address this critical issue, we propose a diffusion-based generative model\nfor synthesizing realistic and diverse spectrograms of five distinct categories\nthat integrate LTE, 5G, and radar signals within the CBRS band. We conduct a\nstructural and statistical fidelity analysis of the generated spectrograms\nusing widely accepted evaluation metrics Structural Similarity Index Measure\n(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from\nthe training data. Furthermore, we demonstrate that pre-training on the\ngenerated spectrograms significantly improves training efficiency on a\nreal-world radar detection task by enabling $51.5\\%$ faster convergence.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u751f\u6210CBRS\u9891\u6bb5LTE/5G/\u96f7\u8fbe\u6df7\u5408\u9891\u8c31\u56fe\uff0c\u4f7f\u7528SSIM/PSNR\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\uff0c\u9884\u8bad\u7ec3\u53ef\u4f7f\u771f\u5b9e\u96f7\u8fbe\u68c0\u6d4b\u4efb\u52a1\u6536\u655b\u901f\u5ea6\u63d0\u534751.5%", "motivation": "real-world radar signals are rare, making balanced labeled datasets for radar detection in shared bands like CBRS difficult to collect, limiting AI model performance and generalizability", "method": "diffusion-based generative model for RF spectrograms", "result": "generated realistic diverse spectrograms across five categories (LTE, 5G, radar combinations); evaluated using SSIM and PSNR showing structural/statistical fidelity; pre-training on generated data yields 51.5% faster convergence on real radar detection task", "conclusion": "\u6269\u6563\u751f\u6210\u7684\u5408\u6210\u9891\u8c31\u56fe\u80fd\u8865\u5145\u7a00\u7f3a\u771f\u5b9e\u6570\u636e\uff0c\u63d0\u5347\u4e0b\u6e38\u96f7\u8fbe\u68c0\u6d4b\u4efb\u52a1\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6cdb\u5316"}}
{"id": "2510.10166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10166", "abs": "https://arxiv.org/abs/2510.10166", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing", "comment": null, "summary": "Edge computing allows for the decentralization of computing resources. This\ndecentralization is achieved through implementing microservice architectures,\nwhich require low latencies to meet stringent service level agreements (SLA)\nsuch as performance, reliability, and availability metrics. While cloud\ncomputing offers the large data storage and computation resources necessary to\nhandle peak demands, a hybrid cloud and edge environment is required to ensure\nSLA compliance. Several auto-scaling algorithms have been proposed to try to\nachieve these compliance challenges, but they suffer from performance issues\nand configuration complexity. This chapter provides a brief overview of edge\ncomputing architecture, its uses, benefits, and challenges for resource\nscaling. We then introduce Service Level Agreements, and existing research on\ndevising algorithms used in edge computing environments to meet these\nagreements, along with their benefits and drawbacks.", "AI": {"tldr": "Edge computing decentralizes resources using microservices requiring low latency; hybrid cloud-edge needed for SLAs; existing auto-scaling algorithms have performance/config complexity issues; chapter surveys architecture, SLAs, and algorithms.", "motivation": "Summarize and critique auto-scaling in edge-cloud hybrid for SLA compliance", "method": "Paper analysis based on abstract", "result": "Overview of edge computing, SLAs, and auto-scaling algorithms with pros/cons", "conclusion": "Need for improved auto-scaling methods balancing performance and configuration simplicity; future work on adaptive, lightweight, SLA-aware scaling."}}
{"id": "2510.10302", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10302", "abs": "https://arxiv.org/abs/2510.10302", "authors": ["Liangkun Chen", "Zijian Wen", "Tian Wu", "Xiaoxi Zhang", "Chuan Wu"], "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large\nlanguage models (LLMs) to reduce computation cost through model sparsity.\nEmploying speculative decoding (SD) can further accelerate MoE inference by\ndrafting multiple tokens per step and verifying them in parallel. However,\ncombining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth\ncontention during multi-token verification. Existing MoE offloading systems are\nSD-agnostic and do not address this bottleneck. We present SP-MoE, the first\nSD-aware expert-offloading and compute-communication pipelining framework.\nSP-MoE introduces: (1) speculative expert prefetching that exploits structural\ncorrespondence between the draft and target models to prefetch likely experts\nahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch\ndepth based on empirical profiles and an analytical latency model, guaranteeing\njust-in-time availability without overfetch; and (3) a pipelined runtime with\nasynchronous prefetch threads and batched I/O to hide loading latency.\nExtensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT\nspeedup over state-of-the-art methods across diverse datasets, environments,\nand MoE-based models.", "AI": {"tldr": "SP-MoE is an SD-aware offloading framework that prefetches likely experts based on draft-target model correspondence, bounds prefetching with a cutoff-layer policy, and pipelines I/O to reduce latency, delivering significant inference speedups.", "motivation": "Combining Mixture-of-Experts (MoE) with Speculative Decoding (SD) inflates GPU memory and creates CPU-GPU bandwidth bottlenecks during multi-token verification; existing offloading systems ignore SD-specific patterns.", "method": "Speculative expert-offloading and pipelining for MoE with SD", "result": "SP-MoE introduces speculative expert prefetching, a cutoff-layer policy for bounded prefetch depth, and a pipelined runtime with async prefetch threads and batched I/O; achieves 1.07\u20133.5x TPOT speedup over SOTA across datasets, environments, and MoE models.", "conclusion": "SD-aware prefetching combined with cutoff-layer control and pipelined runtime effectively mitigates memory and bandwidth bottlenecks in MoE+SD inference, improving throughput and latency compared to prior methods."}}
{"id": "2510.10236", "categories": ["cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10236", "abs": "https://arxiv.org/abs/2510.10236", "authors": ["Dhrumil Bhatt", "Siddharth Penumatsa", "Vidushi Kumar"], "title": "Hybrid MAC Protocol with Integrated Multi-Layered Security for Resource-Constrained UAV Swarm Communications", "comment": "Accepted at ISED 2025", "summary": "Flying Ad Hoc Networks (FANETs) present unique challenges due to high node\nmobility, dynamic topologies, and strict resource constraints. Existing routing\nprotocols often optimize for a single metric, such as path length or energy,\nwhile neglecting the complex dependencies between network performance,\nsecurity, and MAC layer efficiency. This paper introduces a novel hardware\nsoftware co design framework for secure and adaptive UAV swarm communications,\nfeaturing an energy aware protocol stack. The architecture employs a multicast,\nclustered organization where routing decisions integrate dynamic trust scores,\nhistorical link quality, and internodal distance. A hybrid MAC protocol\ncombines contention based and scheduled channel access for optimized\nthroughput. Security is ensured through a zero trust model that fuses\ncryptographic authentication with a behavioral reputation system, alongside\nhardware accelerated AES GCM encryption. Comparative analysis in an NS 3\nsimulation environment demonstrates the framework's superiority in packet\ndelivery ratio, latency, resilience, and overhead, providing a scalable\nfoundation for high performance swarm operations.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u65e0\u4eba\u673a\u7fa4\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u591a\u64ad\u7c07\u7ec4\u7ec7\u3001\u4fe1\u4efb\u9a71\u52a8\u8def\u7531\u3001\u6df7\u5408MAC\u4e0e\u96f6\u4fe1\u4efb\u5b89\u5168\uff0cNS-3\u4eff\u771f\u8bc1\u660e\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6848", "motivation": "FANETs suffer from high mobility and resource constraints; existing protocols optimize single metrics and ignore interdependence of performance, security, and MAC efficiency", "method": "Hardware-software co-design for secure adaptive UAV swarm communications", "result": "Proposed a multicast clustered architecture with routing using trust, link quality, distance; hybrid MAC; zero trust security with cryptographic and reputation fusion; AES-GCM hardware acceleration; NS-3 simulation shows improved PDR, latency, resilience, and overhead", "conclusion": "\u8be5\u6846\u67b6\u5728\u541e\u5410\u3001\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u4e0a\u5747\u4f18\u4e8e\u5355\u4e00\u4f18\u5316\u7684\u4f20\u7edf\u534f\u8bae\uff0c\u4e3a\u9ad8\u6027\u80fd\u53ef\u6269\u5c55\u65e0\u4eba\u673a\u7fa4\u901a\u4fe1\u63d0\u4f9b\u4e86\u57fa\u7840"}}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines.", "AI": {"tldr": "\u63d0\u51faFLAMMABLE\uff0c\u7528\u52a8\u6001\u6279\u6b21\u5927\u5c0f\u548c\u57fa\u4e8e\u80fd\u529b\u7684\u591a\u6a21\u578b\u5206\u914d\u7b56\u7565\u4f18\u5316\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\uff1b\u914d\u5957MMFL\u57fa\u51c6\u5e73\u53f0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u65f6\u95f4-\u51c6\u786e\u7387\u548c\u6700\u7ec8\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "Address inefficiencies in training multiple models concurrently in federated learning due to combined data, system, and model heterogeneity; current single-model FL methods and naive extensions don't scale.", "method": "\u8bbe\u8ba1\u5ba2\u6237\u7aef\u80fd\u529b\u611f\u77e5\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u5ba2\u6237\u7aef\u7684\u8bad\u7ec3\u6279\u6b21\u5927\u5c0f\uff0c\u5e76\u5728\u6bcf\u8f6e\u9009\u62e9\u591a\u4e2a\u9002\u914d\u6a21\u578b\u8ba9\u5ba2\u6237\u7aef\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u9ad8\u5e76\u884c\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\uff1b\u5b9e\u73b0\u5e76\u8bc4\u4f30\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "FLAMMABLE framework that adapts client batch sizes and assigns clients to train multiple chosen models per round based on capabilities; also a first MMFL benchmark platform.", "conclusion": "FLAMMABLE\u6709\u6548\u5e94\u5bf9MMFL\u4e2d\u7684\u5f02\u6784\u6027\uff0c\u901a\u8fc7\u667a\u80fd\u6279\u91cf\u8c03\u6574\u4e0e\u6a21\u578b\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u4e0e\u6a21\u578b\u7cbe\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684MMFL\u57fa\u51c6\u5e73\u53f0\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.10756", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.10756", "abs": "https://arxiv.org/abs/2510.10756", "authors": ["Mayukh Roy Chowdhury", "Eman Hammad", "Lauri Loven", "Susanna Pirttikangas", "Aloizio P da Silva", "Walid Saad"], "title": "A Framework for AI-Native Semantic-Based Dynamic Slicing for 6G Networks", "comment": null, "summary": "In the ensuing ultra-dense and diverse environment in future \\ac{6G}\ncommunication networks, it will be critical to optimize network resources via\nmechanisms that recognize and cater to the diversity, density, and dynamicity\nof system changes. However, coping with such environments cannot be done\nthrough the current network approach of compartmentalizing data as distinct\nfrom network operations. Instead, we envision a computing continuum where the\ncontent of the transmitted data is considered as an essential element in the\ntransmission of that data, with data sources and streams analyzed and distilled\nto their essential elements, based on their semantic context, and then\nprocessed and transmitted over dedicated slices of network resources. By\nexploiting the rich content and semantics within data for dynamic and\nautonomous optimization of the computing continuum, this article opens the door\nto integrating communication, computing, cyber-physical systems, data flow, and\nAI, presenting new and exciting opportunities for cross-layer design. We\npropose semantic slicing, a two-pronged approach that builds multiple virtual\ndivisions within a single physical and data infrastructure, each with its own\ndistinct characteristics and needs. We view semantic slicing as a novel shift\nfrom current static slicing techniques, extending existing slicing approaches\nsuch that it can be applied dynamically at different levels and categories of\nresources in the computing continuum. Further it propels the advancement of\nsemantic communication via the proposed architectural framework.", "AI": {"tldr": "Introduce semantic slicing: dynamically create virtual network/computation slices based on data semantics to optimize resource use in 6G computing continuum.", "motivation": "Optimize network resources in ultra-dense, diverse 6G environments by integrating data content/semantics into network operations for dynamic, autonomous optimization across computing continuum.", "method": "Analyze Abstract", "result": "Proposes 'semantic slicing'\u2014dynamic, content-aware virtual divisions across computing and network resources to enable cross-layer design and advance semantic communication.", "conclusion": "Semantic slicing extends static network slicing by incorporating data semantics and dynamic allocation across communication, computing, and storage, enabling more efficient, adaptive, cross-layer 6G systems."}}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks.", "AI": {"tldr": "\u63d0\u51faDCP\uff1a\u901a\u8fc7\u5757\u72b6\u52a8\u6001\u5212\u5206\u4e0e\u6620\u5c04\uff0c\u9488\u5bf9\u53d8\u957f\u5e8f\u5217\u4e0e\u591a\u6837\u6ce8\u610f\u529b\u6a21\u5f0f\u51cf\u5c11\u901a\u4fe1\u4e0e\u8d1f\u8f7d\u4e0d\u5747\uff0c\u63d0\u5347\u6ce8\u610f\u529b\u4e0e\u6574\u4f53\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5e76\u884c\u65b9\u6848\u4f7f\u7528\u9759\u6001\u5e76\u884c\u914d\u7f6e\uff0c\u5ffd\u89c6\u8bad\u7ec3\u6570\u636e\u5728\u5e8f\u5217\u957f\u5ea6\u4e0e\u6ce8\u610f\u529b\u5173\u7cfb\u4e0a\u7684\u52a8\u6001\u6027\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u901a\u4fe1\u5f00\u9500\u4e0e\u8ba1\u7b97\u4e0d\u5747\u8861\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u9002\u914d\u6837\u672c\u5dee\u5f02\u7684\u5e76\u884c\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7ec6\u7c92\u5ea6\u7684\u5757\u72b6\u5212\u5206\uff08\u6570\u636e\u5757+\u8ba1\u7b97\u5757\uff09\uff0c\u5e76\u5b9e\u884c\u52a8\u6001\u7684\u5757\u2014\u8bbe\u5907\u6620\u5c04\u7b56\u7565\u4ee5\u5339\u914d\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u548c\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u51cf\u5c11\u8de8\u8bbe\u5907\u901a\u4fe1\u548c\u91cd\u590d\u8ba1\u7b97\u3002\u540c\u65f6\u901a\u8fc7\u5fae\u57fa\u51c6\u8bc4\u6d4b\uff08causal\u4e0esparse\u63a9\u7801\uff09\u91cf\u5316\u6027\u80fd\u63d0\u5347\u3002", "result": "DCP\u662f\u4e00\u79cd\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684\u52a8\u6001\u4e0a\u4e0b\u6587\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5757\u72b6\u5212\u5206\u6570\u636e\u4e0e\u8ba1\u7b97\u5e76\u652f\u6301\u7075\u6d3b\u6620\u5c04\u5230\u8bbe\u5907\uff0c\u4ece\u800c\u51cf\u5c11\u901a\u4fe1\u3001\u6539\u5584\u5185\u5b58\u4e0e\u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u3002\u5b9e\u9a8c\u663e\u793a\u5728\u56e0\u679c\u63a9\u7801\u4e0b\u52a0\u901f\u6ce8\u610f\u529b1.19x~2.45x\uff0c\u7a00\u758f\u6ce8\u610f\u529b\u4e0b2.15x~3.77x\uff1b\u7aef\u5230\u7aef\u8bad\u7ec3\u5728\u56e0\u679c\u63a9\u7801\u4e0b\u63d0\u53470.94x~1.16x\uff0c\u7a00\u758f\u63a9\u7801\u4e0b1.00x~1.46x\u3002", "conclusion": "DCP\u80fd\u6709\u6548\u9002\u914d\u8bad\u7ec3\u6837\u672c\u7684\u5e8f\u5217\u957f\u5ea6\u4e0e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u52a8\u6001\u53d8\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e0d\u5fc5\u8981\u901a\u4fe1\u5e76\u6539\u5584\u8ba1\u7b97/\u5185\u5b58\u5747\u8861\uff0c\u4ece\u800c\u5728\u591a\u79cd\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u6ce8\u610f\u529b\u5b50\u7b97\u5b50\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u52a0\u901f\u3002"}}
{"id": "2510.11043", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.11043", "abs": "https://arxiv.org/abs/2510.11043", "authors": ["Yuemeng Xu", "Haoran Chen", "Jiarui Guo", "Mingwei Cui", "Qiuheng Yin", "Cheng Dong", "Daxiang Kang", "Xian Wu", "Chenmin Sun", "Peng He", "Yang Gao", "Lirong Lai", "Kai Wang", "Hongyu Wu", "Tong Yang", "Xiyun Xu"], "title": "Zephyrus: Scaling Gateways Beyond the Petabit-Era with DPU-Augmented Hierarchical Co-Offloading", "comment": null, "summary": "Operating at petabit-scale, ByteDance's cloud gateways are deployed at\ncritical aggregation points to orchestrate a wide array of business traffic.\nHowever, this massive scale imposes significant resource pressure on our\nprevious-generation cloud gateways, rendering them unsustainable in the face of\never-growing cloud-network traffic. As the DPU market rapidly expands, we see a\npromising path to meet our escalating business traffic demands by integrating\nDPUs with our established Tofino-based gateways. DPUs augment these gateways\nwith substantially larger table capacities and richer programmability without\ncompromising previously low-latency and high-throughput forwarding. Despite\ncompelling advantages, the practical integration of DPUs into cloud gateways\nremains unexplored, primarily due to underlying challenges. In this paper, we\npresent Zephyrus, a production-scale gateway built upon a unified P4 pipeline\nspanning high-performance Tofino and feature-rich DPUs, which successfully\novercomes these challenges. We further introduce a hierarchical co-offloading\narchitecture (HLCO) to orchestrate traffic flow within this heterogeneous\ngateway, achieving > 99% hardware offloading while retaining software fallback\npaths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33%\nhigher throughput and our evaluation further indicates 21% lower power\nconsumption and 14% lower hardware cost. Against FPGA-based systems, Albatross\n(SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of\nOwnership (TCO), showcasing its superior performance-per-dollar. Beyond these\nperformance gains, we also share key lessons from several years of developing\nand operating Zephyrus at production scale. We believe these insights provide\nvaluable references for researchers and practitioners designing performant\ncloud gateways.", "AI": {"tldr": "\u63d0\u51faZephyrus\uff0c\u5c06DPU\u4e0eTofino\u7edf\u4e00P4\u6d41\u6c34\u7ebf\u878d\u5408\uff0c\u91c7\u7528\u5206\u5c42\u534f\u540c\u5378\u8f7d\u5b9e\u73b0\u9ad8\u6bd4\u4f8b\u786c\u4ef6\u5378\u8f7d\u5e76\u4fdd\u7559\u8f6f\u4ef6\u56de\u9000\uff1b\u5728\u541e\u5410\u3001\u529f\u8017\u548c\u6210\u672c\u4e0a\u8f83\u524d\u4f5c\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u9762\u5bf9\u5b57\u8282\u8df3\u52a8\u4e91\u7f51\u5173\u7684\u5343\u4e07\u4ebf\u6bd4\u7279\u7ea7\u6d41\u91cf\uff0c\u4f20\u7edfTofino\u7f51\u5173\u8d44\u6e90\u53d7\u9650\uff0cDPU\u5e02\u573a\u6269\u5c55\u63d0\u4f9b\u66f4\u5927\u8868\u5bb9\u91cf\u4e0e\u53ef\u7f16\u7a0b\u6027\uff0c\u6545\u63a2\u7d22\u4e8c\u8005\u878d\u5408\u4ee5\u5e94\u5bf9\u589e\u957f\u7684\u4e1a\u52a1\u6d41\u91cf\u3002", "method": "\u8bbe\u8ba1\u7edf\u4e00\u7684P4\u6d41\u6c34\u7ebf\u8de8\u8d8aTofino\u4e0eDPU\uff0c\u63d0\u51fa\u5c42\u6b21\u5316\u534f\u540c\u5378\u8f7d(HLCO)\u7528\u4e8e\u6d41\u91cf\u5206\u914d\u4e0e\u590d\u6742\u64cd\u4f5c\u56de\u9000\uff0c\u540c\u65f6\u5728\u751f\u4ea7\u73af\u5883\u8bc4\u4f30\u541e\u5410\u3001\u80fd\u8017\u4e0eTCO\u3002", "result": "\u5b9e\u73b0>99%\u786c\u4ef6\u5378\u8f7d\uff0c\u541e\u5410\u6bd4LuoShen\u9ad833%\uff0c\u529f\u8017\u4f4e21%\uff0c\u786c\u4ef6\u6210\u672c\u4f4e14%\uff1b\u5bf9\u6bd4Albatross\u541e\u5410\u7ffb\u500d\u4e14TCO\u66f4\u4f4e\uff1b\u5e76\u5206\u4eab\u591a\u5e74\u751f\u4ea7\u5b9e\u8df5\u7ecf\u9a8c\u3002", "conclusion": "Zephyrus\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u6210\u529f\u5c06DPU\u4e0eTofino\u7f51\u5173\u96c6\u6210\uff0c\u901a\u8fc7HLCO\u5b9e\u73b0>99%\u786c\u4ef6\u5378\u8f7d\u5e76\u4fdd\u7559\u8f6f\u4ef6\u56de\u9000\uff0c\u8f83LuoShen\u548cAlbatross\u5728\u541e\u5410\u3001\u80fd\u8017\u548c\u6210\u672c\u4e0a\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u662f\u6784\u5efa\u9ad8\u6027\u80fd\u4e91\u7f51\u5173\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs).", "AI": {"tldr": "The paper challenges conventional wisdom that container CPU-limits are universally beneficial, providing evidence that they can hurt latency-sensitive applications and proposing to avoid them in such cases while reserving limits for well-justified scenarios.", "motivation": "Investigate whether the widespread use of CPU-limits for containers is necessary or beneficial, given reports that limits may harm performance and increase costs for cloud-native, latency-sensitive applications.", "method": "Empirical evaluation comparing application performance and costs with and without CPU-limits, analysis of cloud user experiences, and discussion of autoscaling/billing implications; case studies demonstrating when limits help (e.g., background jobs).", "result": "Empirical evidence showing that removing CPU-limits improves performance and reduces costs for latency-sensitive applications; proposals for rethinking autoscaling and billing; identification of scenarios where CPU-limits remain useful (e.g., background jobs).", "conclusion": "CPU-limits should often be avoided for latency-sensitive cloud-native applications; autoscaling and billing models should be reconsidered; CPU-limits can still be useful for isolation of non-latency-critical workloads when applied carefully."}}
{"id": "2510.10818", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10818", "abs": "https://arxiv.org/abs/2510.10818", "authors": ["Kevin Chalmers", "Jan B\u00e6kgaard Pedersen"], "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes", "comment": null, "summary": "We present the first spin-free, kernel-lock-free mutex that cooperates with\nuser-mode schedulers and is formally proven FIFO-fair and linearizable using\nCSP/FDR. Our fairness oracle and stability-based proof method are reusable\nacross coroutine runtime designs. We designed the claim/release protocol for a\nprocess-oriented language -- ProcessJ -- to manage the race for claiming shared\ninter-process communication channels. Internally, we use a lock-free queue to\npark waiting processes for gaining access to a shared object, such as exclusive\naccess to a shared channel to read from or write to. The queue ensures control\nand fairness for processes wishing to access a shared resource, as the protocol\nhandles claim requests in the order they are inserted into the queue. We\nproduce CSP models of our protocol and a mutex specification, demonstrating\nwith FDR that our protocol behaves as a locking mutex.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u534f\u7a0b\u8fd0\u884c\u65f6\u7684\u65e0\u81ea\u65cb\u3001\u65e0\u5185\u6838\u9501\u4e92\u65a5\u673a\u5236\uff1b\u57fa\u4e8e\u9501-free \u961f\u5217\u7684\u6392\u961f\u7b56\u7565\u4fdd\u8bc1 FIFO \u516c\u5e73\u6027\uff0c\u5e76\u7528 CSP/FDR \u5f62\u5f0f\u5316\u8bc1\u660e\u7ebf\u6027\u5316\u548c\u516c\u5e73\u6027\uff0c\u8bc1\u660e\u65b9\u6cd5\u4e0e\u516c\u6b63\u6027\u9884\u8a00\u5668\u53ef\u590d\u7528\u3002", "motivation": "\u5728\u7528\u6237\u6001\u8c03\u5ea6\u5668\u548c\u534f\u7a0b\u8fd0\u884c\u65f6\u4e2d\uff0c\u4f20\u7edf\u4e92\u65a5\u673a\u5236\uff08\u81ea\u65cb\u3001\u5185\u6838\u9501\uff09\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u4e0e\u7528\u6237\u6001\u8c03\u5ea6\u534f\u4f5c\uff0c\u6545\u9700\u8981\u4e00\u79cd\u4f4e\u5f00\u9500\u4e14\u53ef\u8bc1\u660e\u516c\u5e73\u4e0e\u7ebf\u6027\u5316\u7684\u4e92\u65a5\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86 claim/release \u534f\u8bae\u4ee5\u7ba1\u7406\u8fdb\u7a0b\u5bf9\u5171\u4eab\u901a\u9053\u7684\u4e89\u7528\uff0c\u4f7f\u7528\u65e0\u9501\u961f\u5217\u505c\u8f66\u7b49\u5f85\u8fdb\u7a0b\uff0c\u63d0\u51fa\u516c\u5e73\u6027\u9884\u8a00\u5668\u5e76\u7528\u7a33\u5b9a\u6027\u8bc1\u660e\u65b9\u6cd5\u7ed3\u5408 CSP \u5efa\u6a21\u548c FDR \u9a8c\u8bc1\u6765\u8bc1\u660e\u534f\u8bae\u6ee1\u8db3\u4e92\u65a5\u8bed\u4e49\u548c FIFO \u516c\u5e73\u6027\u3002", "result": "The paper introduces a novel mutex for coroutine runtimes that is spin-free and kernel-lock-free, compatible with user-mode schedulers, and proven FIFO-fair and linearizable using CSP/FDR. It includes a reusable fairness oracle and stability-based proof technique, a claim/release protocol implemented in ProcessJ, and uses a lock-free queue to manage waiting processes. CSP models and FDR verification show protocol correctness relative to a mutex specification.", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u5176\u4e92\u65a5\u534f\u8bae\u5728\u7528\u6237\u6001\u8c03\u5ea6\u5668\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86 FIFO \u516c\u5e73\u548c\u7ebf\u6027\u5316\uff0c\u4e14\u65e0\u9700\u81ea\u65cb\u6216\u5185\u6838\u9501\uff1b\u63d0\u51fa\u7684\u516c\u5e73\u6027\u9884\u8a00\u5668\u4e0e\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u8bc1\u660e\u65b9\u6cd5\u53ef\u590d\u7528\u4e8e\u5176\u4ed6\u534f\u7a0b\u8fd0\u884c\u65f6\u8bbe\u8ba1\u3002"}}
{"id": "2510.10833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10833", "abs": "https://arxiv.org/abs/2510.10833", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed and\nreliable systems. In the proposed framework we have used three parts to\nincrease Satisfaction and Performance of this framework. At first we analyze\nprevious frameworks related to integrated systems, then represent new proposed\nframework in order to improving previous framework, and we discuss its\ndifferent phases. Finally we compare the results of simulation of the new\nframework with previous ones. In FIDRS framework, the technique of\nheterogeneous distributed data base is used to improve Performance and speed in\nresponding to users and in this way we can improve dependability and\nreliability of framework simultaneously. In extraction phase of the new\nframework we have used RMSD algorithm that decreases responding time in big\ndatabase. Finally by using FDIRS framework we succeeded to increase Efficiency,\nPerformance and reliability of integrated systems and remove some of previous\nframeworks problems.", "AI": {"tldr": "\u63d0\u51faFIDRS\u6846\u67b6\uff1a\u5229\u7528\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e0eRMSD\u7b97\u6cd5\u63d0\u5347\u96c6\u6210\u7cfb\u7edf\u7684\u54cd\u5e94\u901f\u5ea6\u3001\u6027\u80fd\u4e0e\u53ef\u9760\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u76ee\u6807\u662f\u6539\u8fdb\u73b0\u6709\u96c6\u6210\u7cfb\u7edf\u5728\u6027\u80fd\u3001\u54cd\u5e94\u901f\u5ea6\u53ca\u53ef\u9760\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e00\u4e2a\u65e2\u9ad8\u6548\u53c8\u53ef\u9760\u7684\u5206\u5e03\u5f0f\u96c6\u6210\u6846\u67b6\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u6570\u636e\u5e93\u73af\u5883\u4e0b\u7684\u9700\u6c42\u3002", "method": "\u8bba\u6587\u5148\u56de\u987e\u4e86\u76f8\u5173\u5de5\u4f5c\uff0c\u7136\u540e\u8bbe\u8ba1\u4e86\u5305\u542b\u4e09\u90e8\u5206\u7684FIDRS\u6846\u67b6\uff1a\u91c7\u7528\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u54cd\u5e94\u901f\u5ea6\uff1b\u5728\u6570\u636e\u62bd\u53d6\u9636\u6bb5\u5f15\u5165RMSD\u7b97\u6cd5\u4ee5\u51cf\u5c11\u5927\u6570\u636e\u5e93\u7684\u54cd\u5e94\u65f6\u95f4\uff1b\u5e76\u901a\u8fc7\u4eff\u771f\u5bf9\u6bd4\u9a8c\u8bc1\u65b0\u6846\u67b6\u4f18\u4e8e\u5148\u524d\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\uff0cFIDRS\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u9760\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u5148\u524d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u65e7\u6846\u67b6\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u548cRMSD\u7b97\u6cd5\u7684\u7ec4\u5408\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u540d\u4e3aFIDRS\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u5148\u524d\u6846\u67b6\u76f8\u6bd4\uff0cFIDRS\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u4e0a\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2510.11269", "categories": ["cs.NI", "cs.AI", "C.2.3; C.2.4; C.2.5"], "pdf": "https://arxiv.org/pdf/2510.11269", "abs": "https://arxiv.org/abs/2510.11269", "authors": ["Antonio Montieri", "Alfredo Nascita", "Antonio Pescap\u00e8"], "title": "From Prompts to Packets: A View from the Network on ChatGPT, Copilot, and Gemini", "comment": "13 pages, 8 figures, 2 tables, 4 research questions, preprint\n  submitted to Elsevier Computer Networks", "summary": "Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet\ntheir network traffic remains largely underexplored. This study presents an\nin-depth investigation of traffic generated by three leading chatbots (ChatGPT,\nCopilot, and Gemini) when accessed via Android mobile apps for both text and\nimage generation. Using a dedicated capture architecture, we collect and label\ntwo complementary workloads: a 60-hour generic dataset with unconstrained\nprompts, and a controlled dataset built from identical prompts across GenAI\napps and replicated via conventional messaging apps to enable one-to-one\ncomparisons. This dual design allows us to address practical research questions\non the distinctiveness of GenAI traffic, its differences from widely deployed\ntraffic categories, and its novel implications for network usage. To this end,\nwe provide fine-grained traffic characterization at trace, flow, and protocol\nlevels, and model packet-sequence dynamics with Multimodal Markov Chains. Our\nanalyses reveal app- and content-specific traffic patterns, particularly in\nvolume, uplink/downlink profiles, and protocol adoption. We highlight the\npredominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT\nexclusively using TLS 1.3, and app- and content-specific Server Name Indication\n(SNI) values. A payload-based occlusion analysis quantifies SNI's contribution\nto classification: masking it reduces F1-score by up to 20 percentage points in\nGenAI app traffic classification. Finally, compared with conventional messaging\napps when carrying the same content, GenAI chatbots exhibit unique traffic\ncharacteristics, highlighting new stress factors for mobile networks, such as\nsustained upstream activity, with direct implications for network monitoring\nand management. We publicly release the datasets to support reproducibility and\nfoster extensions to other use cases.", "AI": {"tldr": "Study captures and analyzes mobile GenAI chatbot network traffic (ChatGPT, Copilot, Gemini), revealing distinct traffic patterns, protocol use (TLS/QUIC), and the importance of SNI for classification; GenAI traffic differs from messaging apps and stresses networks with sustained uplink; datasets released.", "motivation": "GenAI chatbots are widespread but their network traffic is understudied; need to understand distinctiveness versus other traffic and implications for networks", "method": "Traffic characterization and modeling of mobile GenAI chatbots", "result": "Collected 60-hour generic and controlled datasets from Android apps (ChatGPT, Copilot, Gemini); performed trace/flow/protocol analyses; modeled packet sequences with Multimodal Markov Chains; found app- and content-specific patterns, TLS predominance, Gemini uses QUIC, ChatGPT TLS1.3 only; SNI important for classification", "conclusion": "GenAI chatbots generate distinctive, app-and-content-dependent traffic that impacts mobile network usage and monitoring; SNI is a key feature for classification and masking it degrades performance; datasets available for further research."}}
{"id": "2510.11211", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11211", "abs": "https://arxiv.org/abs/2510.11211", "authors": ["Sheikh Azizul Hakim", "Saem Hasan"], "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models", "comment": null, "summary": "Large language models (LLM) are advanced AI systems trained on extensive\ntextual data, leveraging deep learning techniques to understand and generate\nhuman-like language. Today's LLMs with billions of parameters are so huge that\nhardly any single computing node can train, fine-tune, or infer from them.\nTherefore, several distributed computing techniques are being introduced in the\nliterature to properly utilize LLMs. We have explored the application of\ndistributed computing techniques in LLMs from two angles.\n  \\begin{itemize}\n  \\item We study the techniques that democratize the LLM, that is, how large\nmodels can be run on consumer-grade computers. Here, we also implement a novel\nmetaheuristics-based modification to an existing system.\n  \\item We perform a comparative study on three state-of-the-art LLM serving\ntechniques. \\end{itemize}", "AI": {"tldr": "\u672c\u6587\u4ece\u8ba9LLM\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u53ef\u7528\u4e0e\u5bf9\u6bd4\u4e3b\u6d41\u670d\u52a1\u90e8\u7f72\u65b9\u6848\u4e24\u65b9\u9762\u7814\u7a76\u5206\u5e03\u5f0f\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5143\u542f\u53d1\u5f0f\u7684\u7cfb\u7edf\u6539\u8fdb\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u884c\u6027\u4e0e\u4e0d\u540c\u90e8\u7f72\u7b56\u7565\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524dLLM\u53c2\u6570\u89c4\u6a21\u5de8\u5927\uff0c\u5355\u8282\u70b9\u96be\u4ee5\u5b8c\u6210\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u63a8\u7406\uff0c\u8feb\u5207\u9700\u8981\u5206\u5e03\u5f0f\u548c\u8d44\u6e90\u8282\u7ea6\u7684\u65b9\u6848\uff0c\u4f7f\u66f4\u591a\u7528\u6237\u80fd\u591f\u4f7f\u7528LLM\u5e76\u63d0\u9ad8\u90e8\u7f72\u6548\u7387\u3002", "method": "\u4ece\u4e24\u6761\u4e3b\u7ebf\u5c55\u5f00\uff1a\u4e00\u662f\u7814\u7a76\u8ba9LLM\u5728\u6d88\u8d39\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u7684\u6280\u672f\uff0c\u5305\u542b\u6a21\u578b\u538b\u7f29\u3001\u5206\u6bb5\u52a0\u8f7d\u3001\u91cf\u5316\u3001\u6a21\u578b\u5e76\u884c\u4e0e\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u5e76\u5728\u5df2\u6709\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u6539\u8fdb\u4ee5\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff1b\u4e8c\u662f\u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\u670d\u52a1\u90e8\u7f72\u6280\u672f\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u4f7f\u7528\u6027\u80fd\u6307\u6807\uff08\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\uff09\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u91c7\u7528\u91cf\u5316\u4e0e\u5206\u6bb5\u52a0\u8f7d\u7b49\u7b56\u7565\u53ef\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u4f7f\u6a21\u578b\u5728\u6d88\u8d39\u7ea7GPU/CPU\u4e0a\u53ef\u7528\uff1b\u5143\u542f\u53d1\u5f0f\u6539\u8fdb\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u63d0\u5347\u4e86\u8c03\u5ea6/\u5206\u914d\u6548\u7387\uff1b\u4e09\u79cd\u670d\u52a1\u6280\u672f\u5728\u5ef6\u8fdf\u4e0e\u541e\u5410\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\uff0c\u4e14\u5bf9\u786c\u4ef6\u914d\u7f6e\u654f\u611f\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u9762\u5411\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u6280\u672f\uff0c\u5f97\u51fa\u7ed3\u8bba\uff1a\u901a\u8fc7\u9002\u5f53\u7684\u5206\u5e03\u5f0f\u7b56\u7565\u548c\u8f7b\u91cf\u5316\u6539\u8fdb\uff0c\u53ef\u4ee5\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u5927\u578b\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u90e8\u7f72\u65b9\u6848\u4e4b\u95f4\u5b9e\u73b0\u6743\u8861\u4ee5\u6ee1\u8db3\u6027\u80fd\u3001\u8d44\u6e90\u4e0e\u53ef\u7528\u6027\u9700\u6c42\u3002"}}
{"id": "2510.11513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11513", "abs": "https://arxiv.org/abs/2510.11513", "authors": ["Alex Elwood", "Tom Deakin", "Justin Lovegrove", "Chris Nelson"], "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems", "comment": null, "summary": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a\nchallenge to scale due to complex data dependencies, memory access patterns and\na high-dimensional domain. In this paper, we review the performance bottlenecks\nwithin the shared memory parallelization scheme of an existing transport solver\non modern many-core architectures with high core counts. With this analysis, we\nthen survey the performance of this solver across a variety of compute\nhardware. We then present a new Asynchronous Many-Task (AMT) algorithm for\nshared memory parallelism, present results showing an increase in computational\nperformance over the existing method, and evaluate why performance is improved.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86S_N\u8f93\u8fd0\u6c42\u89e3\u5668\u5728\u9ad8\u6838\u6570\u591a\u6838\u67b6\u6784\u4e0a\u7684\u5171\u4eab\u5185\u5b58\u5e76\u884c\u74f6\u9888\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u5f02\u6b65\u591a\u4efb\u52a1(AMT)\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u5206\u6790\u73b0\u6709\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u7684\u79bb\u6563\u65b9\u5411S_N\u8f93\u8fd0\u6c42\u89e3\u5668\u5728\u5171\u4eab\u5185\u5b58\u5e76\u884c\u5316\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u53ca\u5176\u5728\u591a\u6838\u67b6\u6784\u4e0a\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u5e76\u884c\u7b97\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6027\u80fd\u5206\u6790\u5de5\u5177\u6d4b\u91cf\u70ed\u70b9\u4e0e\u5185\u5b58\u5e26\u5bbd\u5360\u7528\u3001\u5256\u6790\u6570\u636e\u4f9d\u8d56\u548c\u4efb\u52a1\u7c92\u5ea6\uff0c\u6a2a\u5411\u6bd4\u8f83\u4e0d\u540c\u786c\u4ef6\u4e0a\u7684\u57fa\u7ebf\u5b9e\u73b0\uff0c\u7136\u540e\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u57fa\u4e8e\u4efb\u52a1\u56fe\u548c\u5f02\u6b65\u8c03\u5ea6\u7684AMT\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6027\u80fd\u6539\u8fdb\u4e0e\u539f\u56e0\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5206\u6790\u8bc6\u522b\u51fa\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u6570\u636e\u4f9d\u8d56\u3001\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u7ebf\u7a0b\u540c\u6b65\u5f00\u9500\u4e3a\u4e3b\u8981\u74f6\u9888\uff1b\u5728\u591a\u79cd\u786c\u4ef6\u4e0a\u5bf9\u73b0\u6709\u6c42\u89e3\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1b\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f02\u6b65\u591a\u4efb\u52a1(AMT)\u7684\u5171\u4eab\u5185\u5b58\u5e76\u884c\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u89e3\u91ca\u4e86\u6027\u80fd\u63d0\u5347\u7684\u539f\u56e0\u3002", "conclusion": "AMT\u5e76\u884c\u5316\u5728\u51cf\u5c0f\u540c\u6b65\u5f00\u9500\u3001\u6539\u5584\u8d1f\u8f7d\u5747\u8861\u548c\u63d0\u9ad8\u5185\u5b58\u5e76\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u5171\u4eab\u5185\u5b58\u5b9e\u65bd\uff0c\u4ece\u800c\u63d0\u5347\u4e86S_N\u6c42\u89e3\u5668\u5728\u73b0\u4ee3\u591a\u6838\u786c\u4ef6\u4e0a\u7684\u603b\u4f53\u6027\u80fd\u3002"}}
