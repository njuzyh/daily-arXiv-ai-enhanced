{"id": "2510.18525", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18525", "abs": "https://arxiv.org/abs/2510.18525", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing", "comment": null, "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.", "AI": {"tldr": "SPEQ\u662f\u4e00\u79cd\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u5b8c\u6574\u6a21\u578b\u7684\u90e8\u5206\u6743\u91cd\u4f4d\u6784\u5efa\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5b58\u50a8\u5f00\u9500\uff0c\u572815\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad82.07\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff1b\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u867d\u65e0\u635f\u4f46\u5e26\u6765\u989d\u5916\u5f00\u9500\u3002", "challenges": "\u5982\u4f55\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6216\u5b58\u50a8\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u65e0\u635f\u7684\u63a8\u7406\u52a0\u901f\u3002", "contributions": "\u63d0\u51faSPEQ\uff0c\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u90e8\u5206\u6743\u91cd\u4f4d\u7684\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u91cd\u6784\u5904\u7406\u5355\u5143\u9635\u5217\uff0c\u5b8c\u6210\u7b97\u6cd5\u4e0e\u786c\u4ef6\u7684\u534f\u540c\u4f18\u5316\u3002", "results": "\u572815\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\uff0cSPEQ\u76f8\u6bd4FP16\u3001Olive\u548cTender\u5206\u522b\u5b9e\u73b02.07x\u30011.53x\u548c1.45x\u7684\u52a0\u901f\u3002", "conclusion": "SPEQ\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u5927\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u7cbe\u5ea6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "related_work": "\u4e0e\u91cf\u5316\u65b9\u6cd5\u3001\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u4ee5\u53ca\u786c\u4ef6\u52a0\u901f\u5668\u76f8\u5173\u7684\u7814\u7a76\u5de5\u4f5c\u3002"}}
{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u89c4\u6a21\u5927\u6c14\u548c\u6d77\u6d0b\u6a21\u578b\u4ecePyTorch\u8fc1\u79fb\u81f3MindSpore\u5e76\u9488\u5bf9\u56fd\u4ea7\u82af\u7247\u8fdb\u884c\u4f18\u5316\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u8fd0\u884c\u6548\u7387\uff0c\u964d\u4f4e\u5bf9GPU\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u4e86\u6211\u56fd\u5728\u6c14\u8c61\u548c\u6c14\u5019AI\u6a21\u578b\u4e2d\u7684\u6280\u672f\u81ea\u4e3b\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u6c14\u5019\u4e0e\u5929\u6c14\u7814\u7a76\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u589e\u5f3a\uff0c\u5bf9\u9ad8\u6548\u6a21\u578b\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u9700\u6c42\u4e0d\u65ad\u4e0a\u5347\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56GPU\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u72ec\u7acb\u6027\uff0c\u5c24\u5176\u662f\u5728\u56fd\u4ea7\u786c\u4ef6\u548c\u6846\u67b6\u4e0a\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u652f\u6301\u56fd\u4ea7\u82af\u7247\u7684\u8fc1\u79fb\u4e0e\u4f18\u5316\u65b9\u6848\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\u8de8\u6846\u67b6\u6a21\u578b\u8fc1\u79fb\u7684\u517c\u5bb9\u6027\u95ee\u9898\u3001\u5728\u56fd\u4ea7\u82af\u7247\u4e0a\u7684\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u5b9e\u73b0\u3001\u5185\u5b58\u4f18\u5316\u4ee5\u53ca\u5728\u4e0d\u635f\u5931\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u8bad\u7ec3\u4e0e\u63a8\u7406\u901f\u5ea6\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u4ecePyTorch\u5230MindSpore\u7684\u6a21\u578b\u8fc1\u79fb\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u91cd\u70b9\u89e3\u51b3\u8f6f\u4ef6\u4e0e\u786c\u4ef6\u9002\u914d\u3001\u5185\u5b58\u4f18\u5316\u548c\u5e76\u884c\u8ba1\u7b97\u95ee\u9898\uff1b\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u56fd\u4ea7\u82af\u7247\u4e0a\u7684\u8bad\u7ec3\u901f\u5ea6\u3001\u63a8\u7406\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u80fd\u6548\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3aGPU\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fc1\u79fb\u540e\u7684\u6a21\u578b\u5728\u56fd\u4ea7\u82af\u7247\u4e0a\u4fdd\u6301\u4e86\u539f\u59cb\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u4f9d\u8d56\u6027\uff0c\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u4e14\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5229\u7528\u56fd\u4ea7\u82af\u7247\u548c\u6846\u67b6\u5f00\u5c55\u5927\u6c14\u4e0e\u6d77\u6d0bAI\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6211\u56fd\u5728\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u6280\u672f\u81ea\u4e3b\u80fd\u529b\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecFourCastNet\u548cAI-GOMS\u7b49\u57fa\u4e8eGPU\u7684AI\u6c14\u8c61\u6a21\u578b\uff0c\u4ee5\u53caPyTorch\u7b49\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.18300", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18300", "abs": "https://arxiv.org/abs/2510.18300", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"], "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces", "comment": null, "summary": "Large-scale GPU traces play a critical role in identifying performance\nbottlenecks within heterogeneous High-Performance Computing (HPC)\narchitectures. However, the sheer volume and complexity of a single trace of\ndata make performance analysis both computationally expensive and\ntime-consuming. To address this challenge, we present an end-to-end parallel\nperformance analysis framework designed to handle multiple large-scale GPU\ntraces efficiently. Our proposed framework partitions and processes trace data\nconcurrently and employs causal graph methods and parallel coordinating chart\nto expose performance variability and dependencies across execution flows.\nExperimental results demonstrate a 67% improvement in terms of scalability,\nhighlighting the effectiveness of our pipeline for analyzing multiple traces\nindependently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u901a\u8fc7\u5e76\u53d1\u5904\u7406\u548c\u56e0\u679c\u56fe\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\u5728\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e2d\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5355\u4e2a\u8ddf\u8e2a\u6570\u636e\u7684\u4f53\u79ef\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6027\u80fd\u5206\u6790\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "challenges": "\u5982\u4f55\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u3001\u590d\u6742\u7684GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u5e76\u5728\u591a\u6267\u884c\u6d41\u4e2d\u63ed\u793a\u6027\u80fd\u5dee\u5f02\u548c\u4f9d\u8d56\u5173\u7cfb\u3002", "contributions": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u652f\u6301\u5bf9\u591a\u4e2a\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\u8fdb\u884c\u5e76\u53d1\u5206\u533a\u5904\u7406\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u56fe\u65b9\u6cd5\u548c\u5e76\u884c\u534f\u8c03\u56fe\u6765\u66b4\u9732\u6027\u80fd\u53d8\u5f02\u6027\u4e0e\u4f9d\u8d56\u5173\u7cfb\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u63d0\u5347\u4e8667%\uff0c\u9a8c\u8bc1\u4e86\u5176\u72ec\u7acb\u5206\u6790\u591a\u4e2a\u8ddf\u8e2a\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\u5206\u6790\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5f02\u6784HPC\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8bca\u65ad\u3002", "related_work": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4e00\u8f68\u8ff9\u5206\u6790\u6216\u4e32\u884c\u5904\u7406\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5927\u89c4\u6a21GPU\u8f68\u8ff9\u7684\u5e76\u884c\u5316\u652f\u6301\u548c\u7cfb\u7edf\u7ea7\u6027\u80fd\u4f9d\u8d56\u5efa\u6a21\u3002"}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u7684\u65b0\u578b\u8c03\u5ea6\u65b9\u6848SLICE\uff0c\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4e2d\u5dee\u5f02\u5316\u7684\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347SLO\u8fbe\u6210\u7387\u5e76\u7f29\u77ed\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u5bf9\u5ef6\u8fdf\u6781\u4e3a\u654f\u611f\uff0c\u5177\u6709\u591a\u6837\u5316\u7684SLO\u9700\u6c42\uff08\u5982TTFT\u3001TPOT\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\uff09\uff0c\u4f46\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u4ec5\u4ee5\u6700\u5927\u5316\u541e\u5410\u91cf\u4e3a\u76ee\u6807\uff0c\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u5dee\u5f02\u5316\u9700\u6c42\uff0c\u5bfc\u81f4SLO\u8fdd\u89c4\u7387\u9ad8\u3002", "challenges": "\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684SLO\u8981\u6c42\u52a8\u6001\u8c03\u6574\u751f\u6210\u901f\u7387\u5e76\u5408\u7406\u8c03\u5ea6\u8bf7\u6c42\uff0c\u540c\u65f6\u517c\u987e\u7cfb\u7edf\u6548\u7387\u4e0e\u670d\u52a1\u8d28\u91cf\uff0c\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "contributions": "\u63d0\u51fa\u4e86SLICE\u8c03\u5ea6\u6846\u67b6\uff0c\u7ed3\u5408\u6548\u7528\u6700\u5927\u5316\u7684\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u4e0e\u751f\u6210\u901f\u7387\u7684\u52a8\u6001\u8fed\u4ee3\u63a7\u5236\u673a\u5236\uff0c\u9996\u6b21\u5728\u8fb9\u7f18LLM\u670d\u52a1\u4e2d\u5b9e\u73b0\u5bf9\u591a\u6837\u5316SLO\u7684\u7cbe\u7ec6\u5316\u652f\u6301\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0eOrca\u548cFastServe\u76f8\u6bd4\uff0cSLICE\u7684SLO\u8fbe\u6210\u7387\u6700\u9ad8\u63d0\u534735\u500d\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed3.4\u500d\u3002", "conclusion": "SLICE\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bf7\u6c42\u8c03\u5ea6\u4e0e\u751f\u6210\u901f\u7387\u63a7\u5236\uff0c\u5728\u8fb9\u7f18LLM\u670d\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u591a\u6837\u5316SLO\u7684\u6ee1\u8db3\u80fd\u529b\uff0c\u4e3a\u9762\u5411\u5b9e\u65f6\u4ea4\u4e92\u7684\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u63a8\u7406\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u73b0\u6709\u5de5\u4f5c\u5982Orca\u548cFastServe\u4e3b\u8981\u5173\u6ce8\u63d0\u9ad8\u8f93\u51fa\u4ee4\u724c\u541e\u5410\u91cf\uff0c\u7f3a\u4e4f\u5bf9\u5dee\u5f02\u5316SLO\u7684\u652f\u6301\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u8fb9\u7f18\u573a\u666f\u4e0b\u5b9e\u65f6\u4efb\u52a1\u7684\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u3002"}}
{"id": "2510.18586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18586", "abs": "https://arxiv.org/abs/2510.18586", "authors": ["Zhuohang Bian", "Feiyang Wu", "Teng Ma", "Youwei Zhuo"], "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.", "AI": {"tldr": "Tokencake\u662f\u4e00\u79cd\u9762\u5411\u591a\u667a\u80fd\u4f53\u5e94\u7528\u7684KV\u7f13\u5b58\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5185\u5b58\u5212\u5206\u548c\u9884\u6d4b\u6027\u6570\u636e\u5378\u8f7d\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347GPU\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\uff0c\u5916\u90e8\u51fd\u6570\u8c03\u7528\u5bfc\u81f4KV\u7f13\u5b58\u9762\u4e34\u7a7a\u95f4\u4e89\u7528\u548c\u65f6\u95f4\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002", "challenges": "1\uff09\u591a\u4e2a\u667a\u80fd\u4f53\u5171\u4eabKV\u7f13\u5b58\u65f6\u4ea7\u751f\u7a7a\u95f4\u7ade\u4e89\uff0c\u5173\u952e\u667a\u80fd\u4f53\u7684\u7f13\u5b58\u53ef\u80fd\u88ab\u9519\u8bef\u9a71\u9010\uff1b2\uff09\u667a\u80fd\u4f53\u5728\u7b49\u5f85\u957f\u65f6\u5de5\u5177\u8c03\u7528\u671f\u95f4\uff0c\u5176KV\u7f13\u5b58\u4ecd\u5360\u7528GPU\u5185\u5b58\uff0c\u9020\u6210\u8d44\u6e90\u95f2\u7f6e\u3002", "contributions": "\u63d0\u51faTokencake\uff0c\u4e00\u79cd\u4ee5KV\u7f13\u5b58\u4e3a\u4e2d\u5fc3\u7684\u670d\u52a1\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1\u7a7a\u95f4\u8c03\u5ea6\u5668\uff0c\u91c7\u7528\u52a8\u6001\u5185\u5b58\u5206\u533a\u4fdd\u62a4\u5173\u952e\u667a\u80fd\u4f53\uff1b2\uff09\u8bbe\u8ba1\u65f6\u95f4\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u4e3b\u52a8\u5378\u8f7d\u548c\u9884\u6d4b\u52a0\u8f7d\u673a\u5236\uff0c\u5728\u5de5\u5177\u8c03\u7528\u505c\u6ede\u671f\u95f4\u91cd\u65b0\u5229\u7528GPU\u5185\u5b58\u3002", "results": "\u5728\u5178\u578b\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTokencake\u76f8\u6bd4vLLM\u53ef\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e47.06%\uff0c\u6709\u6548GPU\u5185\u5b58\u5229\u7528\u7387\u63d0\u5347\u8fbe16.9%\u3002", "conclusion": "Tokencake\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8c03\u5ea6\u4e0e\u5185\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53LLM\u5e94\u7528\u7684\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86KV\u7f13\u5b58\u4e2d\u5fc3\u5316\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecvLLM\u7b49KV\u7f13\u5b58\u7ba1\u7406\u65b9\u6848\uff0c\u4f46\u5b83\u4eec\u672a\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e0b\u7684\u7a7a\u95f4\u4e89\u7528\u548c\u65f6\u95f4\u6d6a\u8d39\u95ee\u9898\u8fdb\u884c\u4e13\u95e8\u4f18\u5316\u3002"}}
{"id": "2510.18592", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18592", "abs": "https://arxiv.org/abs/2510.18592", "authors": ["Yuval Gil", "Merav Parter"], "title": "Distributed Interactive Proofs for Planarity with Log-Star Communication", "comment": "To appear in SODA 26", "summary": "We provide new communication-efficient distributed interactive proofs for\nplanarity. The notion of a \\emph{distributed interactive proof (DIP)} was\nintroduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \\emph{prover}\nis a single centralized entity whose goal is to prove a certain claim regarding\nan input graph $G$. To do so, the prover communicates with a distributed\n\\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is\nmeasured by the amount of prover-verifier communication it requires. Namely,\nthe goal is to design a DIP with a small number of interaction rounds and a\nsmall \\emph{proof size}, i.e., a small amount of communication per round. Our\nmain result is an $O(\\log ^{*}n)$-round DIP protocol for embedded planarity and\nplanarity with a proof size of $O(1)$ and $O(\\lceil\\log \\Delta/\\log\n^{*}n\\rceil)$, respectively. In fact, this result can be generalized as\nfollows. For any $1\\leq r\\leq \\log^{*}n$, there exists an $O(r)$-round protocol\nfor embedded planarity and planarity with a proof size of $O(\\log ^{(r)}n)$ and\n$O(\\log ^{(r)}n+\\log \\Delta /r)$, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u5f0f\u8bc1\u660e\u534f\u8bae\uff0c\u7528\u4e8e\u56fe\u7684\u5e73\u9762\u6027\u9a8c\u8bc1\uff0c\u5177\u6709\u8f83\u5c11\u7684\u901a\u4fe1\u8f6e\u6570\u548c\u8f83\u5c0f\u7684\u8bc1\u660e\u5927\u5c0f\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u56fe\u5c5e\u6027\uff08\u5982\u5e73\u9762\u6027\uff09\u6240\u9700\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u5f0f\u8bc1\u660e\uff08DIP\uff09\u534f\u8bae\u3002", "challenges": "\u5982\u4f55\u5728\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6700\u5c0f\u5316\u8bc1\u660e\u8fc7\u7a0b\u4e2d\u7684\u901a\u4fe1\u8f6e\u6570\u548c\u6bcf\u8f6e\u7684\u8bc1\u660e\u5927\u5c0f\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u56fe\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9a8c\u8bc1\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u4e2aO(log*n)\u8f6e\u7684DIP\u534f\u8bae\uff0c\u7528\u4e8e\u5d4c\u5165\u5f0f\u5e73\u9762\u6027\u548c\u4e00\u822c\u5e73\u9762\u6027\u9a8c\u8bc1\uff0c\u5206\u522b\u5177\u6709O(1)\u548cO(\u2308log\u0394/log*n\u2309)\u7684\u8bc1\u660e\u5927\u5c0f\uff1b\u5e76\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5230\u5bf9\u4e8e\u4efb\u610f1\u2264r\u2264log*n\uff0c\u5b58\u5728O(r)\u8f6e\u534f\u8bae\uff0c\u5177\u6709\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u6548\u7387\u6743\u8861\u3002", "results": "\u5b9e\u73b0\u4e86O(log*n)\u8f6e\u5185\u5b8c\u6210\u5e73\u9762\u6027\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5927\u5c0f\u4ec5\u4e3a\u5e38\u6570\u6216\u5bf9\u6570\u7ea7\u522b\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6848\uff1b\u5e76\u901a\u8fc7\u53c2\u6570\u5316\u8f6e\u6570r\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6548\u7387\u6298\u4e2d\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u4ea4\u4e92\u8f6e\u6570\uff0c\u53ef\u4ee5\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u4ee5\u6781\u4f4e\u7684\u901a\u4fe1\u6210\u672c\u9a8c\u8bc1\u56fe\u7684\u5e73\u9762\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u8bc1\u660e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u6784\u9020\u65b9\u6cd5\u3002", "related_work": "\u57fa\u4e8eKol\u3001Oshman\u548cSaxena\u5728PODC 2018\u5e74\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\u5f0f\u8bc1\u660e\uff08DIP\uff09\u6846\u67b6\uff0c\u672c\u6587\u5728\u5176\u57fa\u7840\u4e0a\u4f18\u5316\u4e86\u901a\u4fe1\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.18640", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18640", "abs": "https://arxiv.org/abs/2510.18640", "authors": ["Nils Japke", "Sebastian Koch", "Helmut Lukasczyk", "David Bermbach"], "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines", "comment": "Published in 2025 IEEE International Conference on Cloud Engineering\n  (IC2E)", "summary": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u9891\u7e41\u8fdb\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u68c0\u6d4b\u6027\u80fd\u9000\u6b65\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u57fa\u51c6\u4f18\u5316\u6280\u672f\u7f3a\u4e4f\u5728\u5b9e\u9645CI/CD\u6d41\u6c34\u7ebf\u4e2d\u7684\u96c6\u6210\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u4e09\u5927\u6311\u6218\uff1a\u57fa\u51c6\u4f18\u5316\u7b56\u7565\u7684\u53ef\u7ec4\u5408\u6027\u3001\u7ed3\u679c\u7684\u81ea\u52a8\u8bc4\u4f30\u4ee5\u53ca\u5728\u5b9e\u8df5\u4e2d\u5e94\u7528\u7684\u53ef\u7528\u6027\u4e0e\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6027\u7684\u4e91\u57fa\u51c6\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "motivation": "\u6027\u80fd\u9000\u6b65\u4f1a\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u5f71\u54cd\u670d\u52a1\u7ea7\u522b\u534f\u8bae\uff08SLA\uff09\uff0c\u56e0\u6b64\u9700\u8981\u5728CI/CD\u6d41\u7a0b\u4e2d\u9891\u7e41\u6267\u884c\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002\u7136\u800c\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u8017\u65f6\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u6301\u7eed\u96c6\u6210\u4e2d\u7684\u5e94\u7528\u3002", "challenges": "\uff08a\uff09\u4e0d\u540c\u57fa\u51c6\u4f18\u5316\u7b56\u7565\u4e4b\u95f4\u7684\u53ef\u7ec4\u5408\u6027\u5dee\uff1b\uff08b\uff09\u7f3a\u4e4f\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u673a\u5236\uff1b\uff08c\uff09\u73b0\u6709\u5de5\u5177\u5728CI/CD\u7cfb\u7edf\u4e2d\u4f7f\u7528\u65f6\u7684\u53ef\u7528\u6027\u4f4e\u548c\u64cd\u4f5c\u590d\u6742\u3002", "contributions": "\u672c\u6587\u63d0\u51fa\u4e86\u5728CI/CD\u4e2d\u5b9e\u73b0\u9ad8\u6548\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u7684\u4e91\u57fa\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u900f\u660e\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "results": "\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\uff0c\u4f46\u5c1a\u672a\u5b9e\u73b0\u5177\u4f53\u7cfb\u7edf\u6216\u5b9e\u9a8c\u7ed3\u679c\uff0c\u5c5e\u4e8e\u613f\u666f\u8bba\u6587\u3002", "conclusion": "\u8981\u5b9e\u73b0\u9ad8\u6548\u3001\u9891\u7e41\u7684\u6027\u80fd\u56de\u5f52\u68c0\u6d4b\uff0c\u5fc5\u987b\u89e3\u51b3\u57fa\u51c6\u4f18\u5316\u7b56\u7565\u7684\u53ef\u7ec4\u5408\u6027\u3001\u7ed3\u679c\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u5b9e\u9645\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4e91\u539f\u751f\u57fa\u51c6\u6846\u67b6\u662f\u672a\u6765\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u79cd\u52a0\u901f\u57fa\u51c6\u6267\u884c\u7684\u4f18\u5316\u6280\u672f\uff0c\u5982\u91c7\u6837\u3001\u8df3\u8fc7\u65e0\u5173\u6d4b\u8bd5\u3001\u5e76\u884c\u6267\u884c\u7b49\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u6280\u672f\u6574\u5408\u5230\u771f\u5b9eCI/CD\u73af\u5883\u4e2d\u7684\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18838", "categories": ["cs.DC", "physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.18838", "abs": "https://arxiv.org/abs/2510.18838", "authors": ["Jacob S. Merson", "Cameron W. Smith", "Mark S. Shephard", "Fuad Hasan", "Abhiyan Paudel", "Angel Castillo-Crooke", "Joyal Mathew", "Mohammad Elahi"], "title": "PCMS: Parallel Coupler For Multimodel Simulations", "comment": null, "summary": "This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a\nnew GPU accelerated generalized coupling framework for coupling simulation\ncodes on leadership class supercomputers. PCMS includes distributed control and\nfield mapping methods for up to five dimensions. For field mapping PCMS can\nutilize discretization and field information to accommodate physics\nconstraints. PCMS is demonstrated with a coupling of the gyrokinetic\nmicroturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and\nwith a 5D distribution function coupling of an energetic particle transport\ncode (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also\ndemonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of\n85%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PCMS\uff0c\u4e00\u79cd\u7528\u4e8e\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8026\u5408\u6a21\u62df\u4ee3\u7801\u7684GPU\u52a0\u901f\u901a\u7528\u8026\u5408\u6846\u67b6\uff0c\u652f\u6301\u6700\u591a\u4e94\u7ef4\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u573a\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u5728Frontier\u76842,080\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e8685%\u7684\u5f31\u6269\u5c55\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u9886\u5bfc\u7ea7\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u591a\u6a21\u578b\u8026\u5408\u6a21\u62df\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7269\u7406\u573a\u548c\u591a\u7ef4\u6570\u636e\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5145\u5206\u5229\u7528GPU\u52a0\u901f\u80fd\u529b\u7684\u901a\u7528\u8026\u5408\u6846\u67b6\u3002", "challenges": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u901a\u7528\u8026\u5408\u6846\u67b6\uff0c\u540c\u65f6\u652f\u6301\u591a\u7ef4\u573a\u6620\u5c04\u548c\u7269\u7406\u7ea6\u675f\u7684\u5904\u7406\uff0c\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002", "contributions": "\u63d0\u51fa\u4e86PCMS\u6846\u67b6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u591a\u7ef4\u573a\u6620\u5c04\uff0c\u80fd\u591f\u5229\u7528\u79bb\u6563\u5316\u548c\u573a\u4fe1\u606f\u6765\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "results": "PCMS\u5728Frontier\u76842,080\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e8685%\u7684\u5f31\u6269\u5c55\u6548\u7387\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eXGC\u4e0eDEGAS2\u7684\u8026\u5408\u4ee5\u53caGNET\u4e0eGTC\u76845D\u5206\u5e03\u51fd\u6570\u8026\u5408\u3002", "conclusion": "PCMS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684GPU\u52a0\u901f\u8026\u5408\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u6a21\u578b\u6a21\u62df\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7269\u7406\u573a\u548c\u591a\u7ef4\u6570\u636e\u65f6\u8868\u73b0\u51fa\u8272\u3002", "related_work": "\u73b0\u6709\u7684\u8026\u5408\u6846\u67b6\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u6216\u7ef4\u5ea6\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u9ad8\u6548\u7684GPU\u52a0\u901f\u652f\u6301\uff0c\u800cPCMS\u901a\u8fc7\u5f15\u5165\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u591a\u7ef4\u573a\u6620\u5c04\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002"}}
