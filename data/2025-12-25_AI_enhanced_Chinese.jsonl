{"id": "2512.19842", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19842", "abs": "https://arxiv.org/abs/2512.19842", "authors": ["Andrea Sordello", "Marco Mellia", "Idilio Drago", "Rodolfo Valentim", "Francesco Musumeci", "Massimo Tornatore", "Federico Cerutti", "Martino Trevisan", "Alessio Botta", "Willen Borges Coelho"], "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform", "comment": null, "summary": "The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19849", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.19849", "abs": "https://arxiv.org/abs/2512.19849", "authors": ["Ziming Mao", "Yihan Zhang", "Chihan Cui", "Kaichao You", "Zhongjie Chen", "Zhiying Xu", "Scott Shenker", "Costin Raiciu", "Yang Zhou", "Ion Stoica"], "title": "UCCL-EP: Portable Expert-Parallel Communication", "comment": null, "summary": "Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19851", "abs": "https://arxiv.org/abs/2512.19851", "authors": ["Aditya Bhosale", "Laxmikant Kale"], "title": "An Adaptive Distributed Stencil Abstraction for GPUs", "comment": null, "summary": "The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19972", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19972", "abs": "https://arxiv.org/abs/2512.19972", "authors": ["Pengchao Han", "Xi Huang", "Yi Fang", "Guojun Han"], "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions", "comment": "Published in IEEE TNSE", "summary": "Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20073", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20073", "abs": "https://arxiv.org/abs/2512.20073", "authors": ["Hongyang Shang", "Shuai Dong", "Ye Ke", "Arindam Basu"], "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras", "comment": null, "summary": "This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19696", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.19696", "abs": "https://arxiv.org/abs/2512.19696", "authors": ["Sebastian Racedo", "Brigitte Jaumard", "Oscar Delgado", "Meysam Masoudi"], "title": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning", "comment": null, "summary": "Open Radio Access Network (O RAN) disaggregates conventional RAN into interoperable components, enabling flexible resource allocation, energy savings, and agile architectural design. In legacy deployments, the binding between logical functions and physical locations is static, which leads to inefficiencies under time varying traffic and resource conditions. We address this limitation by relaxing the fixed mapping and performing dynamic service function chain (SFC) provisioning with on the fly O CU selection. We formulate the problem as a Markov decision process and solve it using GRLDyP, i.e., a graph neural network (GNN) assisted deep reinforcement learning (DRL). The proposed agent jointly selects routes and the O-CU location (from candidate sites) for each incoming service flow to minimize network energy consumption while satisfying quality of service (QoS) constraints. The GNN encodes the instantaneous network topology and resource utilization (e.g., CPU and bandwidth), and the DRL policy learns to balance grade of service, latency, and energy. We perform the evaluation of GRLDyP on a data set with 24-hour traffic traces from the city of Montreal, showing that dynamic O CU selection and routing significantly reduce energy consumption compared to a static mapping baseline, without violating QoS. The results highlight DRL based SFC provisioning as a practical control primitive for energy-aware, resource-adaptive O-RAN deployments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20017", "categories": ["cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.20017", "abs": "https://arxiv.org/abs/2512.20017", "authors": ["Hexu Zhao", "Xiaoteng Liu", "Xiwen Min", "Jianhao Huang", "Youming Deng", "Yanfei Li", "Ang Li", "Jinyang Li", "Aurojit Panda"], "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction", "comment": "13 pages main text, plus appendix", "summary": "Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20198", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.20198", "abs": "https://arxiv.org/abs/2512.20198", "authors": ["Huizheng Wang", "Taiquan Wei", "Hongbin Wang", "Zichuan Wang", "Xinru Tang", "Zhiheng Yue", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling", "comment": "Accepted for publication in IEEE Transactions on Computers", "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19697", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.19697", "abs": "https://arxiv.org/abs/2512.19697", "authors": ["Parsa Hatami", "Ahmadreza Majlesara", "Ali Majlesi", "Babak Hossein Khalaj"], "title": "Automated Fault Detection in 5G Core Networks Using Large Language Models", "comment": null, "summary": "With the rapid growth of data volume in modern telecommunication networks and the continuous expansion of their scale, maintaining high reliability has become a critical requirement. These networks support a wide range of applications and services, including highly sensitive and mission-critical ones, which demand rapid and accurate detection and resolution of network errors. Traditional fault-diagnosis methods are no longer efficient for such complex environments.\\cite{b1} In this study, we leverage Large Language Models (LLMs) to automate network fault detection and classification. Various types of network errors were intentionally injected into a Kubernetes-based test network, and data were collected under both healthy and faulty conditions. The dataset includes logs from different network components (pods), along with complementary data such as system descriptions, events, Round Trip Time (RTT) tests, and pod status information. The dataset covers common fault types such as pod failure, pod kill, network delay, network loss, and disk I/O failures. We fine-tuned the GPT-4.1 nano model via its API on this dataset, resulting in a significant improvement in fault-detection accuracy compared to the base model. These findings highlight the potential of LLM-based approaches for achieving closed-loop, and operator-free fault management, which can enhance network reliability and reduce downtime-related operational costs for service providers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20064", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20064", "abs": "https://arxiv.org/abs/2512.20064", "authors": ["Yaojian Chen", "Si-Qiu Gong", "Lin Gan", "Yanfei Liu", "An Yang", "Yinuo Wang", "Chao-yang Lu", "Guangwen Yang"], "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling", "comment": "12 pages, 13 figures", "summary": "Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20495", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20495", "abs": "https://arxiv.org/abs/2512.20495", "authors": ["He Zhu", "Zheng Liu", "Xingyang Li", "Anbang Wu", "Jieru Zhao", "Fangxin Liu", "Yiming Gan", "Jingwen Leng", "Yu Feng"], "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization", "comment": null, "summary": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.\n  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19698", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.19698", "abs": "https://arxiv.org/abs/2512.19698", "authors": ["Yejin Cho", "John Heidemann"], "title": "Smoothing Rough Edges of IPv6 in VPNs", "comment": null, "summary": "How do commercial VPNs interact with IPv6? We show two \"rough edges\" in how commercial VPNs handle IPv6. First, we show that many IPv4-only VPNs leak IPv6 traffic to the ISP. Individual use VPNs in part to conceal their local IP addresses, so such leaks reduce user privacy. While prior work has studied VPNs in testbeds, we use a new dataset of 129k VPN-using daily visitors to WhatIsMyIPAddress.com that quantifies these leaks and show 12 VPNs previously considered safe still leak for at least 5% of their users. We show native IPv6 addresses leak most commonly in VPNs that claim only IPv4 support, with 5% to 57% of visitors of v4-only VPNs having their native IPv6 address exposed. Second, we show that most dual-stack VPNs users actually select IPv4 instead of IPv6. We observe this problem in our visitor data, and we identify the root cause arises because when user's computer follows standard address-selection rules, VPN-assigned addresses are often de-preferenced. Testing six VPNs on Android, we show that five consistently de-prioritize IPv6. Finally, we suggest a solution to IPv6 de-preferencing: we define a new IPv6 address range for VPNs that is not de-preferenced by address selection. We prototype this solution on Linux. Our findings help identify and address rough edges in the addition of IPv6 support to VPNs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20163", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20163", "abs": "https://arxiv.org/abs/2512.20163", "authors": ["Leszek G\u0105sieniec", "Tytus Grodzicki", "Tomasz Jurdzi\u0144ski", "Jakub Kowalski", "Grzegorz Stachowiak"], "title": "Population Protocols Revisited: Parity and Beyond", "comment": null, "summary": "For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.\n  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\\log^3 n)$ states and achieve silent stabilisation in $O(\\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20571", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20571", "abs": "https://arxiv.org/abs/2512.20571", "authors": ["Brennan Romero", "D. G. Perera"], "title": "Composing Mini Oscilloscope on Embedded Systems", "comment": "22 pages, 11 figures", "summary": "In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20178", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.20178", "abs": "https://arxiv.org/abs/2512.20178", "authors": ["Chen Zhuang", "Lingqi Zhang", "Benjamin Brock", "Du Wu", "Peng Chen", "Toshio Endo", "Satoshi Matsuoka", "Mohamed Wahib"], "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication", "comment": "Under Review", "summary": "Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \\method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\\times$, 56.0$\\times$, 23.4$\\times$, and 8.8$\\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.19964", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.19964", "abs": "https://arxiv.org/abs/2512.19964", "authors": ["Bruno E. Farias", "Jos\u00e9 Flauzino", "Elias P. Duarte"], "title": "VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization", "comment": null, "summary": "With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20184", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20184", "abs": "https://arxiv.org/abs/2512.20184", "authors": ["Chaoyi Ruan", "Yiliang Wang", "Ziji Shi", "Jialin Li"], "title": "Reaching Agreement Among Reasoning LLM Agents", "comment": null, "summary": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20080", "categories": ["cs.NI", "cs.AI", "physics.optics"], "pdf": "https://arxiv.org/pdf/2512.20080", "abs": "https://arxiv.org/abs/2512.20080", "authors": ["Dianxuan Fu", "Xiaomin Liu", "Yihao Zhang", "Shikui Shen", "Weisheng Hu", "Qunbi Zhuge"], "title": "CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks", "comment": null, "summary": "We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20210", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20210", "abs": "https://arxiv.org/abs/2512.20210", "authors": ["Yinan Ni", "Xiao Yang", "Yuqi Tang", "Zhimin Qiu", "Chen Wang", "Tingzhou Yuan"], "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs", "comment": null, "summary": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20186", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.20186", "abs": "https://arxiv.org/abs/2512.20186", "authors": ["Liang Wang"], "title": "Edge-Served Congestion Control for Wireless Multipath Transmission with a Transformer Agent", "comment": null, "summary": "Multipath TCP is widely adopted to enhance connection quality-of-service by leveraging multiple network pathways on modern devices. However, the evolution of its core congestion control is hindered by the OS kernel, whose monolithic design imposes high development overhead and lacks the resource flexibility required for data-driven methods. Furthermore, inherent noise in network statistics induces a partial observability problem, which can mislead data-driven methods like Deep Reinforcement Learning. To bridge this gap, we propose Jazz, a system that re-architects multipath congestion control through a decoupled architecture that separates the decision-making ``brain'' from the in-kernel datapath, enabling it to operate on an external (edge) entity. At its core, Jazz employs a Transformer-based agent that processes sequences of historical observations to overcome the partial observability of single-step reinforcement learning. This allows it to learn and master fluctuating link conditions and intricate cross-path dependencies. Tested on a dual-band (5GHz/6GHz) Wi-Fi testbed, our implementation improves bandwidth efficiency by at least 2.85\\% over conventional methods and maintains 96.2\\% performance under 1\\% packet loss, validating this design as a practical blueprint for agile network intelligence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20394", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20394", "abs": "https://arxiv.org/abs/2512.20394", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults", "comment": null, "summary": "As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2512.20485", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20485", "abs": "https://arxiv.org/abs/2512.20485", "authors": ["Tanisha Fonseca", "Gengrui Zhang"], "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient", "comment": null, "summary": "Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
