{"id": "2602.13789", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13789", "abs": "https://arxiv.org/abs/2602.13789", "authors": ["Zhengyan Chu"], "title": "TEG: Exascale Cluster Governance via Non-Equilibrium Thermodynamics and Langevin Dynamics", "comment": "7 pages", "summary": "As cloud computing scales toward the Exascale regime ($10^5+$ nodes), the prevailing \"Newtonian\" orchestration paradigm -- exemplified by Kubernetes -- approaches fundamental physical limits. The centralized, deterministic scheduling model suffers from $O(N)$ latency scaling, \"Head-of-Line\" blocking, and thermodynamic blindness, rendering it incapable of managing the stochastic chaos of next-generation AI workloads. This paper proposes a paradigm shift from orchestration to Thermodynamic Governance. We model the compute cluster not as a static state machine, but as a Dissipative Structure far from equilibrium. We introduce TEG (Thermo-Economic Governor), a decentralized architecture that establishes a rigorous topological isomorphism between cluster resource contention and many-body physics. TEG replaces the global scheduler with Langevin Agents that execute Brownian motion on a Holographic Potential Field, reducing decision complexity to $O(1)$. System stability is maintained via a macro-scale Landau Phase Transition mechanism, which modulates global damping (taxation) to physically dissolve deadlocks. Crucially, we enforce Token Evaporation to mirror entropy dissipation, preventing economic inflation and ensuring an open thermodynamic system. We provide formal theoretical analysis proving that: (1) The system converges asymptotically to a Nash Equilibrium via Dual-Number Damping; (2) OOM catastrophic failures are converted into manageable Glassy States via an OS-level Airlock Mutex; and (3) Safety is mathematically guaranteed under high inertia using High-Order Control Barrier Functions (HOCBF). TEG demonstrates that emergent order, rather than deterministic control, is the necessary condition for Exascale scalability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14107", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14107", "abs": "https://arxiv.org/abs/2602.14107", "authors": ["Yuze Liu", "Shibo Chu", "Tiehua Zhang", "Hao Zhou", "Zhishu Shen", "Jinze Wang", "Jianzhong Qi", "Feng Xia"], "title": "ML-ECS: A Collaborative Multimodal Learning Framework for Edge-Cloud Synergies", "comment": null, "summary": "Edge-cloud synergies provide a promising paradigm for privacy-preserving deployment of foundation models, where lightweight on-device models adapt to domain-specific data and cloud-hosted models coordinate knowledge sharing. However, in real-world edge environments, collaborative multimodal learning is challenged by modality heterogeneity (different modality combinations across domains) and model-structure heterogeneity (different modality-specific encoders/fusion modules. To address these issues, we propose ML-ECS, a collaborative multimodal learning framework that enables joint training between a server-based model and heterogeneous edge models. This framework consists of four components: (1) cross-modal contrastive learning (CCL) to align modality representations in a shared latent space, (2) adaptive multimodal tuning (AMT) to preserve domain-specific knowledge from local datasets, (3) modality-aware model aggregation (MMA) to robustly aggregate while mitigating noise caused by missing modalities, and (4) SLM-enhanced CCL (SE-CCL) to facilitate bidirectional knowledge transfer between cloud and edge. Experimental results on various multimodal tasks show that \\pname consistently outperform state-of-the-art baselines under varying modality availability, achieving improvements of 5.44% to 12.08% in Rouge-LSum and improving both client- and server-side performance. In addition, by communicating only low-rank LoRA parameters and fused representations, ML-ECS achieves high communication efficiency, requiring only 0.65% of the total parameter volume.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14302", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14302", "abs": "https://arxiv.org/abs/2602.14302", "authors": ["Chunlin Tian", "Kahou Tam", "Yebo Wu", "Shuaihang Zhong", "Li Li", "Nicholas D. Lane", "Chengzhong Xu"], "title": "Floe: Federated Specialization for Real-Time LLM-SLM Inference", "comment": "Accepted by IEEE Transactions on Parallel and Distributed Systems", "summary": "Deploying large language models (LLMs) in real-time systems remains challenging due to their substantial computational demands and privacy concerns. We propose Floe, a hybrid federated learning framework designed for latency-sensitive, resource-constrained environments. Floe combines a cloud-based black-box LLM with lightweight small language models (SLMs) on edge devices to enable low-latency, privacy-preserving inference. Personal data and fine-tuning remain on-device, while the cloud LLM contributes general knowledge without exposing proprietary weights. A heterogeneity-aware LoRA adaptation strategy enables efficient edge deployment across diverse hardware, and a logit-level fusion mechanism enables real-time coordination between edge and cloud models. Extensive experiments demonstrate that Floe enhances user privacy and personalization. Moreover, it significantly improves model performance and reduces inference latency on edge devices under real-time constraints compared with baseline approaches.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14516", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.14516", "abs": "https://arxiv.org/abs/2602.14516", "authors": ["Wenhao He", "Youhe Jiang", "Penghao Zhao", "Quanqing Xu", "Eiko Yoneki", "Bin Cui", "Fangcheng Fu"], "title": "Efficient Multi-round LLM Inference over Disaggregated Serving", "comment": null, "summary": "With the rapid evolution of Large Language Models (LLMs), multi-round workflows, such as autonomous agents and iterative retrieval, have become increasingly prevalent. However, this raises hurdles for serving LLMs under prefill-decode (PD) disaggregation, a widely adopted paradigm that separates the compute-bound prefill phase and memory-bound decode phase onto individual resources. Specifically, existing systems overlook the interleaved prefill-decode workload pattern in multi-round inference, leading to sub-optimal handling of the incremental prefill workloads and model deployment for the two phases.\n  In this work, we present AMPD, a brand new disaggregated serving framework for multi-round LLM inference. The core of AMPD is to coordinate the prefill workloads based on real-time workloads by adaptively determining where to carry out these workloads and how they are scheduled, in order to maximize service level objective (SLO) attainment. In addition, we tailor a planning algorithm for our scenario, facilitating the deduction of optimal resource allocation and parallel strategies for the two phases. Empirical results demonstrate that AMPD substantially improves SLO attainment compared to state-of-the-art baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13434", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.13434", "abs": "https://arxiv.org/abs/2602.13434", "authors": ["Maccoy Merrell", "Daniel Puckett", "Gino Chacon", "Jeffrey Stuecheli", "Stavros Kalafatis", "Paul V. Gratz"], "title": "ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory", "comment": "15 pages, 19 figures", "summary": "Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13199", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13199", "abs": "https://arxiv.org/abs/2602.13199", "authors": ["Andrii Grekhov", "Volodymyr Kharchenko", "Vasyl Kondratiuk"], "title": "Simulation-Based Study of AI-Assisted Channel Adaptation in UAV-Enabled Cellular Networks", "comment": "13 pages, 8 figures", "summary": "This paper presents a simulation based study of Artificial Intelligence assisted communication channel adaptation in Unmanned Aerial Vehicle enabled cellular networks. The considered system model includes communication channel Ground Base Station Aerial Repeater UAV Base Station Cluster of Cellular Network Users. The primary objective of the study is to investigate the impact of adaptive channel parameter control on communication performance under dynamically changing interference conditions. A lightweight supervised machine learning approach based on linear regression is employed to implement cognitive channel adaptation. The AI model operates on packet level performance indicators and enables real time adjustment of Transaction Size in response to variations in Bit Error Rate and effective Data Rate. A custom simulation environment is developed to generate training and testing datasets and to evaluate system behavior under both static and adaptive channel configurations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14704", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14704", "abs": "https://arxiv.org/abs/2602.14704", "authors": ["Zong Yu Lee", "Xueyan Tang"], "title": "Evaluation of Dynamic Vector Bin Packing for Virtual Machine Placement", "comment": "Extended version of a paper that will appear in IEEE IPDPS 2026 conference", "summary": "Virtual machine placement is a crucial challenge in cloud computing for efficiently utilizing physical machine resources in data centers. Virtual machine placement can be formulated as a MinUsageTime Dynamic Vector Bin Packing (DVBP) problem, aiming to minimize the total usage time of the physical machines. This paper evaluates state-of-the-art MinUsageTime DVBP algorithms in non-clairvoyant, clairvoyant and learning-augmented online settings, where item durations (virtual machine lifetimes) are unknown, known and predicted, respectively. Besides the algorithms taken from the literature, we also develop several new algorithms or enhancements. Empirical experimentation is carried out with real-world datasets of Microsoft Azure. The insights from the experimental results are discussed to explore the structures of algorithms and promising design elements that work well in practice.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13825", "categories": ["cs.AR", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.13825", "abs": "https://arxiv.org/abs/2602.13825", "authors": ["Paras Tiwari", "Narendra Singh Dhakad", "Shalu Rani", "Sanjay Kumar", "Themis Prodromakis"], "title": "Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits", "comment": null, "summary": "In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13200", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13200", "abs": "https://arxiv.org/abs/2602.13200", "authors": ["Andrii Grekhov", "Volodymyr Kharchenko", "Vasyl Kondratiuk"], "title": "Traffic Simulation in Ad Hoc Network of Flying UAVs with Generative AI Adaptation", "comment": "15 pages, 10 figures", "summary": "The purpose of this paper is to model traffic in Ad Hoc network of Unmanned Aerial Vehicles and demonstrate a way for adapting communication channel using Artificial Intelligence. The modeling was based on the original model of Ad Hoc network including 20 Unmanned Aerial Vehicles. The dependences of packet loss on the packet size for different transmission powers, on the packet size for different frequencies, on Unmanned Aerial Vehicles flight area and on the number of Unmanned Aerial Vehicles were obtained and analyzed. The implementation of adaptive data transmission is presented in the program code. The dependences of packet loss, power and transaction size on time during Artificial Intelligence adaptation are shown.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13277", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13277", "abs": "https://arxiv.org/abs/2602.13277", "authors": ["Uma Mahesh Boda", "Mallikharjuna Rao Nuka"], "title": "Intent-driven Diffusion-based Path for Mobile Data Collector in IoT-enabled Dense WSNs", "comment": null, "summary": "Mobile data collection using controllable sinks is an effective approach to improve energy efficiency and data freshness in densely deployed wireless sensor networks (WSNs). However, existing path-planning methods are often heuristic-driven and lack the flexibility to adapt to high-level operational objectives under dynamic network conditions. In this paper, we propose ID2P2, a intent-driven diffusion-based path planning framework for jointly addresses rendezvous point selection and mobile data collector (MDC) tour construction in IoT-enabled dense WSNs. High-level intents, such as latency minimization, energy balancing, or coverage prioritization, are explicitly modeled and incorporated into a generative diffusion planning process that produces feasible and adaptive data collection trajectories. The proposed approach learns a trajectory prior that captures spatial node distribution and network characteristics, enabling the MDC to generate paths that align with specified intents while maintaining collision-free and energy-aware operation. Extensive simulations are conducted to evaluate the effectiveness of the proposed framework against conventional path-planning baselines. The results demonstrate that ID2P2 consistently outperforms representative baselines, achieving up to 25-30% reduction in tour completion time and travel overhead, approximately 10-30% improvement in data freshness, and 15-30% gains in energy efficiency and packet delivery performance, while maintaining higher throughput and fairness as network density increases, confirming its robustness and scalability for WSNs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14262", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14262", "abs": "https://arxiv.org/abs/2602.14262", "authors": ["Siddhartha Raman Sundara Raman", "Jaydeep P. Kulkarni"], "title": "ABI: A tightly integrated, unified, sparsity-aware, reconfigurable, compute near-register file/cache GPU architecture with light-weight softmax for deep learning, linear algebra, and Ising compute", "comment": null, "summary": "We present a tightly integrated and unified near-memory GPU architecture that delivers 6 to 16 times speedup and 6 to 13 times energy savings across Convolutional Neural Networks, Graph Convolutional Networks, Linear Programming, Large Language Models, and Ising workloads compared to MIAOW GPU. The design includes a custom sparsity-aware near-memory circuit providing about 1.5 times energy savings, and a lightweight softmax circuit providing about 1.6 times energy savings. The architecture supports reconfigurable compute up to INT16 with dynamic resolution updates and scales efficiently across problem sizes. ABI-enabled MI300 and Blackwell systems achieve about 4.5 times speedup over baseline MI300 and Blackwell.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13201", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13201", "abs": "https://arxiv.org/abs/2602.13201", "authors": ["Cunlai Pu", "Fangrui Wu", "Zhe Wang", "Xiangbo Shu"], "title": "CLF-ULP: Cross-Layer Fusion-Based Link Prediction in Dynamic Multiplex UAV Networks", "comment": "12 pages, 5 figures", "summary": "In complex Unmanned Aerial Vehicle (UAV) networks, UAVs can establish dynamic and heterogeneous links with one another for various purposes, such as communication coverage, collective sensing, and task collaboration. These interactions give rise to dynamic multiplex UAV networks, where each layer represents a distinct type of interaction among UAVs. Understanding how such links form and evolve is both of theoretical interest and of practical importance for the control and maintenance of networked UAV systems. In this paper, we first develop a dynamic multiplex network model for UAV networks to characterize their dynamic and heterogeneous link properties. We then propose a cross-layer fusion-based deep learning model, termed CLF-ULP, to predict future inter-UAV links based on historical topology data. CLF-ULP incorporates graph attention networks to extract topological features within each layer and perform a cross-layer attention fusion to capture inter-layer dependencies. Furthermore, a shared-parameter long short-term memory network is employed to model the temporal evolution of each layer. To improve embedding quality and link prediction performance, we develop a joint loss function that considers both intra-layer and inter-layer UAV adjacency. Extensive experiments on simulated UAV datasets under diverse mobility patterns demonstrate that CLF-ULP achieves state-of-the-art performance in predicting links within dynamic multiplex UAV networks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13542", "categories": ["cs.NI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13542", "abs": "https://arxiv.org/abs/2602.13542", "authors": ["George M. Gichuru", "Zoe Aiyanna M. Cayetano"], "title": "SIDSense: Database-Free TV White Space Sensing for Disaster-Resilient Connectivity", "comment": "7 pages, 1 figure", "summary": "Small Island Developing States (SIDS) are disproportionately exposed to climate-driven disasters, yet often rely on fragile terrestrial networks that fail when they are most needed. TV White Space (TVWS) links offer long-range, low-power coverage; however, current deployments depend on Protocol to Access White Spaces (PAWS) database connectivity for channel authorization, creating a single point of failure during outages.\n  We present SIDSense, an edge AI framework for database-free TVWS operation that preserves regulatory intent through a compliance-gated controller, audit logging, and graceful degradation. SIDSense couples CNN-based spectrum classification with a hybrid sensing-first, authorization-as-soon-as-possible workflow and co-locates sensing and video enhancement with a private 5G stack on a maritime vessel to sustain situational-awareness video backhaul.\n  Field experiments in Barbados demonstrate sustained connectivity during simulated PAWS outages, achieving 94.2% sensing accuracy over 470-698 MHz with 23 ms mean decision latency, while maintaining zero missed 5G Layer-1 (L1) deadlines under GPU-aware scheduling. We release an empirical Caribbean TVWS propagation and occupancy dataset and look to contribute some of the components of the SIDSense pipeline to the open source community to accelerate resilient connectivity deployments in climate-vulnerable regions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14393", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14393", "abs": "https://arxiv.org/abs/2602.14393", "authors": ["Zongle Huang", "Hongyang Jia", "Kaiwei Zou", "Yongpan Liu"], "title": "Scope: A Scalable Merged Pipeline Framework for Multi-Chip-Module NN Accelerators", "comment": "Accepted in ASP-DAC 2026", "summary": "Neural network (NN) accelerators with multi-chip-module (MCM) architectures enable integration of massive computation capability; however, they face challenges of computing resource underutilization and off-chip communication overheads. Traditional parallelization schemes for NN inference on MCM architectures, such as intra-layer parallelism and inter-layer pipelining, show incompetency in breaking through both challenges, limiting the scalability of MCM architectures.\n  We observed that existing works typically deploy layers separately rather than considering them jointly. This underexploited dimension leads to compromises between system computation and communication, thus hindering optimal utilization, especially as hardware/software scale. To address this limitation, we propose Scope, a merged pipeline framework incorporating this overlooked multi-layer dimension, thereby achieving improved throughput and scalability by relaxing tradeoffs between computation, communication and memory costs. This new dimension, however, adds to the complexity of design space exploration (DSE). To tackle this, we develop a series of search algorithms that achieves exponential-to-linear complexity reduction, while identifying solutions that rank in the top 0.05% of performance. Experiments show that Scope achieves up to 1.73x throughput improvement while maintaining similar energy consumption for ResNet-152 inference compared to state-of-the-art approaches.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13202", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13202", "abs": "https://arxiv.org/abs/2602.13202", "authors": ["Sumita Majhi", "G Vasantha Reddy", "Pinaki Mitra"], "title": "Enhancing NOMA Handover Performance Using Hybrid AI-Driven Modulated Deterministic Sequences", "comment": null, "summary": "Non-Orthogonal Multiple Access (NOMA) is an information-theoretical approach used in 5G networks to improve spectral efficiency, but it is prone to interference during handovers. In this work, we propose a hybrid method that combines Gold-Walsh modulated sequences with Deep Q-Networks (DQN) to intelligently manage interference during NOMA handovers. This method optimizes sequence selection and power allocation dynamically. As a result, it achieves a 95.2\\% handover success rate, which is an improvement of up to 23.1 percentage points. It also delivers up to 28\\% throughput gain and reduces interference by up to 41\\% in various mobility scenarios. All improvements are statistically significant (\\(p < 0.001\\)). The DQN trains in \\(4{,}200 \\pm 400\\) episodes with a complexity of \\(O(N \\log N + d \\cdot h + \\log B)\\) and can be deployed in real-time.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13203", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13203", "abs": "https://arxiv.org/abs/2602.13203", "authors": ["Vignesh Sriram", "Yuqiao Meng", "Luoxi Tang", "Zhaohan Xi"], "title": "Adversarial Network Imagination: Causal LLMs and Digital Twins for Proactive Telecom Mitigation", "comment": null, "summary": "Telecommunication networks experience complex failures such as fiber cuts, traffic overloads, and cascading outages. Existing monitoring and digital twin systems are largely reactive, detecting failures only after service degradation occurs. We propose Adversarial Network Imagination, a closed-loop framework that integrates a Causal Large Language Model (LLM), a Knowledge Graph, and a Digital Twin to proactively generate, simulate, and evaluate adversarial network failures. The Causal LLM produces structured failure scenarios grounded in network dependencies encoded in the Knowledge Graph. These scenarios are executed within a Digital Twin to measure performance degradation and evaluate mitigation strategies. By iteratively refining scenarios based on simulation feedback, the framework shifts network operations from reactive troubleshooting toward anticipatory resilience analysis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13204", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13204", "abs": "https://arxiv.org/abs/2602.13204", "authors": ["Soundes Oumaima Boufaida", "Abdemadjid Benmachiche", "Majda Maatallah", "Chaouki Chemam"], "title": "Hybrid Secure Routing in Mobile Ad-hoc Networks (MANETSs)", "comment": null, "summary": "Because wireless communication is dynamic and has inherent defects, routing algorithms are crucial in the quickly evolving field of mobile ad hoc networks, or MANETs This study looks at the many security problems that MANETs encounter. These problems, which pose major risks to network performance, include flooding, sinkholes, and black hole assaults to address these challenges. We introduce the Hybrid Secure Routing Protocol (HSRP), which enhances the security and robustness of routing operations by fusing trust-based tactics with cryptographic approaches. HSRP combines the strengths of both proactive and reactive routing strategies, enabling it to adapt dynamically to evolving network conditions while protecting against malicious activities. We use extensive simulations with Network Simulator (NS-2) and a thorough review of the literature to assess HSRP's performance under different attack scenarios. The results show that, in comparison to traditional protocols, HSRP increases throughput and decreases latency, hence improving routing efficiency while simultaneously bolstering data transfer security. With uses in vital domains including military operations and disaster response, this study provides a scalable and workable approach for safe routing in MANETs. The findings highlight how crucial it is to include cutting-edge security features in routing protocol design to guarantee the dependability and integrity of MANETs in practical situations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13205", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13205", "abs": "https://arxiv.org/abs/2602.13205", "authors": ["Sumita Majhi", "Kishan Thakkar", "Pinaki Mitra"], "title": "Reinforcement Learning-Enabled Dynamic Code Assignment for Ultra-Dense IoT Networks: A NOMA-Based Approach to Massive Device Connectivity", "comment": null, "summary": "Ultra-dense IoT networks require an effective non-orthogonal multiple access (NOMA) scheme, yet they experience intense interference because of fixed code assignment. We suggest a reinforcement learning (RL) model of dynamic Gold code assignment in IoT-NOMA networks. Our Markov Decision Process which is IoT aware is a joint optimization of throughput, energy efficiency, and fairness. Two RL algorithms are created, including Natural Policy Gradient (NPG) to learn stable discrete actions and Deep Deterministic Policy Gradient (DDPG) with continuous code embedding. Under smart city conditions, NPG can attain throughput of 11.6% and energy efficiency of 15.8 likewise superior to its performance with a static allocation. Nonetheless, the performance is worse in organized industrial settings, and the reliability is minimal (0-2%), which points to the fact that dynamic code assignment is not a sufficient measure of ultra-reliable IoT and needs to be supplemented by power control or retransmission schemes. The work offers a basis to the RL-based resource allocation in massive IoT network.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13316", "categories": ["cs.NI", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.13316", "abs": "https://arxiv.org/abs/2602.13316", "authors": ["Nour Hello", "Mohamed Amine Hamoura", "Francois Rivet", "Emilio Calvanese Strinati"], "title": "Semantic Waveforms for AI-Native 6G Networks", "comment": null, "summary": "In this paper, we propose a semantic-aware waveform design framework for AI-native 6G networks that jointly optimizes physical layer resource usage and semantic communication efficiency and robustness, while explicitly accounting for the hardware constraints of RF chains. Our approach, called Orthogonal Semantic Sequency Division Multiplexing (OSSDM), introduces a parametrizable, orthogonal-base waveform design that enables controlled degradation of the wireless transmitted signal to preserve semantically significant content while minimizing resource consumption. We demonstrate that OSSDM not only reinforces semantic robustness against channel impairments but also improves semantic spectral efficiency by encoding meaningful information directly at the waveform level. Extensive numerical evaluations show that OSSDM outperforms conventional OFDM waveforms in spectral efficiency and semantic fidelity. The proposed semantic waveform co-design opens new research frontiers for AI-native, intelligent communication systems by enabling meaning-aware physical signal construction through the direct encoding of semantics at the waveform level.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13606", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13606", "abs": "https://arxiv.org/abs/2602.13606", "authors": ["Muhammad Baqer Mollah", "Honggang Wang", "Mohammad Ataul Karim", "Hua Fang"], "title": "Multi-Modal Sensing and Fusion in mmWave Beamforming for Connected Vehicles: A Transformer Based Framework", "comment": "13 Pages. arXiv admin note: text overlap with arXiv:2509.11112", "summary": "Millimeter wave (mmWave) communication, utilizing beamforming techniques to address the inherent path loss limitation, is considered as one of the key technologies to support ever increasing high throughput and low latency demands of connected vehicles. However, adopting standard defined beamforming approach in highly dynamic vehicular environments often incurs high beam training overheads and reduction in the available airtime for communications, which is mainly due to exchanging pilot signals and exhaustive beam measurements. To this end, we present a multi-modal sensing and fusion learning framework as a potential alternative solution to reduce such overheads. In this framework, we first extract the representative features from the sensing modalities by modality specific encoders, then, utilize multi-head cross-modal attention to learn dependencies and correlations between different modalities, and subsequently fuse the multimodal features to obtain predicted top-k beams so that the best line-of-sight links can be proactively established. To show the generalizability of the proposed framework, we perform a comprehensive experiment in four different vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) scenarios from real world multimodal and 60 GHz mmWave wireless sensing data. The experiment reveals that the proposed framework (i) achieves up to 96.72% accuracy on predicting top-15 beams correctly, (ii) incurs roughly 0.77 dB average power loss, and (iii) improves the overall latency and beam searching space overheads by 86.81% and 76.56% respectively for top-15 beams compared to standard defined approach.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.13868", "categories": ["cs.NI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13868", "abs": "https://arxiv.org/abs/2602.13868", "authors": ["Udhaya Srinivasan", "Weisi Guo"], "title": "Agentic Assistant for 6G: Turn-based Conversations for AI-RAN Hierarchical Co-Management", "comment": "submitted to IEEE conference", "summary": "New generations of radio access networks (RAN), especially with native AI services are increasingly difficult for human engineers to manage in real-time. Enterprise networks are often managed locally, where expertise is scarce. Existing research has focused on creating Retrieval-Augmented Generation (RAG) LLMs that can help to plan and configure RAN and core aspects only. Co-management of RAN and edge AI is the gap, which creates hierarchical and dynamic problems that require turn-based human interactions. Here, we create an agentic network manager and turn-based conversation assistant that can understand human intent-based queries that match hierarchical problems in AI-RAN. The framework constructed consists of: (a) a user interface and evaluation dashboard, (b) an intelligence layer that interfaces with the AI-RAN, and (c) a knowledge layer for providing the basis for evaluations and recommendations. These form 3 layers of capability with the following validation performances (average response time 13s): (1) design and planning a service (78\\% accuracy), (2) operating specific AI-RAN tools (89\\% accuracy), and (3) tuning AI-RAN performance (67\\%). These initial results indicate the universal challenges of hallucination but also fast response performance success that can really reduce OPEX costs for small scale enterprise users.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.14360", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.14360", "abs": "https://arxiv.org/abs/2602.14360", "authors": ["Zuyuan Zhang", "Vaneet Aggarwal", "Tian Lan"], "title": "LiSFC-Search: Lifelong Search for Network SFC Optimization under Non-stationary Drifts", "comment": "This work has been accepted to the IEEE INFOCOM 2026 Workshop on CNC: Cloud-Network Convergence", "summary": "Edge-cloud convergence is reshaping service provisioning across 5G/6G and computing power networks (CPNs). Service function chaining (SFC) requires continuously placing and scheduling virtual network functions (VNFs) chains under compute/bandwidth and end-to-end QoS constraints. Most SFC optimizers assume static or stationary networks, and degrade under long-term topology/resource changes (failures, upgrades, expansions) that induce non-stationary graph drifts. We propose LiSFC, a Lipschitz lifelong planner that transfers MCTS statistics across drifting network configurations using an MDP-distance bound. More precisely, we formulate the problem as a sequence of MDPs indexed by the underlying network graph and constraints, and we define a \\emph{graph drift} metric that upper-bounds the LiZero MDP distance. This allows LiSFC to import theoretical guarantees on bias and sample efficiency from the LiZero framework while being tailored to cloud-network convergence. We then design \\emph{LiSFC-Search}, an SFC-aware unified MCTS (UMCTS) procedure that uses transferable adaptive UCT (aUCT) bonuses to reuse search statistics from prior CPN configurations. Preliminary results on synthetic CPN topologies and SFC workloads show that LiSFC consistently reduces SFC blocking probability and improves tail delay compared to non-transfer MCTS and purely learning-based baselines, highlighting its potential as an AI/ML building block for cloud-network convergence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
