{"id": "2601.21362", "categories": ["cs.NI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.21362", "abs": "https://arxiv.org/abs/2601.21362", "authors": ["Miao Zhang", "Guanzhen Wu", "Hao Fang", "Yifei Zhu", "Fangxin Wang", "Ruixiao Zhang", "Jiangchuan Liu"], "title": "ViTMAlis: Towards Latency-Critical Mobile Video Analytics with Vision Transformers", "comment": null, "summary": "Edge-assisted mobile video analytics (MVA) applications are increasingly shifting from using vision models based on convolutional neural networks (CNNs) to those built on vision transformers (ViTs) to leverage their superior global context modeling and generalization capabilities. However, deploying these advanced models in latency-critical MVA scenarios presents significant challenges. Unlike traditional CNN-based offloading paradigms where network transmission is the primary bottleneck, ViT-based systems are constrained by substantial inference delays, particularly for dense prediction tasks where the need for high-resolution inputs exacerbates the inherent quadratic computational complexity of ViTs. To address these challenges, we propose a dynamic mixed-resolution inference strategy tailored for ViT-backboned dense prediction models, enabling flexible runtime trade-offs between speed and accuracy. Building on this, we introduce ViTMAlis, a ViT-native device-to-edge offloading framework that dynamically adapts to network conditions and video content to jointly reduce transmission and inference delays. We implement a fully functional prototype of ViTMAlis on commodity mobile and edge devices. Extensive experiments demonstrate that, compared to state-of-the-art accuracy-centric, content-aware, and latency-adaptive baselines, ViTMAlis significantly reduces end-to-end offloading latency while improving user-perceived rendering accuracy, providing a practical foundation for next-generation mobile intelligence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21383", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.21383", "abs": "https://arxiv.org/abs/2601.21383", "authors": ["Zhiyuan Zhao", "Jiasheng Wu", "Shaojie Su", "Wenjun Zhu", "Yue Gao"], "title": "KubeSpace: A Low-Latency and Stable Control Plane for LEO Satellite Container Orchestration", "comment": "11 pages, 11 figures", "summary": "Low Earth orbit (LEO) satellites play a pivotal role in global connectivity-delivering high-speed Internet, cellular coverage, and massive IoT support. With ever-growing onboard computing and storage resources, LEO satellites herald a new cloud paradigm: space cloud computing. While container or chestration platforms (e.g., Kubernetes) excel in terrestrial data centers, they are ill-suited to LEO satellite networks, featuring geographic dispersion and frequent handovers. Those features bring high latency and intermittent management, leading to control plane failure in container orchestration. To address this, we propose KubeSpace, a low-latency and stable control plane specifically designed for container orchestration on LEO satellites. KubeSpace combines two key innovations: a distributed ground-control-node architecture that binds each satellite to its nearest controller for uninterrupted management, and an orbit-aware placement with dynamic assignment strategy that further minimizes communication latency and handover frequency. Extensive experiments based on real satellite traces demonstrate that compared to existing solutions, KubeSpace reduces the average management latency of satellite nodes by 59% without any management interruption time.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21701", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.21701", "abs": "https://arxiv.org/abs/2601.21701", "authors": ["Ankita Koley", "Anu Krishna", "Chandramani Singh", "V Mahendran"], "title": "Age Aware Content Fetching and Broadcast in a Sensing-as-a-Service System", "comment": null, "summary": "We consider a Sensing-as-a-Service (S2aaS) system consisting of a sensor, a set of users, and a sensor cloud service provider (SCSP). The sensor updates its content each time it captures a new measurement. The SCSP occasionally fetches the content from the sensor, caches the latest fetched version and broadcasts it on being requested by the users. The SCSP incurs content fetching costs while fetching and broadcasting the contents. The SCSP also incurs an age cost if users do not receive the most recent version of the content after requesting. We study a content fetching and broadcast problem, aiming to minimize the time-averaged content fetching and age costs. The problem can be framed as a Markov decision process but cannot be elegantly solved owing to its multi-dimensional state space and complex dynamics. To address this, we first obtain the optimal policy for the homogeneous case with all the users having the same request probability and age cost. We extend this algorithm for heterogeneous case but the complexity grows exponentially with the number of users. To tackle this, we propose a low complexity Whittle index based algorithm, which performs very close to the optimal. The complexity of the algorithm is linear in number of users and serves as a heuristic for both homogeneous and heterogeneous cases.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21861", "categories": ["cs.NI", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21861", "abs": "https://arxiv.org/abs/2601.21861", "authors": ["Chuan-Chi Lai"], "title": "Spatiotemporal Continual Learning for Mobile Edge UAV Networks: Mitigating Catastrophic Forgetting", "comment": "12 pages, 9 figures, manuscript submitted to IEEE Transactions on Emerging Topics in Computing", "summary": "This paper addresses the critical challenge of coordinating mobile edge UAV networks to maintain robust service in highly dynamic spatiotemporal environments. Conventional Deep Reinforcement Learning (DRL) approaches often suffer from catastrophic forgetting when transitioning between distinct task scenarios, such as moving from dense urban clusters to sparse rural areas. These transitions typically necessitate computationally expensive retraining or model resets to adapt to new user distributions, leading to service interruptions. To overcome these limitations, we propose a computationally efficient Spatiotemporal Continual Learning (STCL) framework realized through a Group-Decoupled Multi-Agent Proximal Policy Optimization (G-MAPPO) algorithm. Our approach integrates a novel Group-Decoupled Policy Optimization (GDPO) mechanism that utilizes dynamic $z$-score normalization to autonomously balance heterogeneous objectives, including energy efficiency, user fairness, and coverage. This mechanism effectively mitigates gradient conflicts induced by concept drifts without requiring offline retraining. Furthermore, the framework leverages the 3D mobility of UAVs as a spatial compensation layer, enabling the swarm to autonomously adjust altitudes to accommodate extreme density fluctuations. Extensive simulations demonstrate that the proposed STCL framework achieves superior resilience, characterized by an elastic recovery of service reliability to approximately 0.95 during phase transitions. Compared to the MADDPG baseline, G-MAPPO not only prevents knowledge forgetting but also delivers an effective capacity gain of 20\\% under extreme traffic loads, validating its potential as a scalable solution for edge-enabled aerial swarms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21222", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.21222", "abs": "https://arxiv.org/abs/2601.21222", "authors": ["Tenglong Li", "Jindong Li", "Guobin Shen", "Dongcheng Zhao", "Qian Zhang", "Yi Zeng"], "title": "FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control", "comment": "5 pages, 4 figures. Accepted for lecture presentation at the 2026 IEEE International Symposium on Circuits and Systems (ISCAS 2026)", "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$\u03bc$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21090", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21090", "abs": "https://arxiv.org/abs/2601.21090", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies", "comment": null, "summary": "The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.22024", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22024", "abs": "https://arxiv.org/abs/2601.22024", "authors": ["Abhishek Duttagupta", "MohammadErfan Jabbari", "Claudio Fiandrino", "Marco Fiore", "Joerg Widmer"], "title": "SymbXRL: Symbolic Explainable Deep Reinforcement Learning for Mobile Networks", "comment": "10 pages, 9 figures, published in IEEE INFOCOM 2025", "summary": "The operation of future 6th-generation (6G) mobile networks will increasingly rely on the ability of deep reinforcement learning (DRL) to optimize network decisions in real-time. DRL yields demonstrated efficacy in various resource allocation problems, such as joint decisions on user scheduling and antenna allocation or simultaneous control of computing resources and modulation. However, trained DRL agents are closed-boxes and inherently difficult to explain, which hinders their adoption in production settings. In this paper, we make a step towards removing this critical barrier by presenting SymbXRL, a novel technique for explainable reinforcement learning (XRL) that synthesizes human-interpretable explanations for DRL agents. SymbXRL leverages symbolic AI to produce explanations where key concepts and their relationships are described via intuitive symbols and rules; coupling such a representation with logical reasoning exposes the decision process of DRL agents and offers more comprehensible descriptions of their behaviors compared to existing approaches. We validate SymbXRL in practical network management use cases supported by DRL, proving that it not only improves the semantics of the explanations but also paves the way for explicit agent control: for instance, it enables intent-based programmatic action steering that improves by 12% the median cumulative reward over a pure DRL solution.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21584", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21584", "abs": "https://arxiv.org/abs/2601.21584", "authors": ["Pin-Han Ho", "Limei Peng", "Yiming Miao", "Xu Fan", "Kairan Liang", "Haoran Mei", "Wei Duan"], "title": "Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios", "comment": null, "summary": "Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21146", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21146", "abs": "https://arxiv.org/abs/2601.21146", "authors": ["Francesco Paladino", "Shulu Li", "Edward A. Lee"], "title": "Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems", "comment": null, "summary": "Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.22044", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22044", "abs": "https://arxiv.org/abs/2601.22044", "authors": ["MohammadErfan Jabbari", "Abhishek Duttagupta", "Claudio Fiandrino", "Leonardo Bonati", "Salvatore D'Oro", "Michele Polese", "Marco Fiore", "Tommaso Melodia"], "title": "SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control", "comment": "10 pages, 12 figures, accepted at IEEE INFOCOM 2026", "summary": "Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-boxes; operators cannot tell whether predictions guide decisions or justify the added complexity. We propose SIA, the first interpreter that exposes in real time how forecast-augmented DRL agents operate. SIA fuses Symbolic AI abstractions with per-KPI Knowledge Graphs to produce explanations, and includes a new Influence Score metric. SIA achieves sub-millisecond speed, over 200x faster than existing XAI methods. We evaluate SIA on three diverse networking use cases, uncovering hidden issues, including temporal misalignment in forecast integration and reward-design biases that trigger counter-productive policies. These insights enable targeted fixes: a redesigned agent achieves a 9% higher average bitrate in video streaming, and SIA's online Action-Refinement module improves RAN-slicing reward by 25% without retraining. By making anticipatory DRL transparent and tunable, SIA lowers the barrier to proactive control in next-generation mobile networks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21198", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21198", "abs": "https://arxiv.org/abs/2601.21198", "authors": ["Yuchen Yang", "Yaru Zhao", "Pu Yang", "Shaowei Wang", "Zhi-Hua Zhou"], "title": "ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling", "comment": null, "summary": "While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\\%$ inference latency reduction and up to $6.76\\times$ higher throughput than the state-of-the-art systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21855", "categories": ["cs.DC", "cs.DB", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.21855", "abs": "https://arxiv.org/abs/2601.21855", "authors": ["Chuan-Chi Lai"], "title": "Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning", "comment": "12 pages, 4 figures, manuscript submitted to IEEE Transactions on Emerging Topics in Computing", "summary": "In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\\% in communication overhead and 40\\% in total response time, while ensuring robust scalability across diverse data distributions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21286", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.21286", "abs": "https://arxiv.org/abs/2601.21286", "authors": ["Adithya Bhat", "Harshal Bhadreshkumar Shah", "Mohsen Minaei"], "title": "Ira: Efficient Transaction Replay for Distributed Systems", "comment": null, "summary": "In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \\emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.\n  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.\n  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\\sim5\\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\\times$ wall-time speedup).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21758", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21758", "abs": "https://arxiv.org/abs/2601.21758", "authors": ["Bronislav Sidik", "Chaya Levi", "Joseph Kampeas"], "title": "EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2601.21935", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21935", "abs": "https://arxiv.org/abs/2601.21935", "authors": ["Tom Yates", "Yuzhou Cheng", "Ignacio Alzugaray", "Danyal Akarca", "Pedro A. M. Mediano", "Andrew J. Davison"], "title": "Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs", "comment": "Preprint. Under review. 25 pages (including Appendix). 8 Figures", "summary": "Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
