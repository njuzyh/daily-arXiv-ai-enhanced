<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: THEAS通过动态资源适配，在异构系统中对非固定亲和任务提供能效与实时性兼顾的调度方案，并在多项指标上优于或持平于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升系统在负载波动明显情境下的能效与性能平衡，尤其针对异构系统与非固定CPU亲和性的任务。

Method: 基于动态资源水平调整与异构核心选择策略，综合性能与功耗目标并进行多维指标比较评估。

Result: 提出THEAS算法，通过动态调整资源水平以在性能与能耗之间取得平衡，并与CFS、EAS、HeteroSched和Utility-Based Scheduling进行多维比较。

Conclusion: THEAS在适应性、核心选择、性能扩展和实时适用性方面表现良好，适合广泛实时应用。同时需要关注开销和缓存感知等实现细节。

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [2] [QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters](https://arxiv.org/abs/2510.09851)
*Haci Ismail Aslan,Syed Muhammad Mahmudul Haque,Joel Witzke,Odej Kao*

Main category: cs.DC

TL;DR: QONNECT enables declarative QoS-driven deployment across heterogeneous Kubernetes/K3s clusters via a distributed agent-based architecture, automating placement and migration to meet goals like energy, cost, performance.


<details>
  <summary>Details</summary>
Motivation: Provide QoS-driven, vendor-agnostic orchestration across heterogeneous cloud-fog-edge Kubernetes clusters to automate placement and migration decisions and avoid manual operator work.

Method: Design and implement distributed system: central Knowledge Base, Raft-replicated Resource Lead Agents, lightweight Resource Agents; minimal YAML QoS interface; translation of goals into placement/migration actions; evaluation on federated testbed with Istio Bookinfo microservices across up to nine clusters measuring placement, failover, rescheduling, leader re-election.

Result: QONNECT: distributed architecture with central Knowledge Base, Raft-replicated Resource Lead Agents, lightweight Resource Agents per cluster, YAML QoS specification, evaluated on up to nine clusters with Bookinfo app showing dynamic placement, failover, rescheduling, leader re-election without manual intervention.

Conclusion: QONNECT bridges declarative deployment and operational QoS, enabling self-optimizing cross-cluster orchestration and robust fault tolerance in cloud-fog-edge environments.

Abstract: Modern applications increasingly span across cloud, fog, and edge
environments, demanding orchestration systems that can adapt to diverse
deployment contexts while meeting Quality-of-Service (QoS) requirements.
Standard Kubernetes schedulers do not account for user-defined objectives such
as energy efficiency, cost optimization, and global performance, often leaving
operators to make manual, cluster-by-cluster placement decisions. To address
this need, we present QONNECT, a vendor-agnostic orchestration framework that
enables declarative, QoS-driven application deployment across heterogeneous
Kubernetes and K3s clusters. QONNECT introduces a distributed architecture
composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and
lightweight Resource Agents in each cluster. Through a minimal YAML-based
interface, users specify high-level QoS goals, which the system translates into
concrete placement and migration actions. Our implementation is evaluated on a
federated testbed of up to nine cloud-fog-edge clusters using the Istio
Bookinfo microservice application. The system demonstrates dynamic,
policy-driven microservice placement, automated failover, QoS-compliant
rescheduling, and leader re-election after node failure, all without manual
intervention. By bridging the gap between declarative deployment models and
operational QoS goals, QONNECT transforms the cloud-edge continuum into a
unified, self-optimizing platform.

</details>


### [3] [FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments](https://arxiv.org/abs/2510.10126)
*Sehar Zehra,Hassan Jamil Syed,Ummay Faseeha*

Main category: cs.DC

TL;DR: FedMon combines lightweight eBPF agents with federated learning (VAE + Isolation Forest) to detect anomalies across multiple Kubernetes clusters while preserving privacy and reducing bandwidth


<details>
  <summary>Details</summary>
Motivation: Enable scalable, privacy-preserving cross-cluster telemetry and anomaly detection without centralized raw data collection

Method: Federated anomaly detection using eBPF across Kubernetes clusters

Result: FedMon achieves 94% precision, 91% recall, F1 0.92, and 60% bandwidth reduction compared to centralized baselines

Conclusion: FedMon improves accuracy, scalability, and privacy for multi-cluster cloud-native anomaly detection

Abstract: Kubernetes multi-cluster deployments demand scalable and privacy-preserving
anomaly detection. Existing eBPF-based monitors provide low-overhead system and
network visibility but are limited to single clusters, while centralized
approaches incur bandwidth, privacy, and heterogeneity challenges. We propose
FedMon, a federated eBPF framework that unifies kernel-level telemetry with
federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF
agents capture syscalls and network events, extract local statistical and
sequence features, and share only model updates with a global server. A hybrid
detection engine combining Variational Autoencoders (VAEs) with Isolation
Forests enables both temporal pattern modeling and outlier detection. Deployed
across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,
and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to
centralized baselines. Results demonstrate that FedMon enhances accuracy,
scalability, and privacy, providing an effective defense for large-scale,
multi-tenant cloud-native environments.

</details>


### [4] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Edge computing decentralizes resources using microservices requiring low latency; hybrid cloud-edge needed for SLAs; existing auto-scaling algorithms have performance/config complexity issues; chapter surveys architecture, SLAs, and algorithms.


<details>
  <summary>Details</summary>
Motivation: Summarize and critique auto-scaling in edge-cloud hybrid for SLA compliance

Method: Paper analysis based on abstract

Result: Overview of edge computing, SLAs, and auto-scaling algorithms with pros/cons

Conclusion: Need for improved auto-scaling methods balancing performance and configuration simplicity; future work on adaptive, lightweight, SLA-aware scaling.

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [5] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE is an SD-aware offloading framework that prefetches likely experts based on draft-target model correspondence, bounds prefetching with a cutoff-layer policy, and pipelines I/O to reduce latency, delivering significant inference speedups.


<details>
  <summary>Details</summary>
Motivation: Combining Mixture-of-Experts (MoE) with Speculative Decoding (SD) inflates GPU memory and creates CPU-GPU bandwidth bottlenecks during multi-token verification; existing offloading systems ignore SD-specific patterns.

Method: Speculative expert-offloading and pipelining for MoE with SD

Result: SP-MoE introduces speculative expert prefetching, a cutoff-layer policy for bounded prefetch depth, and a pipelined runtime with async prefetch threads and batched I/O; achieves 1.07–3.5x TPOT speedup over SOTA across datasets, environments, and MoE models.

Conclusion: SD-aware prefetching combined with cutoff-layer control and pipelined runtime effectively mitigates memory and bandwidth bottlenecks in MoE+SD inference, improving throughput and latency compared to prior methods.

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [6] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: 提出FLAMMABLE，用动态批次大小和基于能力的多模型分配策略优化多模型联邦学习；配套MMFL基准平台，实验证明在时间-准确率和最终精度上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in training multiple models concurrently in federated learning due to combined data, system, and model heterogeneity; current single-model FL methods and naive extensions don't scale.

Method: 设计客户端能力感知机制，动态调整每客户端的训练批次大小，并在每轮选择多个适配模型让客户端训练，从而提高并行度和资源利用率；实现并评估在多个数据集与模型上的性能提升。

Result: FLAMMABLE framework that adapts client batch sizes and assigns clients to train multiple chosen models per round based on capabilities; also a first MMFL benchmark platform.

Conclusion: FLAMMABLE有效应对MMFL中的异构性，通过智能批量调整与模型分配，显著提升训练速度与模型精度，并提供了可复现的MMFL基准平台推动后续研究。

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [7] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: 提出DCP：通过块状动态划分与映射，针对变长序列与多样注意力模式减少通信与负载不均，提升注意力与整体训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有上下文并行方案使用静态并行配置，忽视训练数据在序列长度与注意力关系上的动态性，导致不必要通信开销与计算不均衡，亟需一种能动态适配样本差异的并行训练框架。

Method: 采用细粒度的块状划分（数据块+计算块），并实行动态的块—设备映射策略以匹配样本的序列长度和注意力稀疏性，减少跨设备通信和重复计算。同时通过微基准评测（causal与sparse掩码）量化性能提升。

Result: DCP是一种针对长上下文训练的动态上下文并行框架，通过细粒度块状划分数据与计算并支持灵活映射到设备，从而减少通信、改善内存与计算负载均衡。实验显示在因果掩码下加速注意力1.19x~2.45x，稀疏注意力下2.15x~3.77x；端到端训练在因果掩码下提升0.94x~1.16x，稀疏掩码下1.00x~1.46x。

Conclusion: DCP能有效适配训练样本的序列长度与注意力模式的动态变化，显著降低不必要通信并改善计算/内存均衡，从而在多种注意力模式下实现注意力子算子与端到端训练的加速。

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [8] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: The paper challenges conventional wisdom that container CPU-limits are universally beneficial, providing evidence that they can hurt latency-sensitive applications and proposing to avoid them in such cases while reserving limits for well-justified scenarios.


<details>
  <summary>Details</summary>
Motivation: Investigate whether the widespread use of CPU-limits for containers is necessary or beneficial, given reports that limits may harm performance and increase costs for cloud-native, latency-sensitive applications.

Method: Empirical evaluation comparing application performance and costs with and without CPU-limits, analysis of cloud user experiences, and discussion of autoscaling/billing implications; case studies demonstrating when limits help (e.g., background jobs).

Result: Empirical evidence showing that removing CPU-limits improves performance and reduces costs for latency-sensitive applications; proposals for rethinking autoscaling and billing; identification of scenarios where CPU-limits remain useful (e.g., background jobs).

Conclusion: CPU-limits should often be avoided for latency-sensitive cloud-native applications; autoscaling and billing models should be reconsidered; CPU-limits can still be useful for isolation of non-latency-critical workloads when applied carefully.

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [9] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: 提出了一种适用于协程运行时的无自旋、无内核锁互斥机制；基于锁-free 队列的排队策略保证 FIFO 公平性，并用 CSP/FDR 形式化证明线性化和公平性，证明方法与公正性预言器可复用。


<details>
  <summary>Details</summary>
Motivation: 在用户态调度器和协程运行时中，传统互斥机制（自旋、内核锁）成本高且不易与用户态调度协作，故需要一种低开销且可证明公平与线性化的互斥方案。

Method: 设计了 claim/release 协议以管理进程对共享通道的争用，使用无锁队列停车等待进程，提出公平性预言器并用稳定性证明方法结合 CSP 建模和 FDR 验证来证明协议满足互斥语义和 FIFO 公平性。

Result: The paper introduces a novel mutex for coroutine runtimes that is spin-free and kernel-lock-free, compatible with user-mode schedulers, and proven FIFO-fair and linearizable using CSP/FDR. It includes a reusable fairness oracle and stability-based proof technique, a claim/release protocol implemented in ProcessJ, and uses a lock-free queue to manage waiting processes. CSP models and FDR verification show protocol correctness relative to a mutex specification.

Conclusion: 论文证明了其互斥协议在用户态调度器环境下实现了 FIFO 公平和线性化，且无需自旋或内核锁；提出的公平性预言器与基于稳定性的证明方法可复用于其他协程运行时设计。

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [10] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出FIDRS框架：利用异构分布式数据库与RMSD算法提升集成系统的响应速度、性能与可靠性，仿真结果优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 目标是改进现有集成系统在性能、响应速度及可靠性方面的不足，提供一个既高效又可靠的分布式集成框架以满足大规模数据库环境下的需求。

Method: 论文先回顾了相关工作，然后设计了包含三部分的FIDRS框架：采用异构分布式数据库以提高性能和响应速度；在数据抽取阶段引入RMSD算法以减少大数据库的响应时间；并通过仿真对比验证新框架优于先前方案。

Result: 通过仿真实验，FIDRS在响应时间、系统效率和可靠性指标上优于先前框架，解决了部分旧框架存在的问题，并证明了异构分布式数据库和RMSD算法的组合有效性。

Conclusion: 该论文提出并验证了一个名为FIDRS的新框架，旨在提升集成分布式系统的性能、效率和可靠性。实验仿真结果显示，与先前框架相比，FIDRS在响应时间、效率和可靠性上有所提升。

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [11] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: 本文从让LLM在消费级硬件可用与对比主流服务部署方案两方面研究分布式技术，提出了基于元启发式的系统改进并通过实验展示在资源受限环境下的可行性与不同部署策略的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM参数规模巨大，单节点难以完成训练、微调和推理，迫切需要分布式和资源节约的方案，使更多用户能够使用LLM并提高部署效率。

Method: 从两条主线展开：一是研究让LLM在消费级计算机上运行的技术，包含模型压缩、分段加载、量化、模型并行与系统级优化，并在已有系统上实现了一种基于元启发式算法的改进以优化资源分配；二是对三种最先进的LLM服务部署技术进行对比评估，使用性能指标（延迟、吞吐量）、资源利用率和可扩展性进行实验比较。

Result: 实验表明：采用量化与分段加载等策略可显著降低内存占用，使模型在消费级GPU/CPU上可用；元启发式改进在资源受限场景下提升了调度/分配效率；三种服务技术在延迟与吞吐间存在明显权衡，且对硬件配置敏感。

Conclusion: 本文探讨了面向大规模语言模型（LLM）的分布式计算技术，得出结论：通过适当的分布式策略和轻量化改进，可以在消费级硬件上运行大型模型，并在不同部署方案之间实现权衡以满足性能、资源与可用性需求。

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [12] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: 本文分析了S_N输运求解器在高核数多核架构上的共享内存并行瓶颈，比较了多种硬件上的性能，并提出并验证了异步多任务(AMT)算法以提高计算性能。


<details>
  <summary>Details</summary>
Motivation: 分析现有非结构化网格上的离散方向S_N输运求解器在共享内存并行化下的性能瓶颈及其在多核架构上的扩展性问题，并提出改进的并行算法以提升性能。

Method: 通过性能分析工具测量热点与内存带宽占用、剖析数据依赖和任务粒度，横向比较不同硬件上的基线实现，然后设计并实现基于任务图和异步调度的AMT算法，并通过基准测试评估性能改进与原因分析。

Result: 通过分析识别出内存访问模式、数据依赖、负载不均衡和线程同步开销为主要瓶颈；在多种硬件上对现有求解器进行基准测试；提出一种基于异步多任务(AMT)的共享内存并行算法，实验显示在计算性能上优于现有方法并解释了性能提升的原因。

Conclusion: AMT并行化在减小同步开销、改善负载均衡和提高内存并行性方面优于传统共享内存实施，从而提升了S_N求解器在现代多核硬件上的总体性能。

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [Fine-grained CDN Delegation](https://arxiv.org/abs/2510.09983)
*Ethan Thompson,Ali Sadeghi Jahromi,AbdelRahman Abdou*

Main category: cs.NI

TL;DR: DeCerts extend X.509 to give domain owners autonomous, fine-grained CDN delegation control (scope, depth, revocation). Implemented in Firefox as proof-of-concept; balances security and performance.


<details>
  <summary>Details</summary>
Motivation: CDN delegation lacks fine-grained controls and autonomy for domain owners; existing solutions like Delegated Credentials don't define delegation length, revocation, operations, or scope well. Need a mechanism to allow domain owners fine-grained delegation and independent issuance.

Method: Design X.509 extensions for delegation, define semantics for subdomain scope and delegation depth, propose multiple revocation mechanisms, implement validation in Firefox, test compatibility and feasibility.

Result: Propose DeCerts: X.509 modifications and extensions enabling domain owners to issue delegation certificates independent of CAs, specify delegated/non-delegated subdomains, control delegation depth, and support revocation methods. Implemented Firefox modification proving feasibility and TLS compatibility.

Conclusion: DeCerts improve security, scalability, and manageability of CDN delegation by allowing domain-owner-issued certificates with controlled scope and revocation, compatible with browsers and TLS.

Abstract: The use of Content Delivery Networks (CDNs) has significantly increased over
the past decade, with approximately 55 million websites currently relying on
CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),
lack fine-grained definitions of many critical aspects of delegation, such as
the length of delegation chains, revocation mechanism, permitted operations,
and a well-defined scope for said delegation. We present Delegation
Certificates (DeCerts), which modify X.509 certificate standard and add new
extensions to enable fine-grained CDN delegation. DeCerts allow domain owners
to specify delegated and non-delegated subdomains, and control the depth of
delegation extended by CDNs, which provides flexibility in delegation
management. But more importantly, DeCerts are built on a new principle which
provides full autonomy to domain owners-domain owners can issue DeCerts fully
independent of Certificate Authorities (CAs), and thus have greater flexibility
in policy control, including revocation methods. Such level of flexibility
would be hard to match if CAs where to issue such certificates. Revoking a
DeCert revokes delegation. We discuss multiple revocation mechanisms for a
DeCerts balancing security, performance, and delegator control. We modify
Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and
test it to demonstrate the feasibility, compatibility of DeCerts with browsers
and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and
manageability of CDN delegation, offering a practical solution for Internet
services.

</details>


### [14] [Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference](https://arxiv.org/abs/2510.10040)
*Shafi Ullah Khan,Michel Kulhandjian,Debashri Roy*

Main category: cs.NI

TL;DR: 本文探讨在CBRS频段中使用机器学习提升商业无线与军用雷达共存的能力，针对高干扰环境下的雷达检测与波形识别问题，提出基于IQ数据和频谱图的模型。实验表明在高达-5 dB SINR 的干扰下仍可达到FCC建议的99%检测率，并能以93%准确率区分6种雷达波形，显著将99%检测所需的SINR下限从约12 dB扩展到-5 dB。


<details>
  <summary>Details</summary>
Motivation: 面对商业无线需求快速增长，频谱共享成为关键策略，但需在不影响关键使命系统（如军用雷达）的前提下实现高效共存。本研究旨在通过机器学习提高雷达在高干扰环境下的检测和波形识别能力，从而推动更灵活的频谱管理与技术部署。

Method: 使用基于IQ样本和频谱图的深度学习模型进行雷达检测与波形分类。训练与评估时采用合成信号与真实世界信号数据集，测量不同SINR条件下的检测率和分类准确率，比较模型性能与FCC推荐阈值及现有方法。

Result: 在合成和真实数据上，提出的模型在IQ与频谱图输入下，在-5 dB SINR条件仍达到99%雷达检测率（优于传统方法需约12 dB），并在雷达波形识别任务上实现93%准确率，验证了ML方法扩展SINR容限与区分多种雷达波形的可行性。

Conclusion: ML模型能在CBRS场景中显著提高雷达检测和波形识别性能，使在高达-5 dB SINR的5G干扰下仍实现99%检测率，并能以93%准确率分类六种雷达波形，从而有望扩展现有监管和共存策略的边界。

Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands
via commercial wireless services, yet its effective regulation and
technological enablement, particularly concerning coexistence with incumbent
systems, remain significant challenges. Federal organizations have established
regulatory frameworks to manage shared commercial use alongside
mission-critical operations, such as military communications. This paper
investigates the potential of machine learning (ML)-based approaches to enhance
spectrum sharing capabilities within the Citizens Broadband Radio Service
(CBRS) band, specifically focusing on the coexistence of commercial signals
(e.g., 5G) and military radar systems. We demonstrate that ML techniques can
potentially extend the Federal Communications Commission (FCC)-recommended
signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar
detection and waveform identification in high-interference environments.
Through rigorous evaluation using both synthetic and real-world signals, our
findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)
data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection
accuracy even when subjected to high interference from 5G signals upto -5dB
SINR, exceeding the required limits of $20$ SINR. Our experimental studies
distinguish this work from the state-of-the-art by significantly extending the
SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down
to $-5$ dB. Subsequent to detection, we further apply ML to analyze and
identify radar waveforms. The proposed models also demonstrate the capability
to classify six distinct radar waveform types with $93\%$ accuracy.

</details>


### [15] [Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures](https://arxiv.org/abs/2510.10044)
*Rahul Vanukuri,Shafi Ullah Khan,Talip Tolga Sarı,Gokhan Secinti,Diego Patiño,Debashri Roy*

Main category: cs.NI

TL;DR: 提出基于扩散模型生成CBRS频段LTE/5G/雷达混合频谱图，使用SSIM/PSNR评估生成质量，预训练可使真实雷达检测任务收敛速度提升51.5%


<details>
  <summary>Details</summary>
Motivation: real-world radar signals are rare, making balanced labeled datasets for radar detection in shared bands like CBRS difficult to collect, limiting AI model performance and generalizability

Method: diffusion-based generative model for RF spectrograms

Result: generated realistic diverse spectrograms across five categories (LTE, 5G, radar combinations); evaluated using SSIM and PSNR showing structural/statistical fidelity; pre-training on generated data yields 51.5% faster convergence on real radar detection task

Conclusion: 扩散生成的合成频谱图能补充稀缺真实数据，提升下游雷达检测任务的训练效率和模型泛化

Abstract: The growing demand for effective spectrum management and interference
mitigation in shared bands, such as the Citizens Broadband Radio Service
(CBRS), requires robust radar detection algorithms to protect the military
transmission from interference due to commercial wireless transmission. These
algorithms, in turn, depend on large, diverse, and carefully labeled
spectrogram datasets. However, collecting and annotating real-world radio
frequency (RF) spectrogram data remains a significant challenge, as radar
signals are rare, and their occurrences are infrequent. This challenge makes
the creation of balanced datasets difficult, limiting the performance and
generalizability of AI models in this domain.
  To address this critical issue, we propose a diffusion-based generative model
for synthesizing realistic and diverse spectrograms of five distinct categories
that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a
structural and statistical fidelity analysis of the generated spectrograms
using widely accepted evaluation metrics Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from
the training data. Furthermore, we demonstrate that pre-training on the
generated spectrograms significantly improves training efficiency on a
real-world radar detection task by enabling $51.5\%$ faster convergence.

</details>


### [16] [Hybrid MAC Protocol with Integrated Multi-Layered Security for Resource-Constrained UAV Swarm Communications](https://arxiv.org/abs/2510.10236)
*Dhrumil Bhatt,Siddharth Penumatsa,Vidushi Kumar*

Main category: cs.NI

TL;DR: 提出面向无人机群的软硬件协同设计，结合多播簇组织、信任驱动路由、混合MAC与零信任安全，NS-3仿真证明性能优于现有方案


<details>
  <summary>Details</summary>
Motivation: FANETs suffer from high mobility and resource constraints; existing protocols optimize single metrics and ignore interdependence of performance, security, and MAC efficiency

Method: Hardware-software co-design for secure adaptive UAV swarm communications

Result: Proposed a multicast clustered architecture with routing using trust, link quality, distance; hybrid MAC; zero trust security with cryptographic and reputation fusion; AES-GCM hardware acceleration; NS-3 simulation shows improved PDR, latency, resilience, and overhead

Conclusion: 该框架在吞吐、延迟、可靠性与安全上均优于单一优化的传统协议，为高性能可扩展无人机群通信提供了基础

Abstract: Flying Ad Hoc Networks (FANETs) present unique challenges due to high node
mobility, dynamic topologies, and strict resource constraints. Existing routing
protocols often optimize for a single metric, such as path length or energy,
while neglecting the complex dependencies between network performance,
security, and MAC layer efficiency. This paper introduces a novel hardware
software co design framework for secure and adaptive UAV swarm communications,
featuring an energy aware protocol stack. The architecture employs a multicast,
clustered organization where routing decisions integrate dynamic trust scores,
historical link quality, and internodal distance. A hybrid MAC protocol
combines contention based and scheduled channel access for optimized
throughput. Security is ensured through a zero trust model that fuses
cryptographic authentication with a behavioral reputation system, alongside
hardware accelerated AES GCM encryption. Comparative analysis in an NS 3
simulation environment demonstrates the framework's superiority in packet
delivery ratio, latency, resilience, and overhead, providing a scalable
foundation for high performance swarm operations.

</details>


### [17] [A Framework for AI-Native Semantic-Based Dynamic Slicing for 6G Networks](https://arxiv.org/abs/2510.10756)
*Mayukh Roy Chowdhury,Eman Hammad,Lauri Loven,Susanna Pirttikangas,Aloizio P da Silva,Walid Saad*

Main category: cs.NI

TL;DR: Introduce semantic slicing: dynamically create virtual network/computation slices based on data semantics to optimize resource use in 6G computing continuum.


<details>
  <summary>Details</summary>
Motivation: Optimize network resources in ultra-dense, diverse 6G environments by integrating data content/semantics into network operations for dynamic, autonomous optimization across computing continuum.

Method: Analyze Abstract

Result: Proposes 'semantic slicing'—dynamic, content-aware virtual divisions across computing and network resources to enable cross-layer design and advance semantic communication.

Conclusion: Semantic slicing extends static network slicing by incorporating data semantics and dynamic allocation across communication, computing, and storage, enabling more efficient, adaptive, cross-layer 6G systems.

Abstract: In the ensuing ultra-dense and diverse environment in future \ac{6G}
communication networks, it will be critical to optimize network resources via
mechanisms that recognize and cater to the diversity, density, and dynamicity
of system changes. However, coping with such environments cannot be done
through the current network approach of compartmentalizing data as distinct
from network operations. Instead, we envision a computing continuum where the
content of the transmitted data is considered as an essential element in the
transmission of that data, with data sources and streams analyzed and distilled
to their essential elements, based on their semantic context, and then
processed and transmitted over dedicated slices of network resources. By
exploiting the rich content and semantics within data for dynamic and
autonomous optimization of the computing continuum, this article opens the door
to integrating communication, computing, cyber-physical systems, data flow, and
AI, presenting new and exciting opportunities for cross-layer design. We
propose semantic slicing, a two-pronged approach that builds multiple virtual
divisions within a single physical and data infrastructure, each with its own
distinct characteristics and needs. We view semantic slicing as a novel shift
from current static slicing techniques, extending existing slicing approaches
such that it can be applied dynamically at different levels and categories of
resources in the computing continuum. Further it propels the advancement of
semantic communication via the proposed architectural framework.

</details>


### [18] [Zephyrus: Scaling Gateways Beyond the Petabit-Era with DPU-Augmented Hierarchical Co-Offloading](https://arxiv.org/abs/2510.11043)
*Yuemeng Xu,Haoran Chen,Jiarui Guo,Mingwei Cui,Qiuheng Yin,Cheng Dong,Daxiang Kang,Xian Wu,Chenmin Sun,Peng He,Yang Gao,Lirong Lai,Kai Wang,Hongyu Wu,Tong Yang,Xiyun Xu*

Main category: cs.NI

TL;DR: 提出Zephyrus，将DPU与Tofino统一P4流水线融合，采用分层协同卸载实现高比例硬件卸载并保留软件回退；在吞吐、功耗和成本上较前作有显著优势。


<details>
  <summary>Details</summary>
Motivation: 面对字节跳动云网关的千万亿比特级流量，传统Tofino网关资源受限，DPU市场扩展提供更大表容量与可编程性，故探索二者融合以应对增长的业务流量。

Method: 设计统一的P4流水线跨越Tofino与DPU，提出层次化协同卸载(HLCO)用于流量分配与复杂操作回退，同时在生产环境评估吞吐、能耗与TCO。

Result: 实现>99%硬件卸载，吞吐比LuoShen高33%，功耗低21%，硬件成本低14%；对比Albatross吞吐翻倍且TCO更低；并分享多年生产实践经验。

Conclusion: Zephyrus在生产环境中成功将DPU与Tofino网关集成，通过HLCO实现>99%硬件卸载并保留软件回退，较LuoShen和Albatross在吞吐、能耗和成本上具有明显优势，是构建高性能云网关的可行方案。

Abstract: Operating at petabit-scale, ByteDance's cloud gateways are deployed at
critical aggregation points to orchestrate a wide array of business traffic.
However, this massive scale imposes significant resource pressure on our
previous-generation cloud gateways, rendering them unsustainable in the face of
ever-growing cloud-network traffic. As the DPU market rapidly expands, we see a
promising path to meet our escalating business traffic demands by integrating
DPUs with our established Tofino-based gateways. DPUs augment these gateways
with substantially larger table capacities and richer programmability without
compromising previously low-latency and high-throughput forwarding. Despite
compelling advantages, the practical integration of DPUs into cloud gateways
remains unexplored, primarily due to underlying challenges. In this paper, we
present Zephyrus, a production-scale gateway built upon a unified P4 pipeline
spanning high-performance Tofino and feature-rich DPUs, which successfully
overcomes these challenges. We further introduce a hierarchical co-offloading
architecture (HLCO) to orchestrate traffic flow within this heterogeneous
gateway, achieving > 99% hardware offloading while retaining software fallback
paths for complex operations. Zephyrus outperforms LuoShen (NSDI '24) with 33%
higher throughput and our evaluation further indicates 21% lower power
consumption and 14% lower hardware cost. Against FPGA-based systems, Albatross
(SIGCOMM '25), it doubles the throughput at a substantially lower Total Cost of
Ownership (TCO), showcasing its superior performance-per-dollar. Beyond these
performance gains, we also share key lessons from several years of developing
and operating Zephyrus at production scale. We believe these insights provide
valuable references for researchers and practitioners designing performant
cloud gateways.

</details>


### [19] [From Prompts to Packets: A View from the Network on ChatGPT, Copilot, and Gemini](https://arxiv.org/abs/2510.11269)
*Antonio Montieri,Alfredo Nascita,Antonio Pescapè*

Main category: cs.NI

TL;DR: Study captures and analyzes mobile GenAI chatbot network traffic (ChatGPT, Copilot, Gemini), revealing distinct traffic patterns, protocol use (TLS/QUIC), and the importance of SNI for classification; GenAI traffic differs from messaging apps and stresses networks with sustained uplink; datasets released.


<details>
  <summary>Details</summary>
Motivation: GenAI chatbots are widespread but their network traffic is understudied; need to understand distinctiveness versus other traffic and implications for networks

Method: Traffic characterization and modeling of mobile GenAI chatbots

Result: Collected 60-hour generic and controlled datasets from Android apps (ChatGPT, Copilot, Gemini); performed trace/flow/protocol analyses; modeled packet sequences with Multimodal Markov Chains; found app- and content-specific patterns, TLS predominance, Gemini uses QUIC, ChatGPT TLS1.3 only; SNI important for classification

Conclusion: GenAI chatbots generate distinctive, app-and-content-dependent traffic that impacts mobile network usage and monitoring; SNI is a key feature for classification and masking it degrades performance; datasets available for further research.

Abstract: Generative AI (GenAI) chatbots are now pervasive in digital ecosystems, yet
their network traffic remains largely underexplored. This study presents an
in-depth investigation of traffic generated by three leading chatbots (ChatGPT,
Copilot, and Gemini) when accessed via Android mobile apps for both text and
image generation. Using a dedicated capture architecture, we collect and label
two complementary workloads: a 60-hour generic dataset with unconstrained
prompts, and a controlled dataset built from identical prompts across GenAI
apps and replicated via conventional messaging apps to enable one-to-one
comparisons. This dual design allows us to address practical research questions
on the distinctiveness of GenAI traffic, its differences from widely deployed
traffic categories, and its novel implications for network usage. To this end,
we provide fine-grained traffic characterization at trace, flow, and protocol
levels, and model packet-sequence dynamics with Multimodal Markov Chains. Our
analyses reveal app- and content-specific traffic patterns, particularly in
volume, uplink/downlink profiles, and protocol adoption. We highlight the
predominance of TLS, with Gemini extensively leveraging QUIC, ChatGPT
exclusively using TLS 1.3, and app- and content-specific Server Name Indication
(SNI) values. A payload-based occlusion analysis quantifies SNI's contribution
to classification: masking it reduces F1-score by up to 20 percentage points in
GenAI app traffic classification. Finally, compared with conventional messaging
apps when carrying the same content, GenAI chatbots exhibit unique traffic
characteristics, highlighting new stress factors for mobile networks, such as
sustained upstream activity, with direct implications for network monitoring
and management. We publicly release the datasets to support reproducibility and
foster extensions to other use cases.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: ISAAC 将 LLM 驱动的多智能体刺激生成与基于 FPGA 的并行共仿真结合，通过微结构感知的测试生成与前向快照+解耦架构，大幅加速 CPU 差分验证并捕获难触发缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统差分测试在 CPU 验证中受前端刺激质量低、冗余高和后端仿真瓶颈限制，导致覆盖难以闭合、角落情况漏检以及调试周期长。

Method: 前端采用多智能体刺激引擎，融合微结构知识与历史缺陷模式生成高针对性测试；后端引入轻量级前向快照机制以及 ISS 与 DUT 解耦共仿真，使单个 ISS 可驱动多 FPGA 上的 DUT 并行运行，消除长尾测试瓶颈。

Result: 在对一个已成熟且多次流片的 CPU 验证中，ISAAC 在仿真吞吐上相较软件 RTL 仿真最高达 17,536x 提速，并发现若干先前未知的缺陷（论文中报告了两例）。

Conclusion: 该论文提出了一个名为 ISAAC 的端到端 LLM 辅助 CPU 验证框架，通过前端多智能体微结构感知的刺激生成器和后端解耦的并行共仿真架构，显著提升了验证效率并发现真实缺陷。

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [21] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 提出一种可变精度systolic阵列ADiP，通过自适应处理单元和共享累加器提高矩阵乘法的吞吐与能效，在22nm实现上达到最高4×吞吐提升，并在GPT-2/BERT/BitNet任务上显著降低延迟与能耗。


<details>
  <summary>Details</summary>
Motivation: Reduce memory and computation demands of transformers by using quantization and reconfigurable architectures to dynamically adjust precision, improving data reuse and PE utilization.

Method: Adaptive-precision systolic array (ADiP)

Result: ADiP supports multiple computation modes (symmetric single-matrix and asymmetric multi-matrix with shared input), adapts to 8×8, 8×4, 8×2 precisions, achieves up to 4× higher computational throughput in 22nm, and shows latency improvement up to 53.6% and energy improvement up to 24.4% on transformer workloads; peak throughputs of 8.192, 16.384, 32.768 TOPS for respective precisions at 64×64 size with 4096 PEs.

Conclusion: ADiP通过多模计算支持与精度自适应，在保证数据重用和高PE利用率情况下，使矩阵乘法加速在吞吐和能效上得到显著提升，适配Transformer类模型的推理需求。

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [22] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 提出了面向边缘FPGA的量化感知多语种翻译系统，FP4量化实现显著的模型压缩与推理加速，同时降低硬件资源占用，适用于低资源语言的实时部署。


<details>
  <summary>Details</summary>
Motivation: 在物联网与边缘设备等资源受限场景下，需要在受限计算与存储资源上实时开展多语种翻译，尤其针对低资源语言，需要一种既高效又可移植的解决方案。

Method: 通过在不同子字节精度（FP8、INT8、INT4、FP4）上进行量化感知训练与评估，并结合针对FPGA的硬件优化以减少逻辑资源（LUT/FF）占用，从而在保持可接受翻译质量的前提下显著减小模型尺寸和提高推理吞吐量。

Result: 在最低精度FP4下模型体积减小约4.1倍，推理速度加速约4.2倍（吞吐量提高至66 token/s，约4.8倍提升）；FPGA部署相比OPU和HPTA分别实现约2.2倍和4.6倍的吞吐量提升，并在LUT和FF占用上分别降低约1.96倍和1.65倍。

Conclusion: 该论文提出了一种面向资源受限场景的轻量多语种翻译系统Bhasha-Rupantarika，通过算法-硬件协同设计和超低比特量化实现可部署性。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [23] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: Use high-density 3D FeNAND and hyperdimensional computing plus D-BAM to parallelize MS spectral library search in-storage, yielding major speed and energy gains while keeping accuracy similar.


<details>
  <summary>Details</summary>
Motivation: Mass spectrometry data growth makes large-scale spectral library search computationally challenging; ISP offers promise but NAND flash limitations hinder throughput; leverage FeNAND advantages and HDC parallelism to overcome serial-read bottleneck.

Method: ISP-based FeNAND HDC for MS library search

Result: Proposed ISP architecture using 3D FeNAND combined with hyperdimensional computing and a dual-bound approximate matching (D-BAM) metric achieves 43x speedup and 21x energy efficiency improvement over prior 3D NAND ISP methods, with comparable accuracy.

Conclusion: Integrating HDC with FeNAND ISP and a tailored D-BAM distance metric enables efficient, parallel MS spectral library search, overcoming traditional NAND throughput limits and significantly improving performance and energy efficiency without sacrificing accuracy.

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [24] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: 提出一种自动化框架，通过利用块对角稀疏性优化稀疏矩阵在计算内存阵列上的映射和调度，使CIM阵列利用率提升50%以上，内存占用与计算量减少4倍以上，从而加速稀疏LLM推理。


<details>
  <summary>Details</summary>
Motivation: Reduce inference cost of large language models on resource-constrained systems by combining structured sparsity and compute-in-memory architectures to eliminate off-chip data movement and improve efficiency.

Method: 设计自动化映射与调度策略，将稀疏矩阵重排为块对角结构并合理分配到CIM阵列以提升利用率，同时结合稀疏感知的调度减少冗余计算与数据移动。

Result: An automated framework with mapping and scheduling strategies exploiting block-diagonal sparsity to improve CIM array utilization by >50% and reduce memory footprint and FLOPs by >4x for sparse LLM inference on CIM accelerators.

Conclusion: 通过针对块对角稀疏性的映射与调度优化，CIM加速器能更高效地执行稀疏LLM推理，显著提升阵列利用率并降低存储与计算开销。

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>
