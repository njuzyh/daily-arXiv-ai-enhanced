<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 5]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: 本文提出了一种名为ISAAC的全栈式、大语言模型辅助的CPU验证框架，结合FPGA并行性，显著提升了验证效率和覆盖率，检测出多个先前未知的漏洞。


<details>
  <summary>Details</summary>
Motivation: CPU功能验证在集成电路开发中面临效率瓶颈，传统差分测试方法在激励生成和仿真基础设施方面存在不足，难以高效发现边缘情况和实现覆盖率收敛。

Method: ISAAC采用多智能体激励引擎，结合微架构知识和历史缺陷模式生成针对性测试；后端引入轻量级前向快照机制和解耦的协同仿真架构，支持单个指令集模拟器驱动多个被测设计并行运行。

Result: 在成熟CPU上的实验表明，相比软件RTL仿真，ISAAC实现了最高17,536倍的速度提升，并发现了多个此前未知的缺陷，其中两个在文中进行了报告。

Conclusion: ISAAC通过前端智能激励生成与后端并行仿真架构的协同优化，显著提高了CPU验证的效率与效果，为工业级CPU验证提供了可扩展的自动化解决方案。

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [2] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 本文提出了一种名为ADiP的新型自适应精度脉动阵列架构，用于高效加速矩阵乘法，支持多种计算模式和精度配置，在Transformer模型中实现了显著的延迟和能耗优化。


<details>
  <summary>Details</summary>
Motivation: 为了应对Transformer模型中矩阵乘法巨大的内存和计算需求，通过量化降低内存使用，并提升计算效率。

Method: 设计了包含NxN自适应精度处理单元和共享累加器的ADiP架构，支持对称单矩阵和非对称多矩阵乘法，动态调整精度（如8bitx8bit、8bitx4bit、8bitx2bit），并通过分析模型评估延迟与吞吐量。

Result: 在22nm工艺下实现最高4倍的计算吞吐量提升；在GPT-2 Medium、BERT Large和BitNet-1.58B等模型上，延迟最高降低53.6%，能量消耗降低24.4%；64x64规模下峰值吞吐量分别达到8.192 TOPS（8bitx8bit）、16.384 TOPS（8bitx4bit）和32.768 TOPS（8bitx2bit）。

Conclusion: ADiP架构通过自适应精度和高效的数据复用机制，显著提升了矩阵乘法的计算密度和能效，适用于多种Transformer工作负载。

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [3] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 本文提出了一种名为Bhasha-Rupantarika的轻量高效多语言翻译系统，通过算法与硬件协同设计，针对资源受限环境进行优化，支持亚字节精度部署（如FP4），显著减小模型体积、提升推理速度，并在FPGA上实现高性能低功耗的实时翻译。


<details>
  <summary>Details</summary>
Motivation: 为解决资源受限设备（如物联网设备）上多语言翻译模型部署的效率与性能瓶颈，推动低资源语言的实时AI翻译应用。

Method: 采用量化感知训练与算法-硬件协同设计，在FP8、INT8、INT4和FP4等亚字节精度下部署模型，并在FPGA上进行硬件加速优化。

Result: FP4精度下模型体积减少4.1倍，推理速度提升4.2倍，吞吐量达66 tokens/s（提升4.8倍）；FPGA部署相比OPU和HPTA分别实现2.2倍和4.6倍吞吐量提升，同时减少1.96倍LUTs和1.65倍FFs资源消耗。

Conclusion: Bhasha-Rupantarika通过超低精度量化与FPGA协同优化，为可部署的多语言AI系统提供了高效、可复现的解决方案，特别适用于低资源语言环境下的实时翻译需求。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [4] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 本文提出了一种基于3D铁电NAND（FeNAND）结构的存内计算（ISP）架构，结合超维计算（HDC）和双边界近似匹配（D-BAM）方法，实现了高效的大规模质谱库搜索，在速度和能效上分别提升了43倍和21倍，同时保持了较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着质谱数据量迅速增长（已达数百TB），传统处理器难以高效完成大规模谱库搜索任务，亟需更高效的计算架构来支持药物发现等应用。

Method: 设计了一种基于3D FeNAND的存内计算架构，利用其高密度、高速度和低电压优势；引入超维计算（HDC）实现高度并行处理，并结合专为FeNAND定制的双边界近似匹配（D-BAM）距离度量方法，实现向量计算的并行化。

Result: 相比当前最先进的3D NAND方法，实现了43倍的速度提升和21倍的能效提升，同时保持了相当的搜索准确性。

Conclusion: 该工作展示了FeNAND与超维计算结合在大规模质谱数据分析中的巨大潜力，为高效、低功耗的存内计算提供了新的可行路径。

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [5] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: 本文提出了一种自动化框架，利用新的映射和调度策略，在计算内存（CIM）加速器上加速稀疏大语言模型（LLM）的推理。通过利用块对角稀疏性，该方法将CIM阵列利用率提高了50%以上，并在内存占用和浮点运算次数上实现了4倍以上的减少。


<details>
  <summary>Details</summary>
Motivation: 尽管密集到稀疏的微调方法在减少模型大小的同时保持了可接受的准确性，但传统的冯·诺依曼架构在处理稀疏大语言模型推理时仍然面临高昂的内存开销。因此，需要一种更高效的方法来优化稀疏矩阵在计算内存架构上的映射和利用。

Method: 提出了一种自动化框架，结合新颖的映射和调度策略，利用块对角稀疏性来优化稀疏大语言模型在计算内存（CIM）加速器上的部署。

Result: 该方法将CIM阵列利用率提高了50%以上，并在内存占用和浮点运算次数上实现了超过4倍的减少。

Conclusion: 所提出的框架显著提升了稀疏大语言模型在CIM架构上的推理效率，为资源受限系统中的高效部署提供了可行解决方案。

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [Fine-grained CDN Delegation](https://arxiv.org/abs/2510.09983)
*Ethan Thompson,Ali Sadeghi Jahromi,AbdelRahman Abdou*

Main category: cs.NI

TL;DR: 本文提出了DeCerts，一种基于X.509标准扩展的细粒度CDN委托机制，允许域名所有者独立于证书颁发机构（CA）自主管理委托，提升安全性、可扩展性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的委托机制（如Delegated Credentials）在委托链长度、撤销机制、操作权限和委托范围等方面缺乏细粒度定义，且依赖CA，限制了灵活性和控制力。

Method: 通过修改X.509证书标准并添加新扩展，设计DeCerts，支持域名所有者指定被委托和非委托的子域名、控制委托深度，并实现无需CA的自主签发与撤销机制；同时在Firefox中实现验证逻辑以验证可行性。

Result: 实现了DeCerts的原型系统，在Firefox中完成集成与测试，验证了其与现有浏览器和TLS/HTTPS协议的兼容性与可行性。

Conclusion: DeCerts提供了一种安全、可扩展且易于管理的CDN委托解决方案，赋予域名所有者更高的自主权和策略控制能力，优于现有依赖CA的委托方法。

Abstract: The use of Content Delivery Networks (CDNs) has significantly increased over
the past decade, with approximately 55 million websites currently relying on
CDN services. Emerging solutions, such as Delegated Credentials (RFC 9345),
lack fine-grained definitions of many critical aspects of delegation, such as
the length of delegation chains, revocation mechanism, permitted operations,
and a well-defined scope for said delegation. We present Delegation
Certificates (DeCerts), which modify X.509 certificate standard and add new
extensions to enable fine-grained CDN delegation. DeCerts allow domain owners
to specify delegated and non-delegated subdomains, and control the depth of
delegation extended by CDNs, which provides flexibility in delegation
management. But more importantly, DeCerts are built on a new principle which
provides full autonomy to domain owners-domain owners can issue DeCerts fully
independent of Certificate Authorities (CAs), and thus have greater flexibility
in policy control, including revocation methods. Such level of flexibility
would be hard to match if CAs where to issue such certificates. Revoking a
DeCert revokes delegation. We discuss multiple revocation mechanisms for a
DeCerts balancing security, performance, and delegator control. We modify
Firefox to support DeCert (i.e., proper validation) as a proof-of-concept, and
test it to demonstrate the feasibility, compatibility of DeCerts with browsers
and TLS/HTTPS protocols. DeCerts enhance the security, scalability, and
manageability of CDN delegation, offering a practical solution for Internet
services.

</details>


### [7] [Pushing the Boundaries in CBRS Band: Robust Radar Detection within High 5G Interference](https://arxiv.org/abs/2510.10040)
*Shafi Ullah Khan,Michel Kulhandjian,Debashri Roy*

Main category: cs.NI

TL;DR: 本文研究了基于机器学习的频谱共享方法，用于提升公民宽带无线电服务（CBRS）频段中商业信号（如5G）与军用雷达系统共存的能力，实验表明ML模型可在低至-5dB SINR条件下实现99%雷达检测准确率，并以93%准确率识别六种雷达波形。


<details>
  <summary>Details</summary>
Motivation: 频谱资源日益紧张，如何在保障军用雷达等关键系统运行的同时实现商业频谱共享成为重大挑战，现有SINR限制制约了共享效率。

Method: 采用基于IQ数据和频谱图的机器学习模型，结合合成与真实信号数据，提升雷达检测与波形识别能力，突破FCC推荐的SINR边界限制。

Result: 在-5dB SINR条件下实现99%雷达检测准确率，较现有技术（约12dB）显著降低SINR要求；并以93%准确率完成六类雷达波形分类。

Conclusion: 机器学习可有效增强CBRS频段的频谱共享能力，显著改善高干扰环境下雷达系统的检测与识别性能，为未来动态频谱管理提供可行技术路径。

Abstract: Spectrum sharing is a critical strategy for meeting escalating user demands
via commercial wireless services, yet its effective regulation and
technological enablement, particularly concerning coexistence with incumbent
systems, remain significant challenges. Federal organizations have established
regulatory frameworks to manage shared commercial use alongside
mission-critical operations, such as military communications. This paper
investigates the potential of machine learning (ML)-based approaches to enhance
spectrum sharing capabilities within the Citizens Broadband Radio Service
(CBRS) band, specifically focusing on the coexistence of commercial signals
(e.g., 5G) and military radar systems. We demonstrate that ML techniques can
potentially extend the Federal Communications Commission (FCC)-recommended
signal-to-interference-plus-noise ratio (SINR) boundaries by improving radar
detection and waveform identification in high-interference environments.
Through rigorous evaluation using both synthetic and real-world signals, our
findings indicate that proposed ML models, utilizing In-phase/Quadrature (IQ)
data and spectrograms, can achieve the FCC-recommended $99\%$ radar detection
accuracy even when subjected to high interference from 5G signals upto -5dB
SINR, exceeding the required limits of $20$ SINR. Our experimental studies
distinguish this work from the state-of-the-art by significantly extending the
SINR limit for $99\%$ radar detection accuracy from approximately $12$ dB down
to $-5$ dB. Subsequent to detection, we further apply ML to analyze and
identify radar waveforms. The proposed models also demonstrate the capability
to classify six distinct radar waveform types with $93\%$ accuracy.

</details>


### [8] [Waves of Imagination: Unconditional Spectrogram Generation using Diffusion Architectures](https://arxiv.org/abs/2510.10044)
*Rahul Vanukuri,Shafi Ullah Khan,Talip Tolga Sarı,Gokhan Secinti,Diego Patiño,Debashri Roy*

Main category: cs.NI

TL;DR: 提出基于扩散模型的生成方法，合成包含LTE、5G和雷达信号的逼真CBRS频谱图，提升雷达检测模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 由于真实雷达信号稀少且标注困难，难以构建平衡的频谱图数据集，限制了AI模型在共享频段（如CBRS）中雷达检测性能与泛化能力。

Method: 采用基于扩散的生成模型合成五类包含LTE、5G和雷达信号的频谱图，并使用SSIM和PSNR指标评估生成数据的结构与统计保真度。

Result: 生成的频谱图在结构和统计特性上接近真实数据，且在真实雷达检测任务中，通过预训练使模型收敛速度加快51.5%。

Conclusion: 该方法有效缓解了真实雷达信号数据稀缺问题，提升了雷达检测模型的训练效率和可行性。

Abstract: The growing demand for effective spectrum management and interference
mitigation in shared bands, such as the Citizens Broadband Radio Service
(CBRS), requires robust radar detection algorithms to protect the military
transmission from interference due to commercial wireless transmission. These
algorithms, in turn, depend on large, diverse, and carefully labeled
spectrogram datasets. However, collecting and annotating real-world radio
frequency (RF) spectrogram data remains a significant challenge, as radar
signals are rare, and their occurrences are infrequent. This challenge makes
the creation of balanced datasets difficult, limiting the performance and
generalizability of AI models in this domain.
  To address this critical issue, we propose a diffusion-based generative model
for synthesizing realistic and diverse spectrograms of five distinct categories
that integrate LTE, 5G, and radar signals within the CBRS band. We conduct a
structural and statistical fidelity analysis of the generated spectrograms
using widely accepted evaluation metrics Structural Similarity Index Measure
(SSIM) and Peak Signal-to-Noise Ratio (PSNR), to quantify their divergence from
the training data. Furthermore, we demonstrate that pre-training on the
generated spectrograms significantly improves training efficiency on a
real-world radar detection task by enabling $51.5\%$ faster convergence.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: 本文提出了一种名为THEAS的动态资源适应算法，旨在异构系统中平衡性能与功耗，尤其适用于非绑定任务和负载波动大的实时场景。


<details>
  <summary>Details</summary>
Motivation: 在负载动态变化的异构系统中，传统调度算法难以兼顾能效与性能，因此需要一种能动态适应资源需求的调度机制。

Method: 提出THEAS算法，通过动态调整资源分配，在性能和功耗之间实现平衡，并在多种调度策略（如CFS、EAS等）之间进行对比分析。

Result: THEAS在适应性、核心选择、性能扩展、缓存感知、开销和实时适用性方面优于或媲美现有调度算法。

Conclusion: THEAS算法能有效提升异构系统中的能效和实时性能，具有广泛的应用前景。

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [10] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文综述了边缘计算架构及其在满足服务等级协议（SLA）方面的资源扩展挑战，介绍了现有的自动扩展算法的研究进展及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 为了应对边缘计算中由于微服务架构导致的低延迟和严格SLA需求带来的资源扩展挑战。

Method: 通过回顾边缘计算架构、其应用、优势及挑战，并介绍现有用于满足SLA的自动扩展算法的研究。

Result: 总结了当前边缘计算环境中为满足SLA而设计的算法的优点与不足。

Conclusion: 混合云和边缘环境是确保SLA合规的关键，但现有自动扩展算法仍存在性能问题和配置复杂性。

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [11] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE是一种面向推测性解码（SD）的专家卸载与计算-通信流水线框架，旨在解决MoE模型在多令牌验证过程中内存占用高和CPU-GPU带宽竞争的问题，显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE卸载系统未考虑推测性解码（SD）带来的内存和带宽瓶颈，导致在结合SD时性能受限，因此需要一种SD感知的优化框架来提升MoE模型的推理速度与资源利用率。

Method: SP-MoE提出了三项关键技术：推测性专家预取、截断层策略以及流水线运行时系统；通过利用草案模型与目标模型之间的结构对应关系预取专家，基于实证分析和延迟模型限制预取深度，并采用异步预取线程与批处理I/O实现延迟隐藏。

Result: 实验表明，SP-MoE在多种数据集、环境和MoE模型上相比现有最先进方法实现了1.07到3.5倍的每令牌时间（TPOT）加速。

Conclusion: SP-MoE是首个支持推测性解码的MoE专家卸载框架，有效缓解了多令牌验证中的内存与带宽压力，显著提升了大规模语言模型中MoE架构的推理效率。

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [12] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: 提出了一种名为FLAMMABLE的多模型联邦学习（MMFL）训练框架，通过智能调整客户端批量大小并根据其系统能力选择多个模型进行训练，显著提升了时间-精度性能和最终模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的单模型联邦学习框架无法有效应对MMFL中因数据、系统和模型异构性带来的挑战，亟需专门的解决方案。

Method: 设计了FLAMMABLE框架，动态适应客户端的系统能力，智能调整批大小，并在每轮训练中选择合适的多模型组合进行并行训练；同时构建了首个MMFL基准测试平台。

Result: 在多个数据集和模型上的实验表明，相比现有基线方法，FLAMMABLE将时间-精度性能提升1.1~10.0倍，最终模型准确率提高1.3%~5.4%。

Conclusion: FLAMMABLE有效解决了MMFL中的多重异构性挑战，显著提升了训练效率和模型性能，为未来MMFL研究提供了可复现的基准平台。

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [13] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: 本文提出了一种名为DCP的动态上下文并行训练框架，通过细粒度的块级数据与计算划分，灵活适应不同序列长度和注意力模式，有效减少通信开销并提升内存与计算负载均衡。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文并行方法采用静态配置，忽略了训练数据中序列长度和注意力模式的动态变化，导致通信开销大且计算负载不均衡。

Method: 提出DCP框架，引入细粒度的块级划分机制，支持数据和计算块在设备间的动态映射，以适应不同样本的序列特征。

Result: 微基准测试显示，在因果掩码下注意力计算加速1.19x~2.45x，稀疏注意力下加速2.15x~3.77x；端到端训练在因果掩码下提速0.94x~1.16x，稀疏掩码下提速1.00x~1.46x。

Conclusion: DCP通过动态、细粒度的并行策略，显著提升了长上下文训练的效率与资源利用率，适用于现代大模型的多样化训练需求。

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [14] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: 本文质疑在云原生应用中普遍使用CPU限制的必要性，提出其可能损害延迟敏感型应用的性能并增加成本，主张重新思考自动扩展和计费范式，并指出在特定场景（如后台任务）中合理使用CPU限制仍具价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究和实践普遍认为CPU限制对云原生应用的资源管理至关重要，但实际用户经验表明其可能损害性能并提高成本，与传统认知相悖，因此有必要重新评估其作用。

Method: 通过实证分析和案例研究，评估CPU限制对延迟敏感型应用的实际影响，并对比有无CPU限制下的性能与成本表现。

Result: 提供了支持完全摒弃延迟敏感型应用中CPU限制的实证证据，揭示了其负面影响，并指出了在特定场景下合理使用CPU限制的潜在益处。

Conclusion: 应摒弃对CPU限制的无差别采用，重新思考自动扩展和计费机制，推动新的研究方向，并在特定场景中审慎使用CPU限制。

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [15] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: 提出了一种无自旋、无内核锁的互斥锁，支持用户态调度器，形式化验证了其先进先出公平性和线性性。


<details>
  <summary>Details</summary>
Motivation: 解决传统互斥锁在协程运行时中缺乏公平性和与用户态调度器协作的问题。

Method: 设计了基于CSP/FDR的形式化验证方法，采用无锁队列管理等待进程，并提出claim/release协议管理进程间通信通道的竞争。

Result: 实现了FIFO公平性和线性化的互斥锁，通过CSP模型和FDR验证了协议与互斥锁规范的一致性。

Conclusion: 该互斥锁适用于协程运行时环境，其公平性验证方法和协议设计具有跨运行时设计的可重用性。

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [16] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 本文提出了一种新的集成分布式可靠系统框架FIDRS，通过采用异构分布式数据库技术和RMSD算法，提升了系统性能、效率和可靠性，并解决了以往框架的一些问题。


<details>
  <summary>Details</summary>
Motivation: 为了提高集成分布式系统的满意度、性能和可靠性，解决现有框架存在的问题。

Method: 分析现有框架，提出包含三个改进部分的新框架FIDRS，采用异构分布式数据库技术和RMSD算法，并通过仿真比较新旧框架的效果。

Result: 新框架在响应速度、性能、效率和可靠性方面均优于以往框架，有效减少了大数据库中的响应时间。

Conclusion: FIDRS框架显著提升了集成分布式系统的整体性能和可靠性，是一种有效的改进方案。

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [17] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: 本文探讨了分布式计算技术在大语言模型（LLM）中的应用，重点研究如何在消费级计算机上运行大型模型，并对三种先进的LLM服务技术进行了比较分析。


<details>
  <summary>Details</summary>
Motivation: 由于当前大语言模型规模巨大，单个计算节点难以完成训练、微调或推理任务，因此需要借助分布式计算技术来有效利用这些模型。

Method: 从两个角度研究分布式计算在LLM中的应用：一是通过基于元启发式的改进方法实现大模型在消费级硬件上的运行；二是对三种先进的LLM服务技术进行对比研究。

Result: 提出了一种新颖的元启发式优化方法以提升现有系统的性能，并通过实验对比了三种主流LLM服务技术的效果与优劣。

Conclusion: 分布式计算技术能够有效支持大语言模型的部署与应用，所提出的改进方法有助于降低大模型运行门槛，促进其在资源受限环境中的普及。

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [18] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: 本文分析了在现代多核架构上，非结构网格上离散坐标Sn输运求解器在共享内存并行化中的性能瓶颈，并提出了一种新的异步多任务（AMT）算法，以提升计算性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据依赖复杂、内存访问模式不规则以及高维问题域，非结构网格上的Sn输运求解器在扩展性方面面临挑战，因此需要改进共享内存并行效率。

Method: 通过分析现有求解器在多种计算硬件上的性能瓶颈，设计并实现了一种新的异步多任务（AMT）共享内存并行算法。

Result: 新AMT算法相比现有方法在计算性能上有所提升，并通过实验验证了其有效性。

Conclusion: 所提出的AMT算法能有效缓解Sn输运求解器在高核数多核架构上的性能瓶颈，提升了并行效率和可扩展性。

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>
