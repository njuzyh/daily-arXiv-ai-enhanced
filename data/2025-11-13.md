<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: A3GNN是一个面向异构CPU-GPU平台的经济、自适应且自动的GNN训练框架，通过局部性感知采样、细粒度并行调度和强化学习优化，显著提升资源受限设备上的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: GNN训练通常依赖昂贵的高性能计算平台，限制了其在资源受限场景下的广泛应用。作者希望在低成本设备上实现高效、可扩展的GNN训练。

Challenges: 在异构平台上高效利用CPU和GPU资源面临挑战，包括数据局部性管理、任务并行调度以及在吞吐量、内存占用和模型精度之间取得平衡。

Contributions: 提出了A3GNN框架，包含局部性感知采样、细粒度并行调度机制，并采用强化学习自动探索设计空间，实现多目标帕累托最优。

Results: 实验表明，使用七块Nvidia 2080Ti GPU的A3GNN在吞吐量上最高可达两块A100 GPU的1.8倍，同时保持极小的精度损失。

Conclusion: A3GNN有效缩小了低成本与高性能平台之间的性能差距，为资源受限环境下的GNN训练提供了高效、实用的解决方案。

Related Work: 相关工作包括GNN训练优化、异构计算资源调度、采样算法设计以及强化学习在系统优化中的应用。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [2] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 本文探讨了大规模语言模型推理中的瓶颈问题，提出通过解耦预填充和解码阶段实现解聚式推理架构，以优化延迟、吞吐量和成本。


<details>
  <summary>Details</summary>
Motivation: 随着大模型参数规模的增长，推理阶段的延迟、吞吐和成本成为部署的主要瓶颈，传统单体GPU集群难以有效应对。

Challenges: LLM推理面临内存带宽、计算吞吐和延迟的多重约束，同时需要在多个目标之间进行权衡优化。

Contributions: 提出了基于服务解耦、资源解聚和工作负载划分的解聚式推理架构，将预填充与解码阶段分离并独立扩展。

Results: 该架构有效缓解了资源争用，可独立优化首令牌时间和令牌间延迟等关键指标。

Conclusion: 解聚式推理是应对大规模语言模型部署挑战的有效范式，有助于实现高效、可扩展的LLM服务。

Related Work: 相关工作包括分布式推理系统、GPU资源调度、以及针对LLM推理阶段特性的优化方法。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [3] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: Synera提出了一种设备-云协同的LLM服务系统，通过高效的SLM-LLM协同机制，优化了移动场景下大语言模型的生成质量与延迟问题，在保持低延迟的同时显著提升了生成质量，并降低了云服务成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在移动端应用日益广泛，但面临生成质量下降和延迟增加的问题。传统的云卸载或本地小模型方案存在通信瓶颈或质量牺牲的局限，亟需更高效的协同推理方案。

Challenges: 主要挑战包括：如何做出高效的卸载决策，避免流水线停顿，解决批处理瓶颈，以及在资源受限的设备和高通信开销之间取得平衡。

Contributions: 本文提出了Synera系统，贡献包括：1）识别了设备-云协同推理中的优化机会；2）设计了通信高效的 selective offloading、无阻塞并行推理和可扩展的云批处理机制；3）实现了质量、延迟和成本的综合优化。

Results: 实验表明，Synera相比基线方法在相同延迟下提升了1.20-5.47倍的生成质量，并在不同基准上比现有云服务降低了8.2%-16.5%的云服务成本。

Conclusion: Synera通过设备-云协同机制有效解决了移动端LLM服务的质量与效率问题，为未来移动端智能应用提供了高性能、低成本的部署方案。

Related Work: 相关工作主要包括基于云的模型卸载方法和本地小语言模型（SLM）的部署方案，但这些方法分别受限于通信瓶颈和模型性能下降，未能充分平衡质量与效率。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [4] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 本研究对25个量化开源大语言模型在树莓派4、树莓派5和Orange Pi 5 Pro三种单板计算机上的推理性能进行了基准测试，比较了Ollama和Llamafile两种推理运行时的生成吞吐量、内存使用和功耗表现。结果表明，单板计算机可稳定支持最高15亿参数的模型，且Llamafile在吞吐量和能效上显著优于Ollama。研究揭示了架构级瓶颈与运行时权衡，并提出了实用部署建议，填补了高性能语言模型与低成本边缘计算之间的空白。


<details>
  <summary>Details</summary>
Motivation: 随着对设备端大语言模型推理需求的增长，亟需在低成本边缘硬件上部署轻量级AI解决方案。单板计算机（如树莓派）具备本地化、隐私保护的优势，但在大模型推理场景下的性能表现尚缺乏系统评估，因此需要开展实证研究以指导实际部署。

Challenges: 单板计算机计算资源有限，难以支持大规模语言模型的高效推理；不同硬件架构和推理运行时之间的性能差异显著；在低功耗条件下实现高吞吐量存在瓶颈；缺乏针对SBC平台的系统性LLM性能基准数据。

Contributions: 1）首次对25个量化开源LLM在三种主流单板计算机上的推理性能进行全面基准测试；2）对比分析Ollama与Llamafile两种运行时在吞吐量、内存和功耗方面的表现差异；3）识别出硬件架构与运行时层面的关键性能瓶颈；4）提供面向实际部署的优化建议，推动大模型在边缘设备上的落地应用。

Results: 实验结果显示：1）单板计算机可稳定运行最高1.5B参数的量化模型；2）Llamafile相比Ollama吞吐量最高提升4倍，功耗降低30-40%；3）CPU配置和模型大小对性能影响显著；4）不同提示类型下的推理效率存在差异，反映出现实工作负载下的性能波动。

Conclusion: 单板计算机能够有效支持中小型量化大语言模型的本地推理，尤其在使用高效运行时（如Llamafile）时表现出良好的性能与能效。该研究验证了低成本边缘设备运行LLM的可行性，为隐私敏感、资源受限场景下的AI部署提供了实践依据。

Related Work: 相关工作主要集中在服务器级GPU上的大模型推理优化，以及移动端或嵌入式设备上的轻量级模型部署。已有研究较少关注单板计算机平台上的开源LLM性能表现，缺乏跨硬件、跨运行时的系统性比较，本文填补了这一空白。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [5] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对Model Context Protocol (MCP) 在大语言模型（LLM）中的应用进行了基于测量的分析，揭示了功能、性能与成本之间的权衡，并探讨了不同模型和配置对令牌效率、成本、任务完成时间和成功率的影响，提出了并行工具调用和任务中止机制等优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP增强了LLM与外部工具交互的能力，但引入大量上下文信息导致令牌使用量激增，带来高昂的成本和计算负担，因此需要系统性评估其代价与收益。

Challenges: MCP带来的上下文膨胀显著增加了token消耗，导致服务成本上升和性能下降，如何在保持功能增强的同时提升效率和降低成本是一大挑战。

Contributions: 本文首次对MCP启用的LLM交互进行了全面的测量分析，量化了不同模型和配置下的性能与成本权衡，并提出了可行的优化策略，如并行工具调用和任务中止机制。

Results: 实验结果显示不同LLM模型和MCP配置在token效率、成本、任务完成时间和成功率方面存在显著差异；启用并行工具调用可提升效率，而有效的任务中止机制有助于控制资源浪费。

Conclusion: 尽管MCP提升了LLM的能力，但其高成本和性能开销不容忽视；通过合理配置和优化策略，可以在保证功能的同时实现更高效、经济的MCP集成。

Related Work: 相关工作主要集中在LLM的上下文优化、工具调用机制设计以及API成本管理，但缺乏针对MCP协议的系统性实证分析。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [6] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: 本文提出DynaKV，一种面向智能手机的自适应KVCache管理方法，通过迁移无关的聚类自适应、连续性感知的闪存管理和高效的缓存设计，显著提升长序列解码的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于DRAM容量有限，智能手机上的长序列大语言模型解码受到KVCache内存占用的限制。现有基于检索的方法因聚类索引静态或局部更新，无法适应KVCache分布变化，导致检索不准确和效率低下。

Challenges: 1) KVCache分布随解码过程动态变化，静态聚类易失效；2) 智能手机带宽、IOPS和内存受限；3) 传统方法在更新聚类时引入额外数据迁移开销。

Contributions: 1) 提出首个面向手机的自适应KVCache管理框架DynaKV；2) 设计无需迁移的动态聚类分裂机制；3) 提出连续性中心化的闪存布局与双头结构；4) 实现跨DRAM与闪存的虚拟缓存与集群感知替换策略。

Results: 实验表明，DynaKV相比现有最先进方法平均提升1.38倍检索准确率，并实现1.47倍的端到端加速。

Conclusion: DynaKV有效解决了智能手机上长序列LLM解码中KVCache管理的准确性与效率问题，具备向其他长上下文任务和多层内存系统扩展的潜力。

Related Work: 基于聚类的KVCache检索方法，如vLLM、PagedAttention等，采用静态或局部更新聚类策略，在动态负载下性能受限。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [7] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: 本文提出了HyProv，一种混合型溯源管理系统，结合集中式与联邦式架构，支持可扩展、实时且具备工作流感知能力的溯源查询。该系统在Airflow与Kubernetes环境中验证，表现出低延迟、高扩展性及低资源开销。


<details>
  <summary>Details</summary>
Motivation: 随着科学工作流日益复杂并运行在分布式集群上，现有溯源系统难以兼顾可扩展性、实时处理、在线分析及跨组件集成。此外，多数系统缺乏工作流感知能力，无法利用工作流结构进行优化。

Challenges: 如何在大规模分布式环境中实现低延迟、可扩展的实时溯源数据收集与集成；如何设计一个既支持复杂查询又具备工作流感知能力的溯源系统；如何平衡集中式管理的效率与联邦式架构的可扩展性。

Contributions: 提出了HyProv，一种结合集中式与联邦式架构的混合溯源管理系统；设计了针对工作流规范的小规模稳定溯源数据的集中管理机制；实现了对大规模执行日志的联邦查询支持；支持复杂溯源查询并在Airflow+Kubernetes环境下验证了系统的有效性。

Results: 实验表明，HyProv能够扩展到大规模工作流；溯源查询响应时间在亚秒级；对集群的CPU和内存开销较小；在真实场景中实现了高效的在线溯源分析。

Conclusion: HyProv通过融合集中式与联邦式架构，有效解决了复杂科学工作流中溯源管理的可扩展性与实时性难题，具备工作流感知能力，能够在低开销下支持复杂查询，适用于现代分布式工作流系统。

Related Work: 相关工作包括传统的集中式溯源系统（如ProvSQL）、联邦式溯源框架以及针对特定工作流系统的监控工具（如Apache Airflow自带的元数据记录），但这些方法通常缺乏对工作流结构的利用或无法支持实时在线分析。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [8] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 该论文探讨了利用本地小型语言模型（≤20B参数）在本地加速器上进行推理，以缓解集中式云基础设施的压力。通过对20多个先进本地模型、8种加速器和100万真实单轮对话与推理查询的实证研究，提出“每瓦智能”（IPW）作为衡量本地推理能力与效率的关键指标。结果显示：本地模型可准确回答88.7%的查询；2023–2025年IPW提升5.3倍，查询覆盖率达71.3%；本地加速器IPW仍低于云端，存在优化空间。研究表明本地推理具备显著分流潜力，并开源IPW评测工具。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型查询需求激增，集中式云基础设施面临扩展瓶颈。与此同时，小型语言模型性能提升和本地硬件算力增强为将部分推理任务下放到终端设备提供了可能。本文旨在评估本地推理是否能在保证准确性的前提下，有效分担云端负载。

Challenges: 主要挑战包括：如何在资源受限的本地设备（如笔记本电脑）上高效运行语言模型；如何平衡模型准确性与能耗；缺乏统一指标来评估不同模型-加速器组合在本地环境下的综合表现。

Contributions: 1) 提出“每瓦智能”（Intelligence Per Watt, IPW）作为衡量本地推理效能的核心指标；2) 构建大规模实证研究框架，涵盖20多个本地模型、8种加速器和100万真实查询；3) 发布IPW性能分析工具，支持系统化的能效基准测试。

Results: 实验结果表明：1) 本地语言模型可准确回答88.7%的真实单轮聊天与推理查询，表现因任务领域而异；2) 从2023到2025年，IPW提升了5.3倍，本地可处理的查询覆盖率从23.2%上升至71.3%；3) 相同模型下，本地加速器的IPW至少比云加速器低1.4倍，显示本地软硬件协同优化仍有较大提升空间。

Conclusion: 本地推理已具备显著分流云端LLM请求的潜力，尤其在能效持续改进的趋势下。IPW是一个有效的评估指标，可用于指导未来本地模型与硬件的设计与优化。研究证实，通过提升本地设备的智能处理能力，可在不牺牲用户体验的前提下减轻对集中式基础设施的依赖。

Related Work: 相关工作包括边缘计算中的模型压缩与量化技术、移动端推理优化、小型语言模型的能力评估，以及能效相关的硬件-软件协同设计研究。本文区别于以往工作的关键在于结合真实查询流量，系统性地评估多维度指标（准确率、延迟、能耗、IPW）。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [9] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: 本文提出了一种名为ProbSelect的新方法，用于在边缘、云和空间设备构成的三维连续体中进行联邦学习的客户端选择。该方法基于分析建模和概率预测，无需历史数据或持续监控，适用于GPU加速训练环境，并在满足SLO的同时显著减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 在包含卫星、移动设备和云端的动态三维计算连续体中，传统依赖历史数据和持续监控的客户端选择方法难以适用，且现有方法多局限于CPU计算模型，无法反映GPU加速训练的复杂特性。因此，需要一种不依赖历史数据、适应动态环境并支持GPU架构的新型客户端选择机制。

Challenges: 主要挑战包括：1）在设备状态频繁变化的动态环境中难以获取可靠的运行时信息；2）缺乏对GPU加速训练特性的建模能力；3）需在满足服务等级目标（SLO）的前提下高效选择客户端；4）避免因频繁监控带来的额外开销。

Contributions: 1）提出ProbSelect，一种无需历史数据和持续监控的客户端选择方法；2）构建适用于GPU加速环境的分析模型与概率预测机制；3）将客户端选择问题建模为满足用户定义SLO的优化问题；4）在多种GPU架构和工作负载下验证了方法的有效性。

Results: 实验结果显示，ProbSelect相比基线方法平均提升了13.77%的SLO合规率，并实现了72.5%的计算浪费减少，表现出在异构动态环境中的优越性能。

Conclusion: ProbSelect通过结合分析建模与概率预测，在不依赖历史数据的情况下有效解决了三维连续体中联邦学习的客户端选择问题，尤其适用于GPU加速、高动态性的场景，显著提升了系统效率与SLO满足度。

Related Work: 相关工作主要包括联邦学习中的客户端选择策略、基于历史数据的资源预测方法以及面向边缘计算的SLO管理机制，但这些方法大多依赖持续监控和CPU计算假设，难以适应本文所考虑的三维动态GPU环境。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [10] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在OBLOT模型下，机器人在顶点和边对称图上进行聚集的问题，考虑了初始配置存在多重性、机器人无法检测多重性等挑战，并设计了针对无限网格和超立方体两种拓扑结构的时间最优聚集算法。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器人系统中，聚集问题是核心挑战之一。本文旨在解决在限制性较强（如无法检测多重性、对称图结构）的环境下，如何实现机器人聚集的问题。

Challenges: 主要挑战包括：初始配置可能存在多个机器人位于同一顶点（多重性），机器人无法感知这种多重性，且只能在顶点和边对称的图上移动；同时需在轮询调度机制下设计有效算法。

Contributions: 提出了在无限网格和超立方体图上的两种时间最优聚集算法；给出了若干基本的不可解性结果；并推测不存在适用于所有可解情况的通用算法。

Results: 设计的算法在特定拓扑（无限网格和超立方体）上实现了时间最优的聚集，充分利用了图的对称性结构；同时证明了一些情况下聚集问题无法解决。

Conclusion: 由于问题环境的限制性和拓扑依赖性，聚集问题在一般情况下可能不存在通用解，需针对特定图结构设计专用算法。

Related Work: 相关工作主要集中在OBLOT模型下的分布式机器人聚集问题，尤其是在离散图上的研究，以及不同感知能力和移动能力下的可解性分析。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [11] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 本文提出了一种基于约束规划的Kubernetes调度优化方法，作为默认调度器的插件，在默认调度失败时进行回退处理。实验表明，该方法在部分场景下能更优地分配高优先级Pod，并可验证默认调度的最优性。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用启发式算法可能导致Pod分配次优和资源碎片化，影响集群资源利用率和高优先级任务部署。

Challenges: 在保证调度效率的同时实现资源分配的最优化；处理不同优先级和资源需求的Pod的复杂约束；在有限时间内求解大规模调度问题。

Contributions: 提出了一种基于约束编程的Kubernetes Pod调度优化方案；实现了与默认调度器集成的插件式架构；通过OR-Tools求解器在1秒和10秒时间窗口内验证了调度性能提升。

Results: 实验结果显示，在小到中等规模集群中，1秒调度窗口内该方法在44%以上可实现的分配场景中优于默认调度器，并在19%以上场景中验证默认调度已是最优；10秒窗口内优化比例提升至73%以上。

Conclusion: 基于约束编程的方法能有效提升Kubernetes调度质量，在合理时间内找到更优或验证现有调度的最优性，适合作为默认调度器的补充机制。

Related Work: 相关工作包括Kubernetes默认调度器机制、资源分配启发式算法、约束编程在调度中的应用以及集群资源优化技术。

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: Re$^{2}$MaP是一种通过递归原型构建和基于打包树重定位生成专家级宏布局的方法，结合PPA感知聚类、椭圆上角度优化与进化搜索，在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统宏布局方法在处理复杂设计约束和优化时序、功耗、布线等方面存在不足，难以兼顾全局布局质量与局部优化精度，因此需要一种能模拟专家经验并自动化生成高质量布局的新方法。

Challenges: 如何在多层级宏分组与单元聚类中统一建模连接性与数据流；如何在保证布线长度和数据流优化的同时将宏均匀分布于芯片边缘；如何在满足多种设计约束的前提下实现宏组及其内部宏的联合优化；如何通过迭代提升原型精度以支持更优的最终布局。

Contributions: 提出了Re$^{2}$MaP方法，包含：1）多级宏分组与PPA感知单元聚类构建统一连接矩阵；2）利用DREAMPlace生成混合尺寸布局原型；3）提出ABPlace角度优化方法在椭圆上分布宏位置；4）设计基于打包树的重定位策略，结合专家启发式代价函数与进化搜索进行联合优化；5）采用递归迭代策略逐步优化子集宏组以提升整体布局质量。

Results: 在七个测试案例中，相比最先进的学术工具Hier-RTLMP，Re$^{2}$MaP在最差负时序裕量（WNS）上最高提升22.22%（平均10.26%），总负时序裕量（TNS）最高改善97.91%（平均33.97%）；同时在WNS、TNS、功耗、DRC违规数和运行时间方面均优于其会议版本ReMaP。

Conclusion: Re$^{2}$MaP通过递归原型构建与基于专家经验的打包树重定位，实现了高质量的宏布局自动化，显著优于现有方法，并在多项关键指标上表现出更强的综合性能，具备实际应用潜力。

Related Work: 与Hier-RTLMP等分层宏布局方法相比，Re$^{2}$MaP引入了更精细的PPA感知聚类与椭圆角度优化；相较于传统解析布局方法（如DREAMPlace），其结合了ABPlace与进化搜索以增强约束满足能力；与早期ReMaP相比，通过递归机制提升了布局精度与整体性能。

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [13] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的硬件高效RBD加速器，通过精度感知量化、延迟除法优化和模块间DSP复用三项创新，显著提升了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 为了提升高自由度机器人系统中刚体动力学（RBD）计算的效率，解决现有加速器在计算精度、延迟和硬件资源利用方面的不足。

Challenges: 如何在保证机器人控制和运动精度的前提下，降低对DSP资源的需求，同时减少关键路径延迟并提高硬件利用率。

Contributions: 1) 提出了精度感知量化框架，首次系统评估了量化对机器人控制和运动的影响；2) 引入除法延迟优化技术，解耦倒数运算与最长延迟路径；3) 提出模块间DSP复用方法，提高DSP利用率。

Results: 实验结果表明，相比最先进的RBD加速器，该方法在多种机器人类型上实现了最高8倍的吞吐量提升和7.4倍的延迟降低。

Conclusion: 所提出的RBD加速器在性能、延迟和硬件效率方面显著优于现有方案，具有良好的可扩展性，适用于高自由度机器人系统。

Related Work: 现有研究主要集中在通用加速架构或未充分优化的RBD实现，缺乏对量化影响的系统分析及关键路径优化。

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


### [14] [CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices](https://arxiv.org/abs/2511.08575)
*Zhenxiao Fu,Chen Fan,Lei Jiang*

Main category: cs.AR

TL;DR: 本文提出了CO2-Meter，一个用于估算边缘设备上大语言模型推理过程中运行时和内含碳排放的统一框架，通过更全面的能耗建模和验证，有效识别碳排放热点并指导绿色LLM设计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在边缘设备上的部署带来了显著的碳排放挑战，而现有碳排放估算方法不完整，忽略了外设能耗、预填充/解码阶段差异以及SoC设计复杂性。

Challenges: 1. 外围设备能耗的准确建模；2. 区分LLM推理中预填充和解码阶段的不同能耗行为；3. 考虑SoC系统级设计复杂性对碳排放的影响；4. 构建包含运行时能耗与硬件制造内含碳的统一估算框架。

Contributions: 1. 基于方程的外设能耗模型与数据集；2. 结合相位特定LLM能耗数据的GNN预测器；3. 用于SoC瓶颈分析的单元级内含碳模型；4. 验证表明其在准确性上优于现有方法。

Results: CO2-Meter在多种边缘平台和LLM配置下验证，展现出比现有方法更高的估算精度；案例研究证明其能有效识别碳排放热点，支持在边缘设备上进行可持续的LLM部署与优化设计。

Conclusion: CO2-Meter为边缘LLM推理提供了更全面、精确的碳足迹评估方案，有助于推动绿色AI的发展，支持从算法到硬件的协同可持续设计。

Related Work: 相关工作主要集中在数据中心级LLM碳排放估算或简化的能耗模型，缺乏对边缘设备中外围组件、推理阶段差异及SoC层面硬件碳成本的系统性建模。

Abstract: LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices](https://arxiv.org/abs/2511.07770)
*Zewei Guo,Zhen Jia,JinXiao Zhu,Wenhao Huang,Yin Chen*

Main category: cs.NI

TL;DR: 本文提出了一种大规模、同型号设备的射频指纹数据集和开源实验框架，用于解决现有数据集设备规模小、型号异构的问题。该数据集包含123个相同的IEEE 802.11g设备，采集了3542万条原始I/Q样本和185万条射频特征，并通过随机森林算法实现了89.06%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 由于同型号设备硬件差异极小，区分它们的射频指纹仍具挑战性；同时现有数据集规模小且设备型号多样，限制了机器学习模型的鲁棒训练与公平评估。

Challenges: 同型号设备间硬件差异微弱，难以提取有效区分特征；缺乏大规模、同型号设备数据集支持模型训练与评估。

Contributions: 1）构建了包含123个相同型号设备的大规模射频指纹数据集，含3542万I/Q样本和185万特征；2）提出完全可复现的开源实验框架，覆盖数据采集到评估全流程；3）设计基于随机森林的识别算法，在该数据集上达到89.06%准确率。

Results: 在大规模同型号设备数据集上，基于随机森林的算法实现了89.06%的设备识别准确率；实验验证了所提取特征之间的关联性，证明了特征的有效性与稳定性。

Conclusion: 本文提供的数据集和开源框架为射频指纹技术的研究提供了重要基础，特别是在同型号设备识别方面，支持更公平、可复现的机器学习模型评估。

Related Work: 已有研究多基于小规模或异构设备数据集进行射频指纹分析，缺乏对同型号设备间细微差异的有效建模与评估环境，本文填补了这一空白。

Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.

</details>


### [16] [SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services](https://arxiv.org/abs/2511.08282)
*Eranga Bandara,Safdar H. Bouk,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Peter Foytik,Ross Gore,Xueping Liang,Ng Wee Keong,Kasun De Zoysa*

Main category: cs.NI

TL;DR: 本文提出了一种结合生成式AI、联邦学习、区块链和NFT的新型SRE平台SRE-Llama，用于自动化生成和管理云原生环境中的SLI/SLO及告警机制，并通过5G核心网用例验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 开发人员普遍缺乏对Prometheus、Grafana等监控工具以及SLI/SLO定义的深入理解，导致SRE实践困难，因此需要一个智能化、易用的自动化平台来降低门槛并提升效率。

Challenges: 主要挑战包括：开发者对SRE工具和指标定义的理解不足；在保护数据隐私的前提下有效识别关键性能指标；实现SLI/SLO的智能生成与动态管理；确保SRE决策过程的可审计性与不可篡改性。

Contributions: 1) 提出SRE-Llama平台，集成生成式AI、联邦学习、区块链与NFT技术；2) 利用联邦学习在保护隐私的同时识别关键SLI；3) 使用微调后的Llama-3大模型自动生成SLI/SLO、错误预算和告警规则；4) 将SLI/SLO编码为NFT存储于区块链，实现不可篡改的审计追踪；5) 通过智能合约驱动自动化流程，并在Open5GS 5G核心网环境中实现原型验证。

Results: 平台成功实现了从云原生服务采集指标、使用联邦学习分析关键SLI、利用Llama-3生成SLO与告警策略、将结果编码为NFT上链存储的全流程自动化，并在一个定制化的Open5GS 5G核心网用例中完成了原型部署与验证。

Conclusion: SRE-Llama通过融合前沿技术有效降低了SRE实践门槛，提升了SLI/SLO定义的智能化水平，同时增强了系统的透明性、安全性和可审计性，展示了在复杂云原生环境中应用AI与区块链进行自动化运维的潜力。

Related Work: 相关工作主要包括传统的SRE实践（如Google SRE手册）、基于Prometheus/Grafana的监控系统、使用机器学习进行异常检测的研究、区块链在日志审计中的应用，以及NFT在数字资产确权方面的探索。本文在这些基础上首次将生成式AI、联邦学习与NFT结合应用于SRE指标自动化生成与管理。

Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.

</details>


### [17] [Demystifying QUIC from the Specifications](https://arxiv.org/abs/2511.08375)
*Darius Saif,Ashraf Matrawy*

Main category: cs.NI

TL;DR: 本文旨在以通俗易懂的方式全面介绍QUIC协议，帮助读者理解其规范，消除因协议快速演进、文档复杂及跨层隐私设计带来的学习障碍。


<details>
  <summary>Details</summary>
Motivation: 由于QUIC协议发展迅速、文档分散且实现复杂，初学者难以掌握，因此需要一篇系统而易懂的综述性介绍。

Challenges: 学习QUIC的主要困难包括：协议快速演进（如标准QUIC与已弃用的Google QUIC的区别）、多个RFC文档结构复杂、语言晦涩，以及其跨层和以隐私为中心的设计导致无法仅通过数据包分析来理解或调试。

Contributions: 本文系统性地梳理并简化了QUIC协议的核心内容，提供了一种易于理解的讲解方式，帮助读者克服学习障碍，深入理解QUIC的设计与实现。

Results: 论文成功地将复杂的QUIC协议以清晰、连贯和可访问的方式呈现，有助于学术界和工业界更好地理解和应用QUIC。

Conclusion: 通过本文的梳理，QUIC协议的复杂性得以降低，为后续研究和实践提供了坚实的基础。

Related Work: 相关工作主要集中在TCP、TLS以及HTTP/2等传统协议的演进，而QUIC作为融合传输与安全的新一代协议，代表了互联网协议发展的新方向。

Abstract: QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.

</details>
