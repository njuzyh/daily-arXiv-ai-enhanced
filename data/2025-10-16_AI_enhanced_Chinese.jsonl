{"id": "2510.12889", "categories": ["cs.DC", "C.2.4"], "pdf": "https://arxiv.org/pdf/2510.12889", "abs": "https://arxiv.org/abs/2510.12889", "authors": ["Wei Da", "Evangelia Kalyvianaki"], "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters", "comment": "single column,20 pages and 8 figures", "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.", "AI": {"tldr": "Dodoor\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u5668\uff0c\u5229\u7528\u6279\u5904\u7406\u7f13\u5b58\u4fe1\u606f\u548c\u65b0\u578b\u8d1f\u8f7d\u8bc4\u5206\u673a\u5236\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u7684\u4efb\u52a1\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u9700\u8981\u9ad8\u6548\u3001\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u53bb\u4e2d\u5fc3\u5316\u4efb\u52a1\u8c03\u5ea6\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u5f02\u6784\u96c6\u7fa4\u4e2d\u52a8\u6001\u591a\u7ef4\u8d44\u6e90\u9700\u6c42\u7684\u6311\u6218\u3002", "challenges": "\u5982\u4f55\u5728\u4e0d\u4f9d\u8d56\u5b9e\u65f6\u63a2\u6d4b\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u8bc4\u4f30\u670d\u52a1\u5668\u8d1f\u8f7d\uff0c\u5e76\u5728\u5f02\u6784\u73af\u5883\u4e2d\u6709\u6548\u8c03\u5ea6\u5177\u6709\u591a\u7ef4\u8d44\u6e90\u9700\u6c42\u7684\u4efb\u52a1\u3002", "contributions": "\u63d0\u51fa\u4e86Dodoor\u8c03\u5ea6\u5668\uff0c\u5f15\u5165\u57fa\u4e8e\u6279\u5904\u7406\u7684\u7f13\u5b58\u66f4\u65b0\u673a\u5236\u548c\u65b0\u7684\u8d1f\u8f7d\u8bc4\u5206\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u4efb\u52a1\u4e0e\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u53cd\u4eb2\u548c\u6027\u3002", "results": "\u5728101\u8282\u70b9\u5f02\u6784\u96c6\u7fa4\u4e0a\uff0cDodoor\u51cf\u5c11\u4e8655-66%\u7684\u8c03\u5ea6\u6d88\u606f\uff0c\u541e\u5410\u91cf\u63d0\u5347\u8fbe33.2%\u548c21.5%\uff0c\u5e73\u5747makespan\u5ef6\u8fdf\u964d\u4f4e12.1%\u548c7.2%\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u6539\u558421.9%\u548c24.6%\u3002", "conclusion": "Dodoor\u901a\u8fc7\u6279\u5904\u7406\u7f13\u5b58\u548c\u65b0\u578b\u8d1f\u8f7d\u8bc4\u5206\u673a\u5236\uff0c\u5728\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6548\u7387\u548c\u7cfb\u7edf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u4e2d\u5fc3\u3002", "related_work": "\u57fa\u4e8e\u52a0\u6743\u7403\u76d2\u6a21\u578b\u548c\u6279\u91cf\u8bbe\u7f6e\u7684\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u7814\u7a76\uff0c\u4ee5\u53ca\u4f20\u7edf\u7684\u57fa\u4e8e\u5f85\u5904\u7406\u4efb\u52a1\u6570\u7684\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u3002"}}
{"id": "2510.13203", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13203", "abs": "https://arxiv.org/abs/2510.13203", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "Scrutiny new framework in integrated distributed reliable systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed\nsystems. In the proposed framework we have used three parts to increase\nSatisfaction and Performance of this framework. At first we analyse integrated\nsystems and their evolution process and also ERPSD and ERPDRT framework briefly\nthen we explain the new FDIRS framework. Finally we compare the results of\nsimulation of the new framework with presented frameworks. Result showed In\nFIDRS framework, the technique of heterogeneous distributed data base is used\nto improve Performance and speed in responding to users. Finally by using FDIRS\nframework we succeeded to increase Efficiency, Performance and reliability of\nintegrated systems and remove some of previous frameworks problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u96c6\u6210\u7cfb\u7edf\u6846\u67b6FDIRS\uff0c\u901a\u8fc7\u91c7\u7528\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u54cd\u5e94\u901f\u5ea6\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u4ee5\u5f80\u6846\u67b6\u7684\u4e00\u4e9b\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6ee1\u610f\u5ea6\u548c\u6027\u80fd\uff0c\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5b58\u5728\u7684\u95ee\u9898\u3002", "challenges": "\u96c6\u6210\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3001\u6027\u80fd\u74f6\u9888\u3001\u54cd\u5e94\u901f\u5ea6\u6162\u4ee5\u53ca\u53ef\u9760\u6027\u4e0d\u8db3\u662f\u4e3b\u8981\u6311\u6218\u3002", "contributions": "\u63d0\u51fa\u4e86FDIRS\u65b0\u6846\u67b6\uff0c\u91c7\u7528\u4e86\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "results": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFDIRS\u6846\u67b6\u5728\u6027\u80fd\u3001\u54cd\u5e94\u901f\u5ea6\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5df2\u6709\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e4b\u524d\u6846\u67b6\u7684\u95ee\u9898\u3002", "conclusion": "FDIRS\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "related_work": "\u6587\u4e2d\u7b80\u8981\u5206\u6790\u4e86\u96c6\u6210\u7cfb\u7edf\u7684\u53d1\u5c55\u8fc7\u7a0b\u4ee5\u53caERPSD\u548cERPDRT\u6846\u67b6\uff0c\u4f5c\u4e3a\u65b0\u6846\u67b6\u8bbe\u8ba1\u7684\u57fa\u7840\u3002"}}
{"id": "2510.13223", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13223", "abs": "https://arxiv.org/abs/2510.13223", "authors": ["Yiyuan He", "Minxian Xu", "Jingfeng Wu", "Jianmin Hu", "Chong Ma", "Min Shen", "Le Chen", "Chengzhong Xu", "Lin Qu", "Kejiang Ye"], "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure", "comment": "23 pages", "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.", "AI": {"tldr": "BanaServe\u662f\u4e00\u4e2a\u9488\u5bf9\u89e3\u8026\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u52a8\u6001\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8d44\u6e90\u8fc1\u79fb\u548c\u5168\u5c40\u7f13\u5b58\u5171\u4eab\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u5206\u914d\u3001\u8d1f\u8f7d\u4e0d\u5747\u548c\u7f13\u5b58\u611f\u77e5\u8def\u7531\u5bfc\u81f4\u7684\u70ed\u70b9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u8026\u5f0fLLM\u670d\u52a1\u7cfb\u7edf\u56e0\u9759\u6001\u8d44\u6e90\u5206\u914d\u3001\u8ba1\u7b97\u4e0e\u5185\u5b58\u8d1f\u8f7d\u4e0d\u5747\u8861\u4ee5\u53ca\u7f13\u5b58\u611f\u77e5\u8def\u7531\u5f15\u53d1\u7684\u8d1f\u8f7d\u503e\u659c\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u6216\u8fdd\u53cdSLO\uff0c\u4e9f\u9700\u4e00\u79cd\u52a8\u6001\u3001\u9ad8\u6548\u7684\u8d44\u6e90\u8c03\u5ea6\u673a\u5236\u3002", "challenges": "1) \u9759\u6001\u8d44\u6e90\u5206\u914d\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\uff1b2) \u9884\u586b\u5145\uff08\u8ba1\u7b97\u5bc6\u96c6\uff09\u4e0e\u89e3\u7801\uff08\u5185\u5b58\u5bc6\u96c6\uff09\u9636\u6bb5\u5b58\u5728\u56fa\u6709\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff1b3) \u524d\u7f00\u7f13\u5b58\u611f\u77e5\u8def\u7531\u5bfc\u81f4\u9ad8\u547d\u4e2d\u8282\u70b9\u8fc7\u8f7d\uff0c\u52a0\u5267\u8d1f\u8f7d\u4e0d\u5747\u3002", "contributions": "\u63d0\u51fa\u4e86BanaServe\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u5c42\u7ea7\u522b\u6743\u91cd\u8fc1\u79fb\u3001\u6ce8\u610f\u529b\u7ea7\u522bKV\u7f13\u5b58\u8fc1\u79fb\u4ee5\u53ca\u5e26\u5c42\u95f4\u91cd\u53e0\u4f20\u8f93\u7684\u5168\u5c40KV\u7f13\u5b58\u5171\u4eab\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7c97\u7c92\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u7684\u52a8\u6001\u8d1f\u8f7d\u91cd\u5e73\u8861\uff0c\u5e76\u89e3\u8026\u4e86\u8def\u7531\u51b3\u7b56\u4e0e\u7f13\u5b58\u4f4d\u7f6e\u7684\u4f9d\u8d56\u3002", "results": "\u76f8\u6bd4vLLM\uff0cBanaServe\u5b9e\u73b0\u4e861.2\u500d\u81f33.9\u500d\u7684\u66f4\u9ad8\u541e\u5410\u91cf\uff0c\u603b\u5904\u7406\u65f6\u95f4\u51cf\u5c113.9%\u81f378.4%\uff1b\u76f8\u6bd4DistServe\uff0c\u541e\u5410\u91cf\u63d0\u53471.1\u500d\u81f32.8\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e1.4%\u81f370.1%\u3002", "conclusion": "BanaServe\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u7f16\u6392\u548c\u521b\u65b0\u7684\u7f13\u5b58\u7ba1\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u89e3\u8026\u5f0fLLM\u670d\u52a1\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548c\u8d1f\u8f7d\u4e0d\u5747\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u6548\u7387\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecvLLM\u548cDistServe\u7b49\u89e3\u8026\u5f0fLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u5b83\u4eec\u867d\u5b9e\u73b0\u4e86\u9884\u586b\u5145\u4e0e\u89e3\u7801\u7684\u5206\u79bb\uff0c\u4f46\u5728\u52a8\u6001\u8d44\u6e90\u8c03\u5ea6\u548c\u7f13\u5b58\u8d1f\u8f7d\u5747\u8861\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0cBanaServe\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6539\u8fdb\u3002"}}
{"id": "2510.13306", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.13306", "abs": "https://arxiv.org/abs/2510.13306", "authors": ["Jannick Borowitz", "Ernestine Gro\u00dfmann", "Mattthias Schimek"], "title": "Distributed Reductions for the Maximum Weight Independent Set Problem", "comment": null, "summary": "Finding maximum-weight independent sets in graphs is an important NP-hard\noptimization problem. Given a vertex-weighted graph $G$, the task is to find a\nsubset of pairwise non-adjacent vertices of $G$ with maximum weight. Most\nrecently published practical exact algorithms and heuristics for this problem\nuse a variety of data-reduction rules to compute (near-)optimal solutions.\nApplying these rules results in an equivalent instance of reduced size. An\noptimal solution to the reduced instance can be easily used to construct an\noptimal solution for the original input.\n  In this work, we present the first distributed-memory parallel reduction\nalgorithms for this problem, targeting graphs beyond the scale of previous\nsequential approaches. Furthermore, we propose the first distributed\nreduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight\nindependent set heuristically.\n  In our practical evaluation, our experiments on up to $1024$ processors\ndemonstrate good scalability of our distributed reduce algorithms while\nmaintaining good reduction impact. Our asynchronous reduce-and-peel approach\nachieves an average speedup of $33\\times$ over a sequential state-of-the-art\nreduce-and-peel approach on 36 real-world graphs with a solution quality close\nto the sequential algorithm. Our reduce-and-greedy algorithms even achieve\naverage speedups of up to $50\\times$ at the cost of a lower solution quality.\nMoreover, our distributed approach allows us to consider graphs with more than\none billion vertices and 17 billion edges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5bfb\u627e\u56fe\u4e2d\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u5316\u7ea6\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5206\u5e03\u5f0freduce-and-greedy\u4e0ereduce-and-peel\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u8d85\u5927\u89c4\u6a21\u56fe\u4e0a\u5b9e\u73b0\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8f83\u9ad8\u7684\u6c42\u89e3\u901f\u5ea6\uff0c\u652f\u6301\u5904\u7406\u8d85\u8fc7\u5341\u4ebf\u9876\u70b9\u548c170\u4ebf\u8fb9\u7684\u56fe\u3002", "motivation": "\u7531\u4e8e\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u95ee\u9898\u662fNP\u96be\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u5904\u7406\u7684\u89c4\u6a21\u548c\u6548\u7387\uff0c\u96be\u4ee5\u5e94\u5bf9\u8d85\u5927\u89c4\u6a21\u56fe\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u4ee5\u63d0\u5347\u5904\u7406\u80fd\u529b\u548c\u6c42\u89e3\u901f\u5ea6\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\u5982\u4f55\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u9ad8\u6548\u5b9e\u73b0\u6570\u636e\u7ea6\u7b80\u89c4\u5219\u3001\u4fdd\u6301\u826f\u597d\u7684\u7ea6\u7b80\u6548\u679c\u3001\u534f\u8c03\u591a\u5904\u7406\u5668\u95f4\u7684\u5f02\u6b65\u64cd\u4f5c\uff0c\u4ee5\u53ca\u5728\u63d0\u5347\u901f\u5ea6\u7684\u540c\u65f6\u5c3d\u53ef\u80fd\u4fdd\u8bc1\u89e3\u7684\u8d28\u91cf\u3002", "contributions": "1) \u9996\u4e2a\u7528\u4e8e\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u95ee\u9898\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7ea6\u7b80\u7b97\u6cd5\uff1b2) \u9996\u4e2a\u5206\u5e03\u5f0freduce-and-greedy\u548creduce-and-peel\u542f\u53d1\u5f0f\u7b97\u6cd5\uff1b3) \u5b9e\u73b0\u4e86\u5bf9\u8d85\u8fc7\u5341\u4ebf\u9876\u70b9\u548c170\u4ebf\u8fb9\u7684\u5927\u89c4\u6a21\u56fe\u7684\u5904\u7406\u80fd\u529b\uff1b4) \u5b9e\u9a8c\u663e\u793a\u826f\u597d\u7684\u52a0\u901f\u6bd4\u548c\u53ef\u6269\u5c55\u6027\u3002", "results": "\u5728\u6700\u591a1024\u4e2a\u5904\u7406\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f02\u6b65reduce-and-peel\u65b9\u6cd5\u76f8\u6bd4\u987a\u5e8f\u7b97\u6cd5\u5e73\u5747\u52a0\u901f33\u500d\uff0c\u89e3\u8d28\u91cf\u63a5\u8fd1\uff1breduce-and-greedy\u65b9\u6cd5\u6700\u9ad8\u52a0\u901f\u8fbe50\u500d\uff0c\u4f46\u89e3\u8d28\u91cf\u7565\u4f4e\u3002\u7b97\u6cd5\u5177\u6709\u826f\u597d\u53ef\u6269\u5c55\u6027\u548c\u7ea6\u7b80\u6548\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u548c\u89c4\u6a21\uff0c\u4e3a\u5904\u7406\u8d85\u5927\u89c4\u6a21\u56fe\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u5728\u901f\u5ea6\u4e0e\u89e3\u8d28\u91cf\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u6743\u8861\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u5305\u62ec\u9488\u5bf9\u8be5\u95ee\u9898\u7684\u5404\u7c7b\u987a\u5e8f\u7cbe\u786e\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5e7f\u6cdb\u4f7f\u7528\u6570\u636e\u7ea6\u7b80\u89c4\u5219\u6765\u7f29\u5c0f\u95ee\u9898\u89c4\u6a21\uff0c\u4f46\u53d7\u9650\u4e8e\u5355\u673a\u5904\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u8d85\u5927\u89c4\u6a21\u56fe\u3002"}}
{"id": "2510.13147", "categories": ["cs.AR", "cs.LG", "cs.PF", "C.1.4"], "pdf": "https://arxiv.org/pdf/2510.13147", "abs": "https://arxiv.org/abs/2510.13147", "authors": ["Faraz Tahmasebi", "Michael Pelluer", "Hyoukjun Kwon"], "title": "D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations", "comment": "12 pages, 13 figures", "summary": "The computation and memory costs of large language models kept increasing\nover last decade, which reached over the scale of 1T parameters. To address the\nchallenges from the large scale models, model compression techniques such as\nlow-rank decomposition have been explored. Previous model decomposition works\nhave focused on weight decomposition to avoid costly runtime decomposition,\nwhose latency often significantly exceeds the benefits from decomposition\n(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K\nsequence length with activation decomposition compared to no decomposition). In\nthis work, we debunk such observations and report that the input decomposition\ncan be significantly beneficial with a proper choice of decomposition algorithm\nand hardware support. We adopt progressive decomposition algorithm, Lanczos\nalgorithm, and design a co-accelerator architecture for the decomposition\nalgorithm. To address the memory- boundness of the decomposition operation, we\nintroduce a novel compute replication methodology that moves the op- eration\ntoward compute-bound region, which enables 6.2x speedup in our evaluation. We\nalso develop an output shape- preserving computation scheme that eliminates\ndecomposi- tion costs in consecutive layers. To compensate model quality loss\nfrom compression, we introduce a multi-track decom- position approach that\nseparately handles outlier channels for high accuracy and low perplexity with\nminimal compu- tational costs. Combined together, our accelerator, D-com,\nprovides 22% end-to-end latency improvements compared to A100 GPU at the cost\nof small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD-com\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u91c7\u7528\u6e10\u8fdb\u5f0f\u5206\u89e3\u7b97\u6cd5\u3001Lanczos\u7b97\u6cd5\u548c\u534f\u540c\u52a0\u901f\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u8ba1\u7b97\u590d\u5236\u548c\u8f93\u51fa\u5f62\u72b6\u4fdd\u6301\u8ba1\u7b97\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5927\u6a21\u578b\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u7684\u5feb\u901f\u589e\u957f\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u6025\u5267\u4e0a\u5347\uff0c\u4f20\u7edf\u7684\u4f4e\u79e9\u5206\u89e3\u6280\u672f\u56e0\u8fd0\u884c\u65f6\u5206\u89e3\u5e26\u6765\u7684\u989d\u5916\u5ef6\u8fdf\u800c\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u7cbe\u5ea6\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5206\u89e3\u64cd\u4f5c\u7684\u5185\u5b58\u74f6\u9888\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff1b\u8fde\u7eed\u5c42\u95f4\u7684\u5206\u89e3\u5f00\u9500\u7d2f\u79ef\uff1b\u6a21\u578b\u538b\u7f29\u5e26\u6765\u7684\u7cbe\u5ea6\u635f\u5931\uff1b\u4ee5\u53ca\u73b0\u6709\u5206\u89e3\u65b9\u6cd5\u5728\u5b9e\u9645\u786c\u4ef6\u4e0a\u7684\u5ef6\u8fdf\u8fc7\u9ad8\u3002", "contributions": "1) \u91c7\u7528Lanczos\u7b97\u6cd5\u4e0e\u6e10\u8fdb\u5f0f\u5206\u89e3\u7b97\u6cd5\uff1b2) \u8bbe\u8ba1\u4e86\u9762\u5411\u5206\u89e3\u7b97\u6cd5\u7684\u534f\u540c\u52a0\u901f\u5668\u67b6\u6784\uff1b3) \u63d0\u51fa\u8ba1\u7b97\u590d\u5236\u65b9\u6cd5\uff0c\u5c06\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u8f6c\u4e3a\u8ba1\u7b97\u5bc6\u96c6\u578b\uff0c\u5b9e\u73b06.2\u500d\u52a0\u901f\uff1b4) \u5f00\u53d1\u8f93\u51fa\u5f62\u72b6\u4fdd\u6301\u7684\u8ba1\u7b97\u65b9\u6848\uff0c\u6d88\u9664\u8fde\u7eed\u5c42\u7684\u5206\u89e3\u5f00\u9500\uff1b5) \u63d0\u51fa\u591a\u8f68\u9053\u5206\u89e3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u3002", "results": "D-com\u52a0\u901f\u5668\u76f8\u6bd4A100 GPU\u5b9e\u73b0\u4e8622%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5728AI2\u63a8\u7406\u6311\u6218\u4efb\u52a1\u4e2d\u4ec5\u5e26\u67653%\u7684\u6a21\u578b\u8d28\u91cf\u4e0b\u964d\uff0c\u5e76\u5728Llama2-7b\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5206\u89e3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u8f93\u5165\u5206\u89e3\u53ef\u4ee5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0cD-com\u4e3a\u5927\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u6743\u91cd\u5206\u89e3\u548c\u4f4e\u79e9\u8fd1\u4f3c\u6280\u672f\uff0c\u5982SVD\u5206\u89e3\u3001LoRA\u7b49\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u591a\u907f\u514d\u8fd0\u884c\u65f6\u6fc0\u6d3b\u5206\u89e3\u4ee5\u51cf\u5c11\u5ef6\u8fdf\uff0c\u672c\u6587\u5219\u901a\u8fc7\u7b97\u6cd5\u4f18\u5316\u4e0e\u786c\u4ef6\u652f\u6301\u91cd\u65b0\u8bc4\u4f30\u5e76\u63d0\u5347\u4e86\u8f93\u5165\u5206\u89e3\u7684\u53ef\u884c\u6027\u4e0e\u6548\u76ca\u3002"}}
{"id": "2510.13362", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.13362", "abs": "https://arxiv.org/abs/2510.13362", "authors": ["Angelos Athanasiadis", "Nikolaos Tampouratzis", "Ioannis Papaefstathiou"], "title": "Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks", "comment": "9th International Workoshop on Microsystems, International Hellenic\n  University", "summary": "The growing demand for real-time processing in artificial intelligence\napplications, particularly those involving Convolutional Neural Networks\n(CNNs), has highlighted the need for efficient computational solutions.\nConventional processors, very often, fall short in balancing performance, power\nconsumption, and latency, especially in embedded systems and edge computing\nplatforms. Field-Programmable Gate Arrays (FPGAs) offer a promising\nalternative, combining high performance with energy efficiency and\nreconfigurability. The presented framework addresses the complex and demanding\ncomputations of CNNs on FPGAs maintaining full precision in all neural network\nparameters. Specifically, our framework is based on Darknet which is very\nwidely used for the design of CNNs and allows the designer, by using a similar\ninput to that given to Darknet, to efficiently implement a CNN in a\nheterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA\nframeworks that support quantization, our solution aims to offer similar\nperformance and/or energy efficiency without any degradation on the NN\naccuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u9ad8\u6548CNN\u8ba1\u7b97\u6846\u67b6\uff0c\u5229\u7528Darknet\u8bbe\u8ba1\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u5168\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u80fd\u6548\uff0c\u9002\u7528\u4e8eCPU-FPGA\u5f02\u6784\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u5904\u7406\u5668\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5c24\u5176\u5728\u8fb9\u7f18\u8ba1\u7b97\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u800cFPGA\u56e0\u5176\u9ad8\u80fd\u6548\u548c\u53ef\u91cd\u6784\u6027\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "challenges": "\u5728FPGA\u4e0a\u9ad8\u6548\u5b9e\u73b0\u5168\u7cbe\u5ea6CNN\u8ba1\u7b97\u9762\u4e34\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\u7684\u6311\u6218\uff0c\u540c\u65f6\u9700\u4fdd\u6301\u4e0e\u91cf\u5316\u65b9\u6848\u76f8\u5f53\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDarknet\u7684FPGA\u6846\u67b6\uff0c\u652f\u6301\u5168\u7cbe\u5ea6CNN\u90e8\u7f72\uff0c\u53ef\u5728CPU-FPGA\u5f02\u6784\u7cfb\u7edf\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u5e76\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e0e\u91cf\u5316\u65b9\u6848\u76f8\u5f53\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "results": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u5168\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u91cf\u5316FPGA\u6846\u67b6\u76f8\u4f3c\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8fb9\u7f18AI\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u4e3a\u5728FPGA\u4e0a\u90e8\u7f72\u9ad8\u7cbe\u5ea6CNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u51c6\u786e\u6027\u3001\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6AI\u5e94\u7528\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u591a\u91c7\u7528\u91cf\u5316\u6280\u672f\u964d\u4f4eFPGA\u4e0aCNN\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u53ef\u80fd\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\uff1bDarknet\u4f5c\u4e3a\u4e3b\u6d41CNN\u8bbe\u8ba1\u5de5\u5177\uff0c\u5c1a\u672a\u5145\u5206\u7528\u4e8eFPGA\u5f02\u6784\u7cfb\u7edf\u4f18\u5316\u3002"}}
{"id": "2510.13668", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13668", "abs": "https://arxiv.org/abs/2510.13668", "authors": ["Zhibin Wang", "Zetao Hong", "Xue Li", "Zibo Wang", "Shipeng Li", "Qingkai Meng", "Qing Wang", "Chengying Huan", "Rong Gu", "Sheng Zhong", "Chen Tian"], "title": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference", "comment": null, "summary": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ARES\uff0c\u4e00\u79cd\u57fa\u4e8e\u957f\u5ea6\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u89e3\u7801\u91cd\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LLM\u539f\u751f\u9884\u6d4b\u65b9\u6cd5\u548c\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u8f93\u51fa\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u7531\u4e8e\u8f93\u51fa\u957f\u5ea6\u53d8\u5316\u5bfc\u81f4\u89e3\u7801\u9636\u6bb5\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u73b0\u6709\u9759\u6001\u9884\u586b\u5145-\u89e3\u7801\u8c03\u5ea6\u65b9\u6cd5\u5728\u9762\u5bf9\u52a8\u6001\u89e3\u7801\u8d1f\u8f7d\u65f6\u5bb9\u6613\u5f15\u53d1SLO\u8fdd\u89c4\u548c\u5185\u5b58\u6ea2\u51fa\u95ee\u9898\u3002", "challenges": "\u51c6\u786e\u9884\u6d4bLLM\u751f\u6210\u7684\u5269\u4f59\u957f\u5ea6\u3001\u5728\u4f4e\u5f00\u9500\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3001\u5728\u89e3\u7801\u9636\u6bb5\u52a8\u6001\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u4ee5\u907f\u514d\u8d44\u6e90\u74f6\u9888\u3002", "contributions": "1) \u63d0\u51fa\u4e00\u79cd\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u7684\u8f7b\u91cf\u7ea7\u8fde\u7eed\u957f\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\u548c\u53c2\u6570\u5f00\u9500\uff1b2) \u8bbe\u8ba1\u4e00\u79cd\u7ed3\u5408\u5f53\u524d\u4e0e\u9884\u6d4b\u8d1f\u8f7d\u7684\u52a8\u6001\u91cd\u8c03\u5ea6\u673a\u5236\uff0c\u6709\u6548\u6539\u5584P99 TPOT\u548c\u6574\u4f53\u541e\u5410\u91cf\u3002", "results": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u957f\u5ea6\u9884\u6d4b\u7684MAE\u964d\u4f4e\u4e8649.42%\uff0c\u9884\u6d4b\u5668\u53c2\u6570\u51cf\u5c1193.28%\uff1b\u89e3\u7801\u9636\u6bb5P99 TPOT\u964d\u4f4e74.77%\uff0c\u6700\u9ad8\u53ef\u63d0\u53472.24\u500dgoodput\u3002", "conclusion": "ARES\u901a\u8fc7\u7cbe\u51c6\u7684\u957f\u5ea6\u9884\u6d4b\u548c\u52a8\u6001\u91cd\u8c03\u5ea6\uff0c\u5728\u4e0d\u589e\u52a0\u663e\u8457\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u7f13\u89e3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u89e3\u7801\u8d1f\u8f7d\u4e0d\u5747\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u7cfb\u7edf\u6548\u7387\u3002", "related_work": "\u4e0ePD\u5206\u79bb\u67b6\u6784\u53ca\u9759\u6001prefill-to-decode\u8c03\u5ea6\u76f8\u5173\u7684\u7814\u7a76\uff0c\u4ee5\u53ca\u65e9\u671f\u5173\u4e8eLLM\u63a8\u7406\u8c03\u5ea6\u548c\u8d44\u6e90\u7ba1\u7406\u7684\u5de5\u4f5c\u3002"}}
{"id": "2510.13401", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13401", "abs": "https://arxiv.org/abs/2510.13401", "authors": ["Jude Haris", "Jos\u00e9 Cano"], "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs", "comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025", "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u5757\u6d6e\u70b9\u91cf\u5316\u52a0\u901f\u5668F-BFQ\uff0c\u80fd\u591f\u5728\u4e0d\u540cBFP\u53d8\u4f53\u95f4\u52a8\u6001\u5207\u6362\uff0c\u4ee5\u52a0\u901f\u8fb9\u7f18\u8bbe\u5907\u4e0a\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728AMD Kria\u677f\u4e0a\u76f8\u6bd4ARM NEON CPU\u5e73\u5747\u63d0\u53471.4\u500d\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9700\u8981\u5229\u7528\u91cf\u5316\u6280\u672f\u964d\u4f4e\u6a21\u578b\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u800c\u73b0\u6709\u52a0\u901f\u5668\u96be\u4ee5\u652f\u6301\u6df7\u5408BFP\u91cf\u5316\u4e0b\u7684\u591a\u53d8\u4f53\u52a8\u6001\u5207\u6362\u9700\u6c42\u3002", "challenges": "\u652f\u6301\u6df7\u5408BFP\u91cf\u5316\u7684LLM\u5728\u4e0d\u540c\u5c42\u4f7f\u7528\u4e0d\u540c\u7684BFP\u53d8\u4f53\uff0c\u4f20\u7edf\u52a0\u901f\u5668\u9700\u9891\u7e41\u91cd\u65b0\u914d\u7f6e\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff1b\u5982\u4f55\u5b9e\u73b0\u65e0\u9700\u91cd\u914d\u7f6e\u5373\u53ef\u52a8\u6001\u5207\u6362BFP\u53d8\u4f53\u7684\u786c\u4ef6\u52a0\u901f\u662f\u4e00\u5927\u6311\u6218\u3002", "contributions": "\u63d0\u51fa\u4e86F-BFQ\u52a0\u901f\u5668\u67b6\u6784\uff0c\u652f\u6301\u4e24\u79cdBFP\u91cf\u5316\u53d8\u4f53\u7684\u52a8\u6001\u5207\u6362\uff0c\u5e76\u96c6\u6210\u4e8eAMD Kria\u677f\u5b9e\u73b0\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\uff0c\u63d0\u5347\u4e86BFP\u91cf\u5316LLM\u7684\u63a8\u7406\u6548\u7387\u3002", "results": "\u5728\u4e09\u4e2aBFP\u91cf\u5316LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cF-BFQ\u52a0\u901f\u5668\u76f8\u6bd4Arm NEON CPU\u5e73\u5747\u63a8\u7406\u901f\u5ea6\u63d0\u53471.4\u500d\uff0c\u8fbe\u5230\u6bcf\u79d25.2\u4e2atoken\uff08\u7ea63.9\u4e2a\u5355\u8bcd\uff09\u3002", "conclusion": "F-BFQ\u52a0\u901f\u5668\u6709\u6548\u89e3\u51b3\u4e86BFP\u91cf\u5316LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8de8\u5c42\u53d8\u4f53\u5207\u6362\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u4f4e\u7cbe\u5ea6\u91cf\u5316\u6a21\u578b\u7684\u9ad8\u6548\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecllama.cpp\u7b49LLM\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u53ca\u5757\u6d6e\u70b9\uff08BFP\uff09\u91cf\u5316\u6280\u672f\u5728\u6a21\u578b\u538b\u7f29\u4e0e\u8fb9\u7f18\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2510.13724", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13724", "abs": "https://arxiv.org/abs/2510.13724", "authors": ["Aditya Tanikanti", "Benoit C\u00f4t\u00e9", "Yanfei Guo", "Le Chen", "Nickolaus Saint", "Ryan Chard", "Ken Raffenetti", "Rajeev Thakur", "Thomas Uram", "Ian Foster", "Michael E. Papka", "Venkatram Vishwanath"], "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access", "comment": null, "summary": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure.", "AI": {"tldr": "FIRST\u662f\u4e00\u4e2a\u5728\u5206\u5e03\u5f0f\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u63a8\u7406\u5373\u670d\u52a1\u7684\u6846\u67b6\uff0c\u901a\u8fc7OpenAI\u517c\u5bb9API\u63d0\u4f9b\u5bf9\u591a\u79cdAI\u6a21\u578b\u7684\u5b89\u5168\u3001\u79c1\u6709\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u8bbf\u95ee\u3002", "motivation": "\u6ee1\u8db3\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u5bf9\u79c1\u6709\u3001\u5b89\u5168\u548c\u53ef\u6269\u5c55AI\u63a8\u7406\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u907f\u514d\u4f9d\u8d56\u5546\u4e1a\u4e91\u57fa\u7840\u8bbe\u65bd\u3002", "challenges": "\u5982\u4f55\u5728\u591a\u4e2a\u5f02\u6784HPC\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684\u8054\u90a6\u63a8\u7406\u8c03\u5ea6\uff0c\u5e76\u652f\u6301\u81ea\u52a8\u6269\u7f29\u5bb9\u4e0e\u5b89\u5168\u8ba4\u8bc1\u3002", "contributions": "\u63d0\u51faFIRST\u6846\u67b6\uff0c\u96c6\u6210Globus Auth\u4e0eGlobus Compute\uff0c\u5b9e\u73b0\u8de8\u96c6\u7fa4\u7684\u7edf\u4e00API\u8bbf\u95ee\u3001\u591a\u540e\u7aef\u652f\u6301\u3001\u8d44\u6e90\u81ea\u52a8\u6269\u5c55\u548c\u70ed\u8282\u70b9\u7ef4\u6301\u3002", "results": "\u7cfb\u7edf\u652f\u6301\u6bcf\u65e5\u6570\u5341\u4ebftoken\u7684\u672c\u5730\u63a8\u7406\uff0c\u63d0\u4f9b\u6279\u91cf\u4e0e\u4ea4\u4e92\u4e24\u79cd\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5728\u79c1\u6709\u73af\u5883\u4e2d\u9ad8\u6548\u5e76\u884c\u6267\u884cLLM\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "FIRST\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u73b0\u6709HPC\u8bbe\u65bd\u4e0a\u5b89\u5168\u3001\u9ad8\u6548\u7684\u8054\u90a6\u63a8\u7406\u670d\u52a1\uff0c\u4e3a\u79d1\u7814\u4eba\u5458\u63d0\u4f9b\u4e86\u7c7b\u4e91\u4f53\u9a8c\u7684\u672c\u5730AI\u63a8\u7406\u80fd\u529b\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u5206\u5e03\u5f0f\u63a8\u7406\u7cfb\u7edf\u3001HPC\u4e0a\u7684AI\u670d\u52a1\u90e8\u7f72\u4ee5\u53ca\u57fa\u4e8eAPI\u7684\u6a21\u578b\u8bbf\u95ee\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u591a\u96c6\u7fa4\u8054\u90a6\u8c03\u5ea6\u548c\u5b89\u5168\u79c1\u6709\u5316\u652f\u6301\u7684\u6574\u5408\u3002"}}
{"id": "2510.13755", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13755", "abs": "https://arxiv.org/abs/2510.13755", "authors": ["Timoth\u00e9 Albouy", "Antonio Fern\u00e1ndez Anta", "Chryssis Georgiou", "Nicolas Nicolaou", "Junlang Wang"], "title": "Tight Conditions for Binary-Output Tasks under Crashes", "comment": null, "summary": "This paper explores necessary and sufficient system conditions to solve\ndistributed tasks with binary outputs (\\textit{i.e.}, tasks with output values\nin $\\{0,1\\}$). We focus on the distinct output sets of values a task can\nproduce (intentionally disregarding validity and value multiplicity),\nconsidering that some processes may output no value. In a distributed system\nwith $n$ processes, of which up to $t \\leq n$ can crash, we provide a complete\ncharacterization of the tight conditions on $n$ and $t$ under which every class\nof tasks with binary outputs is solvable, for both synchronous and asynchronous\nsystems. This output-set approach yields highly general results: it unifies\nmultiple distributed computing problems, such as binary consensus and symmetry\nbreaking, and it produces impossibility proofs that hold for stronger task\nformulations, including those that consider validity, account for value\nmultiplicity, or move beyond binary outputs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u540c\u6b65\u548c\u5f02\u6b65\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5177\u6709\u4e8c\u5143\u8f93\u51fa\u7684\u4efb\u52a1\u53ef\u89e3\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f93\u51fa\u96c6\u7684\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u4e8c\u5143\u5171\u8bc6\u548c\u5bf9\u79f0\u6027\u7834\u7f3a\u7b49\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86\u9002\u7528\u4e8e\u66f4\u5f3a\u4efb\u52a1\u5b9a\u4e49\u7684\u4e0d\u53ef\u80fd\u6027\u8bc1\u660e\u3002", "motivation": "\u4e3a\u4e86\u7edf\u4e00\u5206\u6790\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5177\u6709\u4e8c\u5143\u8f93\u51fa\u7684\u4efb\u52a1\u7684\u53ef\u89e3\u6027\uff0c\u7279\u522b\u662f\u5f53\u90e8\u5206\u8fdb\u7a0b\u53ef\u80fd\u53d1\u751f\u5d29\u6e83\u65f6\uff0c\u9700\u8981\u660e\u786e\u7cfb\u7edf\u53c2\u6570\uff08\u5982\u8fdb\u7a0b\u6570n\u548c\u5bb9\u9519\u6570t\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "challenges": "\u5728\u5b58\u5728\u6700\u591at\u4e2a\u8fdb\u7a0b\u5d29\u6e83\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u5b9a\u5728\u4f55\u79cdn\u548ct\u6761\u4ef6\u4e0b\uff0c\u6240\u6709\u4e8c\u5143\u8f93\u51fa\u4efb\u52a1\u5728\u540c\u6b65\u548c\u5f02\u6b65\u7cfb\u7edf\u4e2d\u5747\u53ef\u89e3\uff1b\u540c\u65f6\u8981\u5904\u7406\u8f93\u51fa\u96c6\u3001\u6709\u6548\u6027\u3001\u503c\u591a\u91cd\u6027\u7b49\u4e0d\u540c\u4efb\u52a1\u53d8\u4f53\u7684\u590d\u6742\u6027\u3002", "contributions": "\u63d0\u4f9b\u4e86n\u548ct\u7684\u7d27\u81f4\u6761\u4ef6\uff0c\u5b8c\u5168\u523b\u753b\u4e86\u4e8c\u5143\u8f93\u51fa\u4efb\u52a1\u5728\u540c\u6b65\u548c\u5f02\u6b65\u7cfb\u7edf\u4e2d\u7684\u53ef\u89e3\u6027\uff1b\u63d0\u51fa\u8f93\u51fa\u96c6\u89c6\u89d2\uff0c\u7edf\u4e00\u4e86\u591a\u4e2a\u7ecf\u5178\u95ee\u9898\uff08\u5982\u4e8c\u5143\u5171\u8bc6\u3001\u5bf9\u79f0\u6027\u7834\u7f3a\uff09\uff0c\u5e76\u5bfc\u51fa\u4e86\u9002\u7528\u4e8e\u66f4\u5f3a\u4efb\u52a1\u5f62\u5f0f\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002", "results": "\u7ed9\u51fa\u4e86\u5728\u540c\u6b65\u548c\u5f02\u6b65\u7cfb\u7edf\u4e2d\uff0c\u6240\u6709\u4e8c\u5143\u8f93\u51fa\u4efb\u52a1\u53ef\u89e3\u7684\u5145\u8981\u6761\u4ef6\uff1b\u8bc1\u660e\u4e86\u5728\u67d0\u4e9bn\u548ct\u6761\u4ef6\u4e0b\u4efb\u52a1\u4e0d\u53ef\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u5982\u4f55\u63a8\u5e7f\u5230\u66f4\u590d\u6742\u7684\u4efb\u52a1\u5b9a\u4e49\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8f93\u51fa\u96c6\u7684\u6982\u5ff5\uff0c\u672c\u6587\u5efa\u7acb\u4e86\u4e8c\u5143\u8f93\u51fa\u4efb\u52a1\u53ef\u89e3\u6027\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7cfb\u7edf\u89c4\u6a21\u4e0e\u5bb9\u9519\u80fd\u529b\u4e4b\u95f4\u7684\u672c\u8d28\u5173\u7cfb\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u7c7b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u6790\u5de5\u5177\u3002", "related_work": "\u4e0e\u4e8c\u5143\u5171\u8bc6\u3001\u5171\u8bc6\u95ee\u9898\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff08\u5982FLP\uff09\u3001\u5bf9\u79f0\u6027\u7834\u7f3a\u95ee\u9898\u4ee5\u53ca\u5206\u5e03\u5f0f\u4efb\u52a1\u53ef\u89e3\u6027\u7684\u4e00\u822c\u7406\u8bba\uff08\u5982\u8272\u5355\u7eaf\u5f62\u6cd5\uff09\u5bc6\u5207\u76f8\u5173\u3002"}}
{"id": "2510.13664", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.13664", "abs": "https://arxiv.org/abs/2510.13664", "authors": ["Muhammad Haseeb", "Jinkun Geng", "Radhika Mittal", "Aurojit Panda", "Srinivas Narayana", "Anirudh Sivaraman"], "title": "Fair Ordering", "comment": null, "summary": "A growing class of applications demands \\emph{fair ordering/sequencing} of\nevents which ensures that events generated earlier by one client are processed\nbefore later events from other clients. However, achieving such sequencing is\nfundamentally challenging due to the inherent limitations of clock\nsynchronization. We advocate for an approach that embraces, rather than\neliminates, clock variability. Instead of attempting to remove error from a\ntimestamp, Tommy, our proposed system, leverages a statistical model to compare\ntwo noisy timestamps probabilistically by learning per-clock offset\ndistributions. Our preliminary statistical model computes the probability that\none event precedes another w.r.t. the wall-clock time without access to the\nwall-clock. This serves as a foundation for a new relation:\n\\emph{likely-happened-before} denoted by $\\xrightarrow{p}$ where $p$ represents\nthe probability of an event to have happened before another. The\n$\\xrightarrow{p}$ relation provides a basis for ordering multiple events which\nare otherwise considered \\emph{concurrent} by the typical\n\\emph{happened-before} ($\\rightarrow$) relation. We highlight various related\nchallenges including intransitivity of $\\xrightarrow{p}$ relation as opposed to\nthe transitive $\\rightarrow$ relation. We also outline several research\ndirections: online fair sequencing, stochastically fair total ordering,\nhost-level support for fairness and more.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6027\u4e8b\u4ef6\u6392\u5e8f\u65b9\u6cd5\u201clikely-happened-before\u201d\uff08\u8bb0\u4f5c\u2192p\uff09\uff0c\u901a\u8fc7\u7edf\u8ba1\u6a21\u578b\u5229\u7528\u5e26\u6709\u566a\u58f0\u7684\u65f6\u95f4\u6233\u6765\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u4e8b\u4ef6\u6392\u5e8f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65f6\u949f\u540c\u6b65\u9650\u5236\u4e0b\u7684\u516c\u5e73\u6392\u5e8f\u96be\u9898\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u65f6\u949f\u540c\u6b65\u5b58\u5728\u56fa\u6709\u8bef\u5dee\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u6b63\u516c\u5e73\u7684\u4e8b\u4ef6\u6392\u5e8f\uff0c\u800c\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\u9700\u8981\u201c\u516c\u5e73\u6392\u5e8f\u201d\uff0c\u5373\u4e00\u4e2a\u5ba2\u6237\u7aef\u65e9\u751f\u6210\u7684\u4e8b\u4ef6\u5e94\u4f18\u5148\u4e8e\u5176\u4ed6\u5ba2\u6237\u7aef\u7684\u540e\u7eed\u4e8b\u4ef6\u88ab\u5904\u7406\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5bb9\u5fcd\u65f6\u949f\u504f\u5dee\u7684\u65b0\u6392\u5e8f\u673a\u5236\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5982\u4f55\u5728\u65e0\u771f\u5b9e\u5899\u949f\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u6bd4\u8f83\u4e24\u4e2a\u5e26\u566a\u58f0\u7684\u65f6\u95f4\u6233\uff1b\u6240\u63d0\u51fa\u7684\u2192p\u5173\u7cfb\u4e0d\u5177\u5907\u4f20\u9012\u6027\uff0c\u5bfc\u81f4\u6392\u5e8f\u590d\u6742\u5316\uff1b\u7ef4\u6301\u9ad8\u6982\u7387\u6392\u5e8f\u7684\u540c\u65f6\u4fdd\u8bc1\u7cfb\u7edf\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "contributions": "\u63d0\u51fa\u4e86\u201clikely-happened-before\u201d\uff08\u2192p\uff09\u8fd9\u4e00\u65b0\u5173\u7cfb\uff0c\u57fa\u4e8e\u7edf\u8ba1\u6a21\u578b\u5bf9\u5e26\u566a\u58f0\u65f6\u95f4\u6233\u8fdb\u884c\u6982\u7387\u6027\u6bd4\u8f83\uff1b\u8bbe\u8ba1\u4e86Tommy\u7cfb\u7edf\uff0c\u5229\u7528\u6bcf\u53f0\u65f6\u949f\u7684\u504f\u79fb\u5206\u5e03\u5efa\u6a21\u6765\u5224\u65ad\u4e8b\u4ef6\u987a\u5e8f\uff1b\u4e3a\u89e3\u51b3\u5e76\u53d1\u4e0e\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u7814\u7a76\u65b9\u5411\u3002", "results": "\u521d\u6b65\u7edf\u8ba1\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u771f\u5b9e\u5899\u949f\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u8ba1\u7b97\u4e00\u4e2a\u4e8b\u4ef6\u5148\u4e8e\u53e6\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c\u4ece\u800c\u4e3a\u57fa\u7840\u7684happened-before\u5173\u7cfb\u63d0\u4f9b\u8865\u5145\uff0c\u6709\u6548\u8bc6\u522b\u5e76\u6392\u5e8f\u539f\u672c\u88ab\u89c6\u4e3a\u5e76\u53d1\u7684\u4e8b\u4ef6\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6982\u7387\u6027\u6392\u5e8f\u5173\u7cfb\u2192p\uff0c\u672c\u6587\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u4e8b\u4ef6\u6392\u5e8f\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u5229\u7528\u65f6\u949f\u53d8\u5f02\u6027\u800c\u975e\u6d88\u9664\u5b83\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5982\u5728\u7ebf\u516c\u5e73\u6392\u5e8f\u3001\u968f\u673a\u516c\u5e73\u5168\u5e8f\u7b49\u6307\u660e\u4e86\u65b9\u5411\u3002", "related_work": "\u4e0eLamport\u7684happened-before\u5173\u7cfb\u3001\u5411\u91cf\u65f6\u949f\u3001\u7269\u7406\u65f6\u949f\u540c\u6b65\uff08\u5982NTP\uff09\u4ee5\u53ca\u56e0\u679c\u4e00\u81f4\u6027\u7b49\u7814\u7a76\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u4e0d\u540c\u4e8e\u8fd9\u4e9b\u786e\u5b9a\u6027\u65b9\u6cd5\uff0c\u672c\u6587\u91c7\u7528\u6982\u7387\u6027\u5efa\u6a21\u5904\u7406\u65f6\u949f\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.13732", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.13732", "abs": "https://arxiv.org/abs/2510.13732", "authors": ["Mohd Saif Ali Khan", "Karthik RM", "Samar Agnihotri"], "title": "Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error", "comment": null, "summary": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment strategies designed for practical deployment in such\nnetworks. First, we present a low complexity centralized algorithm that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed algorithm that uses\na priority-based pilot selection approach. In this algorithm, each selected AP\nminimizes estimation error using only local information and offers candidate\npilots to the UEs. Every UE then selects a suitable pilot based on AP priority.\nThis approach ensures consistency and minimizes interference while\nsignificantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of our\nproposed schemes in terms of network throughput when compared to other\nstate-of-the-art benchmark schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u52a8\u6001\u53ef\u6269\u5c55\u7684\u5bfc\u9891\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u7684\u5bfc\u9891\u6c61\u67d3\uff0c\u63d0\u5347\u4fe1\u9053\u4f30\u8ba1\u8d28\u91cf\u548c\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u5bfc\u9891\u6c61\u67d3\u662f\u9650\u5236\u5206\u5e03\u5f0f\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u590d\u6742\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u4f30\u8ba1\u7cbe\u5ea6\u3002", "challenges": "\u5982\u4f55\u5728\u964d\u4f4e\u5bfc\u9891\u6c61\u67d3\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4f4e\u590d\u6742\u5ea6\u3001\u65e0\u9700\u5168\u5c40\u534f\u8c03\u7684\u53ef\u6269\u5c55\u5bfc\u9891\u5206\u914d\u3002", "contributions": "1\uff09\u63d0\u51fa\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u96c6\u4e2d\u5f0f\u5bfc\u9891\u5206\u914d\u7b97\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u5206\u914d\u6700\u5c0f\u5316\u5168\u5c40\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\uff1b2\uff09\u8bbe\u8ba1\u4e00\u79cd\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\u4f18\u5148\u7ea7\u5bfc\u9891\u9009\u62e9\u7b97\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\u5373\u53ef\u6709\u6548\u51cf\u5c11\u5e72\u6270\u548c\u5bfc\u9891\u6c61\u67d3\u3002", "results": "\u6570\u503c\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u7cfb\u7edf\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u5bfc\u9891\u5206\u914d\u7b56\u7565\u5728\u964d\u4f4e\u5bfc\u9891\u6c61\u67d3\u3001\u63d0\u5347\u9891\u8c31\u6548\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u5206\u5e03\u5f0f\u65b9\u6848\u5177\u5907\u4f4e\u4fe1\u4ee4\u5f00\u9500\u548c\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002", "related_work": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u9759\u6001\u5bfc\u9891\u5206\u914d\u6216\u9700\u8981\u5168\u5c40\u4fe1\u606f\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u3001\u53ef\u6269\u5c55\u548c\u4f4e\u590d\u6742\u5ea6\u65b9\u6848\u7684\u7efc\u5408\u8003\u8651\u3002"}}
