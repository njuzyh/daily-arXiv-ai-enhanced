<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation](https://arxiv.org/abs/2510.14172)
*Yuchao Su,Srikar Chundury,Jiajia Li,Frank Mueller*

Main category: cs.AR

TL;DR: 本文提出了一种针对量子模拟中哈密顿量矩阵对角线结构优化的新型加速器\name，通过重构脉动阵列数据流，将稀疏对角矩阵转化为密集计算，显著提升了经典哈密顿模拟的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 由于哈密顿模拟中的矩阵维度随量子比特数指数增长，矩阵指数运算成本极高；现有稀疏矩阵加速器主要针对机器学习设计，无法有效应对量子模拟中常见的结构化对角稀疏模式，因此需要专用优化架构。

Challenges: 如何高效处理哈密顿矩阵中普遍存在的结构化对角稀疏性；在保持计算精度的同时将稀疏矩阵运算转化为高利用率的密集计算；设计适配此类结构的高效数据流与硬件架构。

Contributions: \name是首个针对对角结构优化的量子模拟加速器；提出重构的脉动数组数据流，将对角稀疏矩阵转换为密集计算；在HamLib基准集上实现了高达127倍的性能提升和超过4600倍的能效改进。

Results: \name在多个基准测试中平均性能较SIGMA、外积法和Gustavson算法分别提升10.26倍、33.58倍和53.15倍，最高加速达127.03倍；能量消耗平均降低471.55倍，最高达4630.58倍。

Conclusion: \name通过利用哈密顿矩阵的对角结构特征和定制化硬件设计，显著提升了经典哈密顿模拟的效率与可扩展性，为未来量子设备的验证与研究提供了强有力的工具。

Related Work: SIGMA、外积法和Gustavson算法是当前用于稀疏矩阵乘法的主要方法，但它们未针对哈密顿矩阵的对角主导特性进行优化，导致在量子模拟中效率低下。

Abstract: Hamiltonian simulation is a key workload in quantum computing, enabling the
study of complex quantum systems and serving as a critical tool for classical
verification of quantum devices. However, it is computationally challenging
because the Hilbert space dimension grows exponentially with the number of
qubits. The growing dimensions make matrix exponentiation, the key kernel in
Hamiltonian simulations, increasingly expensive. Matrix exponentiation is
typically approximated by the Taylor series, which contains a series of matrix
multiplications. Since Hermitian operators are often sparse, sparse matrix
multiplication accelerators are essential for improving the scalability of
classical Hamiltonian simulation. Yet, existing accelerators are primarily
designed for machine learning workloads and tuned to their characteristic
sparsity patterns, which differ fundamentally from those in Hamiltonian
simulations that are often dominated by structured diagonals.
  In this work, we present \name, the first diagonal-optimized quantum
simulation accelerator. It exploits the diagonal structure commonly found in
problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic
array dataflow to transform diagonally sparse matrices into dense computations,
enabling high utilization and performance. Through detailed cycle-level
simulation of diverse benchmarks in HamLib, \name{} demonstrates average
performance improvements of $10.26\times$, $33.58\times$, and $53.15\times$
over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak
speedups up to $127.03\times$ while reducing energy consumption by an average
of $471.55\times$ and up to $4630.58\times$ compared to SIGMA.

</details>


### [2] [Computing-In-Memory Aware Model Adaption For Edge Devices](https://arxiv.org/abs/2510.14379)
*Ming-Han Lin,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 本文提出了一种两阶段的计算内存（CIM）感知模型自适应方法，通过模型压缩、资源重分配和量化感知训练，显著提升了CIM阵列利用率和吞吐量，同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 由于CIM宏的尺寸限制和ADC精度不足，导致深度学习加速中存在吞吐量和精度瓶颈，因此需要一种能兼顾效率与精度的模型优化方法。

Challenges: 主要挑战包括CIM宏尺寸有限导致的资源利用率低、ADC精度不足引发的量化误差，以及模型压缩与加速之间的精度平衡问题。

Contributions: 1）提出两阶段CIM感知模型自适应方法；2）结合层重要性与硬件约束进行模型压缩与资源重分配；3）引入部分和量化与ADC精度建模的量化感知训练，减少推理误差。

Results: 实现了高达93%的模型压缩率，CIM阵列利用率提升至90%，支持最多256条字线并发激活，推理精度与先前方法相当。

Conclusion: 该方法有效提升了CIM系统的效率与可扩展性，在保持模型精度的同时显著优化了资源利用与吞吐量。

Related Work: 相关工作包括基于CIM的深度学习加速器设计、模型压缩技术（如剪枝与量化）以及量化感知训练方法。

Abstract: Computing-in-Memory (CIM) macros have gained popularity for deep learning
acceleration due to their highly parallel computation and low power
consumption. However, limited macro size and ADC precision introduce throughput
and accuracy bottlenecks. This paper proposes a two-stage CIM-aware model
adaptation process. The first stage compresses the model and reallocates
resources based on layer importance and macro size constraints, reducing model
weight loading latency while improving resource utilization and maintaining
accuracy. The second stage performs quantization-aware training, incorporating
partial sum quantization and ADC precision to mitigate quantization errors in
inference. The proposed approach enhances CIM array utilization to 90\%,
enables concurrent activation of up to 256 word lines, and achieves up to 93\%
compression, all while preserving accuracy comparable to previous methods.

</details>


### [3] [ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems](https://arxiv.org/abs/2510.14750)
*İsmail Emir Yüksel,Ataberk Olgun,F. Nisa Bostancı,Haocong Luo,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文首次在商用DRAM芯片中发现了一种新型的列间干扰现象ColumnDisturb，通过实验验证其在多子阵列中引发位翻转的机制，并指出其对现有刷新机制（如保留时间感知刷新）的重大影响，随着DRAM工艺节点缩小，该问题将更加严重。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加和工艺尺寸缩小，传统行级干扰（如RowHammer）之外的新型干扰现象可能浮现，但列级干扰尚未被系统研究。作者旨在揭示一种新的、跨子阵列的列间干扰机制，评估其对可靠性和现有缓解策略的影响。

Challenges: 1. 发现并验证一种全新的DRAM干扰机制（ColumnDisturb），其通过列线（位线）而非行线引发跨子阵列的位翻转；2. 在多种商用DDR4和HBM2芯片上进行大规模实验表征；3. 分析该现象对现有保留时间感知刷新机制的实际影响。

Contributions: 1. 首次提出并实验证实ColumnDisturb现象，展示其可同时影响多达三个子阵列（如3072行）；2. 在220颗真实DRAM芯片上系统表征该现象，揭示27项关键观察结果；3. 证明ColumnDisturb在标准刷新周期内即可引发多位翻转，且影响范围远超保留时间失效；4. 指出该现象将削弱保留时间感知刷新机制的有效性。

Results: 1. ColumnDisturb存在于三大厂商的DDR4和HBM2芯片中，且随工艺节点缩小而加剧（首次位翻转时间最多缩短5.06倍）；2. 在标准DDR4刷新窗口（63.6ms）内即可诱发多位翻转；3. 受影响的行数最多可达保留时间失效的198倍；4. 预测未来更小工艺节点下该问题将更严重。

Conclusion: ColumnDisturb是一种严重且普遍存在的新型DRAM干扰机制，影响范围广、随工艺演进恶化，且会显著削弱当前先进的刷新优化策略，未来DRAM可靠性设计需考虑此类列级干扰。

Related Work: RowHammer、RowPress等行级干扰研究；DRAM保留时间异质性及保留时间感知刷新机制（如PARA、RTA）；DRAM物理结构与位线耦合效应的相关建模工作。

Abstract: We experimentally demonstrate a new widespread read disturbance phenomenon,
ColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a
DRAM row (aggressor row) open, we show that it is possible to disturb DRAM
cells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells
sharing the same columns as the aggressor row (across multiple DRAM subarrays).
With ColumnDisturb, the activation of a single row concurrently disturbs cells
across as many as three subarrays (e.g., 3072 rows) as opposed to
RowHammer/RowPress, which affect only a few neighboring rows of the aggressor
row in a single subarray. We rigorously characterize ColumnDisturb and its
characteristics under various operational conditions using 216 DDR4 and 4 HBM2
chips from three major manufacturers. Among our 27 key experimental
observations, we highlight two major results and their implications.
  First, ColumnDisturb affects chips from all three major manufacturers and
worsens as DRAM technology scales down to smaller node sizes (e.g., the minimum
time to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We
observe that, in existing DRAM chips, ColumnDisturb induces bitflips within a
standard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict
that, as DRAM technology node size reduces, ColumnDisturb would worsen in
future DRAM chips, likely causing many more bitflips in the standard refresh
window. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows
than retention failures. Therefore, ColumnDisturb has strong implications for
retention-aware refresh mechanisms that leverage the heterogeneity in cell
retention times: our detailed analyses show that ColumnDisturb greatly reduces
the benefits of such mechanisms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 提出“普遍上下文管理”技术，通过解耦大语言模型初始化上下文与推理过程，显著减少科学应用的执行时间并提升资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有HPC集群设计无法有效支持结合轻量级大语言模型的新型高性能计算工作负载，存在排队时间长和模型启动开销高的问题。

Challenges: 如何在资源抢占频繁的HPC环境中避免重复支付高昂的LLM启动成本，同时充分利用碎片化GPU资源进行高效推理。

Contributions: 提出了“普遍上下文管理”技术，实现了LLM上下文的持久化保留，并改造了事实验证应用以支持该技术，实现了执行时间大幅缩短和机会性扩展。

Results: 使用相同数量的GPU将执行时间从3小时减少到48分钟（提升72.1%），并在利用集群中32.8%的GPU进行机会扩展后进一步缩短至13分钟。

Conclusion: 普遍上下文管理有效解决了HPC中LLM工作负载的启动开销与资源利用问题，为AI增强型科学应用提供了更高效的运行模式。

Related Work: 相关工作包括HPC资源调度优化、LLM推理加速技术以及上下文复用机制等。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [5] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 本文提出了一种可扩展的、基于覆盖树的分布式内存算法，用于在一般度量空间中计算固定半径近邻图，支持稀疏性感知，并在多种真实和合成数据集上展示了良好的并行扩展性与显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和数据获取手段的进步，许多科学数据集需要高效、可扩展的近邻图计算方法，尤其是在非欧几里得度量空间中需要精确解的应用场景。

Challenges: 在一般度量空间中高效构建精确的近邻图，同时实现良好的分布式并行扩展性，尤其面对高维和大规模数据时存在性能和内存使用挑战。

Contributions: 1) 提出了一种共享内存的覆盖树构建算法；2) 设计了两种分布式内存近邻图算法（点划分和空间划分策略）；3) 在多种度量下实现了对大规模数据的高效处理，并展示了显著的加速比。

Results: 在百万级高维真实数据集上，使用1024核达到最高678.34倍加速（平均70个邻居），使用4096核达到最高1590.99倍加速（平均500个邻居），显著优于现有方法。

Conclusion: 所提出的基于覆盖树的分布式算法在一般度量空间中高效且可扩展，能有效支持大规模科学数据分析中的近邻图构建。

Related Work: 先前工作主要集中在k近邻和近似最近邻搜索，尤其是欧几里得空间中的并行算法，但在精确解和非欧几里得度量方面的研究相对不足。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [6] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG是一种高性能、可插拔的顺序公平性服务，通过引入不对称架构打破传统拜占庭容错共识中对交易排序的冗余验证，实现高效验证而无需重复计算，显著提升吞吐量并降低延迟，同时保障批次顺序公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序公平共识协议（如Themis）虽然能防范恶意交易排序攻击（如抢跑、夹心攻击），但要求所有副本重复执行领导者昂贵的排序计算，导致性能低下。这种对称验证机制造成了资源浪费和系统瓶颈，因此需要一种更高效的验证方式来解决这一问题。

Challenges: 主要挑战在于如何在不重新执行完整排序计算的前提下，确保交易顺序的公平性可被高效且安全地验证；同时需设计一种轻量、无状态的验证机制，使跟随者无需维护历史状态即可完成验证，并保证系统在部分同步条件下的高性能与安全性。

Contributions: 1. 提出AUTIG，首个基于不对称架构的高效顺序公平性服务；2. 设计增量式未确认交易图（UTIG）结构，由领导者维护以摊销跨轮次的图构建开销；3. 构建无需状态的验证流水线，实现领导者的排序与跟随者的验证并行化；4. 提出包含内部对覆盖和前沿完整性检查的结构化公平性证明机制。

Results: 实验表明，AUTIG在保持gamma-batch-order-fairness的同时，相比对称式的图基基准方案，实现了更高的吞吐量和更低的端到端延迟，验证过程无需历史状态，显著降低了跟随者的计算与存储开销。

Conclusion: AUTIG通过将排序与验证解耦，提出了一种高效、可扩展的顺序公平共识新范式，解决了传统对称验证带来的性能瓶颈，为区块链系统中的公平交易排序提供了实用且高性能的解决方案。

Related Work: 与Themis等基于图的顺序公平共识协议密切相关，这些工作首次将交易依赖关系建模为图结构以实现公平排序，但采用对称验证方式导致所有副本重复高成本计算，AUTIG在此基础上提出非对称验证机制，避免了冗余计算，提升了整体效率。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [7] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: 本文提出了JASDA，一种面向MIG-GPU的去中心化调度范式，通过结合拍卖理论与在线优化，实现工作负载的自适应、公平且高效的资源管理。


<details>
  <summary>Details</summary>
Motivation: 传统集中式调度在MIG-GPU上面对复杂且时变的工作负载时扩展性受限，亟需更灵活、可扩展的调度机制。

Challenges: 如何在保证资源利用率、公平性和响应时效的同时，实现去中心化调度的可扩展性与稳定性。

Contributions: 提出了JASDA调度框架，将SJA模型从集中式扩展为完全去中心化的协商机制，引入双向迭代交互与策略驱动的清算机制，嵌入反馈与概率安全性。

Results: JASDA通过拍卖机制与在线优化结合，提升了调度的适应性与透明度，在利用率、公平性和响应性之间实现了良好平衡。

Conclusion: JASDA为MIG-GPU环境下的资源调度提供了可扩展、市场感知且公平驱动的解决方案，弥合了理论调度模型与实际AI和农业4.0应用部署之间的差距。

Related Work: SJA（Scheduler-Job Alignment）模型是本文的基础，相关工作还包括基于拍卖理论的资源分配和在线优化调度方法。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [8] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: 本文提出了一种基于CXL的新型MPI通信范式MPI-over-CXL，利用跨主机的缓存一致性共享内存，取代传统的数据拷贝方式，显著降低了通信延迟和内存带宽消耗，提升了大规模HPC环境中的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统MPI实现依赖显式内存拷贝操作，导致冗余数据移动和缓冲区管理开销，尤其影响高强度进程间通信的HPC工作负载性能。

Challenges: 如何在多主机间高效实现缓存一致的共享内存通信，避免传统MPI中的冗余数据拷贝和管理开销；同时构建支持该机制的硬件与软件协同环境。

Contributions: 提出了MPI-over-CXL通信范式；设计了支持CXL 3.2的自定义控制器；实现了基于FPGA的多主机仿真环境和专用软件栈；验证了该方案在典型基准测试中的性能优势。

Results: 实验结果表明，相比传统MPI系统，MPI-over-CXL显著降低了通信延迟和内存带宽使用，在代表性基准测试中实现了显著的性能提升。

Conclusion: MPI-over-CXL通过利用CXL提供的跨主机共享内存能力，有效减少了通信开销，展现出在大规模高性能计算环境中提升效率和可扩展性的巨大潜力。

Related Work: 相关工作包括传统基于消息传递的MPI实现、共享内存编程模型、以及近年来利用新型互连技术（如CXL、Gen-Z）优化系统间通信的研究。

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [9] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM是一个面向高性能、大规模企业级服务的高效大语言模型推理框架，通过解耦的服务-引擎架构和多层次优化，在多种AI加速器上实现了显著的吞吐量和资源效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了应对大规模、多模态、高并发的企业级LLM推理需求，现有框架在资源利用率、调度效率和跨加速器兼容性方面存在不足，亟需一个更智能、更高效的解决方案。

Challenges: 主要挑战包括：如何高效处理多模态请求并共置在线与离线任务以提升集群利用率；如何在动态负载下实现灵活的Prefill-Decode阶段拆分；如何实现高效的全局KV缓存管理和容错机制；以及如何在系统与算法层面协同优化以充分饱和计算资源。

Contributions: 1）提出了一种新颖的解耦式服务-引擎架构；2）设计了工作负载自适应的动态PD拆分策略和面向多模态的EPD拆分策略；3）实现了支持全局KV缓存管理和容错的分布式服务层；4）在引擎层实现了多层执行流水线优化、自适应图模式和xTensor内存管理；5）集成了优化的推测解码和动态EPLB等算法增强。

Results: 实验表明，在相同TPOT约束下，xLLM在Qwen系列模型上吞吐量达到MindIE的1.7倍、vLLM-Ascend的2.2倍，在Deepseek系列模型上平均吞吐量达到MindIE的1.7倍，显著优于现有框架。

Conclusion: xLLM通过系统与算法的深度协同优化，构建了一个高性能、高可用、高资源利用率的企业级LLM推理框架，适用于大规模多模态场景，且已开源。

Related Work: 与vLLM、MindIE等主流LLM推理框架相关，这些工作聚焦于PagedAttention、内存管理或特定硬件优化，而xLLM进一步提出了服务与引擎解耦架构，并在多模态调度、动态阶段拆分和全局资源管理方面进行了更深层次的优化。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [10] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 本文研究了一个带有删除操作的无限球-箱过程，分析了在动态插入和删除情况下系统的负载差异和过载情况，证明了在任意时刻负载差异为O(log n)，过载为log log n + O(1)，并针对特定插入概率序列进一步优化了差异界。


<details>
  <summary>Details</summary>
Motivation: 理解在动态环境中（包含插入和删除）负载均衡策略的性能表现，特别是在长期运行下的最大负载与平均负载之间的差异。

Challenges: 由于系统同时存在插入和删除操作，且插入概率随时间变化，导致负载动态复杂，难以精确刻画最大负载与平均负载之间的差异和过载行为。

Contributions: 1. 证明了任意时刻总负载超过平均值的部分为O(n)，差异为O(log n)，且给出了匹配的下界；2. 首次证明过载为log log n + O(1)；3. 对‘良好’插入序列，将差异也压缩至log log n + O(1)；4. 使用分层归纳和势能分析结合概率耦合技术简化了复杂条件分析。

Results: 在任意时间t，差异为O(log n)，过载为log log n + O(1)；对于良好插入序列，差异可进一步降至log log n + O(1)；并提供了差异的匹配下界。

Conclusion: Greedy[2]策略在动态带删除的球-箱过程中仍能保持良好的负载均衡性能，尤其在长期运行中最大负载仅略高于历史平均，且通过精细分析可获得紧确界。

Related Work: 基于经典的ABKU99的分层归纳方法，并扩展了以往仅考虑插入的模型，纳入了删除操作和时变插入概率，与动态负载均衡、随机分配和马尔可夫过程相关工作密切相关。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: 本文研究了基于可重构智能表面（RIS）的用户定位方法，提出了一种结合神经进化和监督学习的多智能体算法，用于联合控制RIS相位配置和用户发射功率。该方案仅需单比特反馈信息，支持离散响应的RIS元件，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了提升RIS辅助下的用户定位精度和系统效率，尤其是在资源受限条件下实现高效、低开销的上行链路功率控制与RIS配置。

Challenges: 如何在有限反馈（如单比特反馈）下实现RIS相位与用户发射功率的联合优化；处理RIS元件的离散响应特性；在复杂无线环境中实现高精度定位。

Contributions: 提出了一种基于神经进化与监督学习混合框架的多智能体联合优化算法；实现了仅需单比特反馈的上行功率控制机制；支持具有离散相位响应的RIS，并在定位性能上超越指纹识别、深度强化学习和基于反向传播的位置估计器。

Results: 数值结果表明，所提方案在定位精度方面显著优于指纹识别、深度强化学习基线和基于反向传播的位置估计方法，同时具备低反馈开销和良好的实用性。

Conclusion: 该混合学习方法有效解决了RIS辅助定位中的联合优化问题，具备低通信开销、高精度和对实际RIS硬件特性的良好适应性，适用于未来低功耗智能反射面系统。

Related Work: 相关工作包括基于RIS的定位技术、深度强化学习在无线资源管理中的应用、指纹定位方法以及基于反向传播的可微分RIS优化方法。

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [12] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: 本文提出了一种基于条件生成扩散模型的高精度室外用户设备定位框架DiffLoc，直接利用大规模MIMO信道状态信息实现亚厘米级定位精度，显著优于现有方法，并通过一致性训练实现快速推理，适用于实时6G应用。


<details>
  <summary>Details</summary>
Motivation: 传统指纹定位方法在大范围动态室外环境中扩展性差，依赖密集且不切实际的数据采集，难以满足高精度和实时性需求。

Challenges: 如何从高维、复杂的信道状态信息中直接学习到高精度的连续地理坐标映射；在减少推理步骤的同时保持对高速移动用户和未知轨迹的鲁棒性。

Contributions: 提出了DiffLoc框架，首次将条件扩散模型应用于基于SRS指纹的室外定位；实现了亚厘米级定位精度；引入一致性训练将推理步骤从200步降至2步，显著提升效率。

Results: 在东京城市宏小区环境中，DiffLoc-CT模型实现0.5厘米融合定位精度和1-2厘米单基站精度，较监督回归方法（>10米误差）和网格融合方法（3米误差）提升一个数量级。

Conclusion: DiffLoc框架在精度和效率上均显著超越现有方法，具备在6G系统中实现实时高精度定位的潜力。

Related Work: 相关工作包括基于指纹的定位方法、监督回归模型、网格化融合技术以及生成模型在定位中的初步应用，但均未达到本文所实现的精度与效率平衡。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>
