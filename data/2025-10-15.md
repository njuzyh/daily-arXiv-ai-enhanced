<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一种优化的基于描述符的DMAC，针对小数据量任意传输的低效问题，通过轻量级描述符格式和低开销的推测性描述符预取机制，显著降低了传输启动延迟，提高了总线利用率，并在FPGA和12nm工艺上验证了其高性能与低资源占用。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用推动计算系统异构化发展，内存系统面临更复杂和频繁的数据传输需求。传统基于描述符的DMAC在处理小尺寸任意传输时因描述符开销大和串行处理而效率低下，亟需优化。

Challenges: 主要挑战包括：1）传统描述符格式开销大，导致小数据传输效率低；2）描述符处理串行化带来高启动延迟；3）在不增加误预测延迟的前提下实现高效预取机制；4）在资源受限条件下实现高性能DMAC设计。

Contributions: 1）提出一种轻量级描述符格式，减小描述符大小；2）设计低开销的推测性描述符预取机制，提升处理效率；3）实现并集成AXI4兼容的DMAC到RISC-V SoC中；4）在FPGA和12nm工艺上完成验证，展示高性能与低资源消耗。

Results: 相比商用DMAC IP，传输启动延迟降低1.66倍；理想内存系统下64字节传输总线利用率提升达2.5倍；深度内存系统中可达3.6倍；资源消耗减少11%查找表、23%触发器，且无需块RAM；在GF12LP+工艺下达到1.44GHz以上主频，仅占49.5 kGE。

Conclusion: 所提出的优化DMAC有效解决了小尺寸任意传输的效率问题，在性能、延迟和资源利用方面均显著优于传统方案，适用于高异构计算场景下的高效内存传输需求。

Related Work: 相关工作主要集中在基于描述符的DMAC架构及其在嵌入式系统和SoC中的应用，已有研究关注描述符管理与DMA效率优化，但对小数据量传输的静态开销和预取机制优化研究不足。

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe是一种针对大语言模型服务中动态请求模式和资源碎片化问题的新型系统，通过运行时动态重构流水线架构，实现细粒度模型划分、运行中流水线重构和拓扑感知资源分配，显著提升资源效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 生产环境中大语言模型面临请求模式高度变化和服务器集群资源碎片化的挑战，现有静态流水线配置难以适应动态工作负载，导致效率低下。

Challenges: 如何在动态请求模式下有效管理资源，避免服务器less集群中的资源碎片化，并在不牺牲性能的情况下提升资源利用率。

Contributions: 提出FlexPipe系统，包含三项创新：保持计算图约束的细粒度模型划分、具有缓存一致性的运行中流水线重构、以及应对GPU碎片化的拓扑感知资源分配策略。

Results: 在82-GPU集群上的实验表明，与现有最先进系统相比，FlexPipe资源效率提升最高达8.5倍，延迟降低38.3%，GPU预留需求从峰值的75%降至30%。

Conclusion: FlexPipe通过动态调整流水线结构，有效应对了大模型服务中的动态性和资源碎片化问题，显著提升了资源利用率和响应性能，适用于高变化负载的生产环境。

Related Work: 相关工作主要集中在静态模型并行和流水线并行技术，如PipeDream和GPipe，这些方法缺乏运行时动态重构能力，难以适应变化的请求模式和资源状况。

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [3] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: 本文提出以单个计算节点为基本单位进行跨平台性能和可扩展性研究的方法，并提供了设置、运行和分析节点间扩展性研究的指导，以及结果展示模板和案例研究。


<details>
  <summary>Details</summary>
Motivation: 由于高性能计算架构的多样性增加，研究人员和实践者对跨平台比较代码性能和可扩展性越来越感兴趣，但缺乏相关研究设置和分析的指导。

Challenges: 如何在不同平台上设置和分析跨平台性能和可扩展性研究缺乏明确的指导。

Contributions: 提出了以单个计算节点为基本单位的研究方法，提供了设置、运行和分析节点间扩展性研究的指导，设计了结果展示模板，并通过多个案例研究验证了该方法的优势。

Results: 通过多个案例研究展示了所提方法在跨平台性能比较中的有效性，证明了该方法能够清晰呈现不同平台间的性能差异和扩展性特征。

Conclusion: 以单个计算节点为基础进行跨平台性能和可扩展性研究是一种有效且可行的方法，有助于标准化和提升跨平台性能分析的可靠性。

Related Work: 现有研究多关注单一平台内的性能优化，较少涉及跨平台节点级性能比较的系统性方法。

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [4] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 提出了两种基于GPU加速的进程映射算法，一种采用分层多分区策略，另一种将进程映射集成到现代多级图划分流程中，均显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 受近期基于GPU的图划分器成功的启发，旨在加速超算中任务图到处理单元的映射过程，以平衡计算负载并最小化通信开销。

Challenges: 如何在利用GPU并行性加速映射的同时，保持较低的通信成本并确保负载均衡。

Contributions: 提出了首个基于GPU的进程映射算法：一是结合超级计算机层次结构的分层多分区方法；二是将映射直接集成到多级图划分流程中，并加速其关键阶段。

Results: 实验显示，相比当前最先进的CPU算法，两种方法的速度提升超过300倍；第二种方法几何平均加速达77.6倍，峰值达598倍，但解的质量略有下降。

Conclusion: 这是首次将GPU用于进程映射问题，两种算法均实现显著加速，其中第二种更快但通信开销较高，为大规模科学计算中的映射问题提供了高效新方案。

Related Work: 现有工作主要集中在CPU上的图划分与进程映射，近年来GPU在图划分任务中展现出高性能，但尚未应用于进程映射问题。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [5] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出一种基于Kubernetes的工具，支持延迟、非侵入式地注入云设计模式，提升数据共享管道的能效与灵活性。


<details>
  <summary>Details</summary>
Motivation: 在数据网格架构中，传统云设计模式的预定义和嵌入会破坏模块化、降低可重用性，并与消费者驱动的动态特性冲突。

Challenges: 如何在不修改服务源码的前提下，实现云设计模式的灵活集成，同时保持管道的模块化与可重用性。

Contributions: 设计并实现了一个基于Kubernetes的工具，支持延迟注入云设计模式，并提供能耗监控功能，使开发者能在不影响架构灵活性的情况下进行能效优化。

Results: 该工具实现了非侵入式的模式注入，能够在不影响服务代码的前提下应用设计模式，并成功收集能耗数据，支持能效感知的开发决策。

Conclusion: 所提方法在保持数据共享管道高模块化和可重用性的同时，有效集成了云设计模式，并为能效优化提供了实践支持。

Related Work: 已有研究关注于模块化数据管道的构建及其在可扩展性和能效方面的优势，但缺乏对设计模式动态集成的支持。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [6] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: TALP-Pages是一个易于集成的框架，用于在HPC代码开发过程中提供快速、基于仓库的性能反馈，通过可视化展示性能因子回归和扩展效率表，帮助开发者及时发现性能变化。


<details>
  <summary>Details</summary>
Motivation: 在高性能计算（HPC）代码的持续开发中，及早发现性能退化并获得与开发流程紧密结合的应用程序扩展行为洞察是关键需求。

Challenges: 如何在资源受限的情况下，以低开销实现对代码性能的持续监控，并在CI流程中快速生成有意义的性能分析报告。

Contributions: 提出了TALP-Pages框架，能够基于TALP收集的性能指标，在CI环境中自动生成包含性能回归和扩展效率的HTML报告，并展示了其在GENE-X项目中的易集成性和有效性。

Results: 相比基于追踪的工具，TALP-Pages能在更短时间内、更低资源消耗下生成扩展效率表，并成功在GENE-X的CI流程中检测到性能改进。

Conclusion: TALP-Pages为HPC应用提供了轻量、高效的性能监控解决方案，可无缝集成到现有开发流程中，提升性能问题的可见性与可解释性。

Related Work: 基于追踪的性能分析工具，以及现有的CI集成性能监控方法。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>
