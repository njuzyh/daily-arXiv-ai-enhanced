<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一种优化的基于描述符的DMAC，旨在高效处理小数据单元的任意内存传输，通过轻量级描述符格式和低开销的推测性预取机制，在FPGA和标准工艺中实现了显著的性能提升和资源节约。


<details>
  <summary>Details</summary>
Motivation: 传统基于描述符的DMAC在处理小数据单元的任意传输时效率低下，存在描述符开销大和处理串行化的问题，难以满足现代异构计算系统对高效内存传输的需求。

Method: 设计了一种轻量级描述符格式，并在AXI4-based DMAC中实现；引入低开销的推测性描述符预取机制，避免误预测带来的额外延迟；将DMAC集成到64位RISC-V SoC中，并在Kintex FPGA上进行仿真验证，同时在GF12LP+工艺节点上进行综合。

Result: 相比商用DMAC IP，传输启动延迟降低1.66倍，理想内存系统下总线利用率提升达2.5倍（64字节传输），资源占用减少（LUT减少11%，FF减少23%，无需BRAM）；在深层内存系统中总线利用率可达3.6倍；在GF12LP+工艺下达到1.44 GHz以上主频，面积仅49.5 kGE。

Conclusion: 所提出的优化DMAC有效解决了小数据单元传输的效率问题，在性能、资源利用率和可集成性方面均表现出显著优势，适用于现代异构计算系统的高效内存传输需求。

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe是一种针对大语言模型服务中动态请求模式和资源碎片化问题的新型系统，通过运行时动态重构流水线架构，显著提升了资源效率并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型服务系统依赖静态流水线配置，难以应对动态工作负载，导致资源利用率低和延迟高。

Method: FlexPipe将模型分解为细粒度阶段，并基于实时请求模式分析动态调整流水线粒度，提出细粒度模型划分、运行时流水线重构与一致性缓存迁移、拓扑感知的资源分配三项关键技术。

Result: 在82-GPU集群上的实验表明，与现有最先进系统相比，FlexPipe资源效率提升高达8.5倍，延迟降低38.3%，GPU预留需求从峰值的75%降至30%。

Conclusion: FlexPipe通过动态流水线重构有效应对了服务器less集群中大语言模型服务的资源碎片化和负载波动挑战，显著提高了系统性能和资源利用率。

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [3] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: 本文提出以单个计算节点为基本单位进行跨平台性能和可扩展性研究的方法，并提供了设置、运行和分析节点间扩展性研究的指导，以及结果展示模板和案例研究。


<details>
  <summary>Details</summary>
Motivation: 由于高性能计算架构的多样性增加，研究人员和实践者对跨平台比较代码性能和可扩展性越来越感兴趣，但缺乏相关研究设置和分析的指导。

Method: 提出以单个计算节点为基本单位进行跨平台研究，提供节点间扩展性研究的设置、运行和分析指导，并设计结果展示模板。

Result: 提供了跨平台性能研究的系统化方法和模板，并通过多个案例研究验证了该方法的有效性和优势。

Conclusion: 以单个计算节点为单位的跨平台研究方法有助于标准化性能比较，提升研究的可重复性和实用性。

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [4] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 本文提出了两种基于GPU的加速算法来解决超级计算机中任务图到处理单元的映射问题，旨在平衡计算负载并最小化通信开销。


<details>
  <summary>Details</summary>
Motivation: 受基于GPU的图划分器近期成功的启发，研究者希望利用GPU的并行性加速过程映射这一优化问题。

Method: 第一种算法采用分层多分区策略，沿超级计算机的层次结构划分任务图，并利用基于GPU的图划分器加速映射过程；第二种算法将过程映射直接集成到现代多级图划分流程中，通过GPU加速关键阶段（如粗化和细化）。

Result: 实验表明，两种方法相比最先进的CPU算法实现了超过300倍的加速。第一种算法通信开销平均高约10%，仍具竞争力；第二种方法更快，几何平均加速达77.6倍，峰值达598倍，但解的质量较低。

Conclusion: 据作者所知，这是首次提出的基于GPU的过程映射算法，展示了GPU在该问题上的巨大加速潜力。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [5] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出一种基于Kubernetes的工具，支持延迟、非侵入式地注入云设计模式，提升数据共享管道的能效与灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统云设计模式的预定义和嵌入会破坏数据共享管道的模块化、可重用性及其动态响应消费者需求的能力。

Method: 开发一个基于Kubernetes的工具，实现无需修改服务源码即可动态注入云设计模式，并自动收集能耗指标。

Result: 该工具支持自动化模式注入并提供能效数据，帮助开发者在不牺牲架构灵活性的前提下进行能效优化决策。

Conclusion: 所提方法在保持数据共享管道高可组合性和模块化的同时，有效集成了云设计模式并提升了能源效率。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [6] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: 本文提出了一种名为TALP-Pages的轻量级框架，用于在HPC代码开发过程中快速检测性能退化并提供可扩展性分析，通过与CI流程集成实现即时性能反馈。


<details>
  <summary>Details</summary>
Motivation: 在高性能计算（HPC）代码持续开发过程中，亟需在早期检测性能退化，并将性能分析紧密集成到开发流程中，以提升开发效率。

Method: 基于TALP工具实现运行时性能指标采集，利用适合CI的目录结构存储数据，TALP-Pages据此生成包含性能回归图表和扩展效率表的HTML报告。

Result: 相比基于追踪的工具，TALP-Pages在更少资源消耗和更低后处理成本下，能更快生成扩展效率分析结果，并成功集成到GENE-X的CI系统中，验证了其检测性能改进的能力。

Conclusion: TALP-Pages是一种易于集成、高效且适用于持续集成环境的性能监控框架，可有效支持HPC应用开发中的性能回归检测与扩展性分析。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>
