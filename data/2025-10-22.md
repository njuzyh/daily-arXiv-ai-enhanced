<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 22]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Is It Worth to Use Feedback Channel in 5G V2X Platoon Scenarios?](https://arxiv.org/abs/2510.17410)
*Dmitry Bankov,Artem Krasilov,Artem Otmakhov,Pavel Savlukovich,Evgeny Khorov*

Main category: cs.NI

TL;DR: 本文研究了5G车联网（V2X）中反馈信道的使用对系统容量的影响，特别是在车队组播和周围车辆广播共存的场景下。通过NS-3仿真发现，反馈信道在某些条件下可使系统容量提升一倍，而在其他情况下则可能使其减半，进而提出了自适应调整反馈信道参数的方法。


<details>
  <summary>Details</summary>
Motivation: 5G V2X引入反馈信道以提高数据传输可靠性并支持新应用场景（如车队行驶、远程驾驶等），但反馈信道占用资源可能影响数据传输资源，因此需评估其对系统容量的净影响。

Challenges: 反馈信道虽能提升传输可靠性，但会占用部分无线资源，降低可用于数据传输的容量；不同交通场景（如车队大小、组播与广播流量强度、QoS需求）下反馈信道的影响差异显著，难以统一配置参数。

Contributions: 1. 在NS-3中构建了包含车队组播和周边车辆广播的混合V2X通信场景；2. 量化分析了反馈信道对系统整体容量的影响；3. 揭示了反馈信道在不同条件下可能显著提升或降低系统容量的原因；4. 提出了反馈信道参数的自适应选择策略。

Results: 仿真结果表明，反馈信道的使用效果高度依赖于车队规模、组播与广播流量强度及QoS要求：在某些配置下系统容量可提升至2倍，而在其他情况下容量几乎减半。

Conclusion: 反馈信道并非在所有场景下都有益于系统容量，其使用需根据具体交通和通信条件进行权衡；通过自适应调整反馈信道参数，可在保证可靠性的同时最大化系统效率。

Related Work: 相关工作主要集中在4G V2X的广播通信机制以及5G V2X的组播与单播能力扩展，已有研究初步探讨了反馈机制对可靠性的提升，但缺乏对系统容量影响的系统性评估。

Abstract: 5G Vehicle-to-Everything (V2X) is a new technology developed by 3GPP to
support inter-vehicle communication. In contrast to 4G V2X which allows only
broadcast communication, 5G V2X enables groupcast and unicast communication.
Such types of communication are needed for new V2X scenarios: platooning,
extended sensors, remote driving, etc. To improve the data transmission
reliability and assist in the selection of the transmission parameters in these
scenarios, 5G V2X introduces a feedback channel that allows receivers to send
acknowledgments in response to data packets. However, some part of the overall
resource shall be allocated for the feedback channel, which reduces the amount
of channel resources available for data transmission. In this paper, we
consider a scenario with a platoon, which generates groupcast traffic, and
surrounding vehicles, which generate legacy broadcast traffic. Using extensive
simulations in NS-3, we analyze how the usage of the feedback channel
influences the overall system capacity. Our results show that depending on the
platoon size, groupcast, and broadcast traffic intensities, and their quality
of service requirements, the usage of the feedback channel can in some cases
significantly increase the system capacity (up to 2x), while in other cases it
almost halves the system capacity. We explain the reasons for such effects and
discuss how to adaptively select the feedback channel parameters.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [2] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 本文提出了一种基于多模态大语言模型的助手（MLLMA），用于芯片物理设计中的拥塞预测与可解释的设计优化建议，结合遗传提示与偏好学习框架，生成可视化的“设计建议卡组”，在准确性和可解释性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的电子设计自动化（EDA）工具在提供可解释的布线拥塞反馈和 actionable 优化建议方面存在不足，难以满足工程师对可解释性和实际指导的需求。

Challenges: 如何从复杂的多模态设计数据（视觉、表格、文本）中提取关键特征；如何建模与拥塞相关的权衡并生成人类可理解的建议；如何确保建议符合实际设计原则且具有可操作性。

Contributions: 1）提出MLLMA框架，结合多模态大语言模型与遗传提示实现自动化特征生成；2）设计可解释的偏好学习框架，融合多模态输入进行拥塞预测；3）构建“设计建议卡组”，提供针对性优化建议；4）在CircuitNet基准上验证了方法在准确性和可解释性上的优越性。

Results: 在CircuitNet基准上，本方法在拥塞预测准确性方面优于现有模型；定性分析和案例研究表明，生成的设计建议符合实际设计原则，并被工程师认为具有可操作性。

Conclusion: MLLMA能够作为交互式助手，有效支持可解释、上下文感知的芯片物理设计优化，展示了多模态大语言模型在EDA领域的重要潜力。

Related Work: 相关工作包括基于机器学习的拥塞预测方法、可解释AI在EDA中的应用、以及大语言模型在代码或设计生成中的尝试，但缺乏对多模态输入的联合建模与可操作建议生成。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [3] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 本文提出了一种在内存读取地址流中以非破坏性方式编码用户可见状态的方法，使内存设备能够获取程序上下文信息，从而实现更优的运行时优化，如请求优先级调度、数据重映射等，并通过原型系统验证了元数据注入与检测的可行性。


<details>
  <summary>Details</summary>
Motivation: 由于硬件预取、内存调度和地址映射等原因，操作系统难以将内存访问活动准确映射回程序中的函数或对象，导致缺乏程序上下文信息，限制了内存优化的潜力。程序员具备关于数据访问模式和优先级的先验知识，若能传递给内存设备，可提升系统性能。

Challenges: 如何在不增加显著开销、无需特殊权限或驱动的情况下，将程序上下文信息传递到内存设备；如何在保持内存协议兼容的同时，实现元数据的可靠注入与检测；如何在处理器与内存解耦的架构下重建程序与内存访问之间的语义关联。

Contributions: 提出一种将程序上下文编码为内存读地址流中可检测包的机制；实现了端到端的元数据注入与检测原型系统；展示了精确代码执行标记和对象地址范围跟踪的应用案例；为未来基于近内存计算的实时元数据处理和应用提示优化提供了基础。

Results: 原型系统能够可靠地从内存地址轨迹中检测和解码注入的元数据，验证了该方法在主机处理器和内存模块上的可行性；初步用例显示该方法可用于精确跟踪程序执行和数据布局。

Conclusion: 通过在内存地址流中编码上下文信息，可以在不破坏现有系统结构的前提下，有效恢复程序与内存访问之间的语义联系，为未来内存系统的智能化优化提供了新途径。

Related Work: 相关工作包括页面访问热度图单元（HMU）、内存侧遥测硬件、硬件预取器、近内存计算（NMC）架构以及内存访问追踪技术，但这些方法大多缺乏来自应用程序的主动提示机制，本文通过编码上下文信息填补了这一空白。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [4] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D集成技术因光刻微缩挑战和微型通孔制造能力的提升而受到广泛关注，其通过提高封装密度加速系统小型化，但也带来了更高的功耗密度及设计、制造、测试等方面的新挑战，需跨层协同设计并确保可测试性和良率。


<details>
  <summary>Details</summary>
Motivation: 随着传统光刻微缩难度增加，3D集成成为提升系统密度的重要替代方案，同时微型通孔技术的进步使其更具可行性。

Challenges: 3D集成面临高功率密度、跨层电路/通孔/宏单元的协同布局、各层独立可测试性、组装后的良率管理以及最终产品测试等挑战。

Contributions: 本文系统性地阐述了3D系统在设计、组装和测试方面的新挑战，并强调了发挥3D集成优势所需解决的关键问题。

Results: 提出了3D集成在提升密度方面的潜力，并识别出影响其成功应用的关键技术挑战，为后续研究提供了方向。

Conclusion: 要充分发挥3D集成的优势，必须解决与设计、制造和测试相关的一系列新挑战，包括跨层协同设计、独立测试能力和合理良率保障。

Related Work: 已有研究对3D系统的潜在优势进行了更全面的探讨，本文在此基础上聚焦于设计、组装与测试中的关键技术挑战。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [5] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink是一个创新的集合通信框架，通过整合NVLink、PCIe和RDMA NIC等异构链路，提升多节点大语言模型部署中的通信性能，在8-GPU H800服务器上相比NCCL基线最高提升26%至27%的带宽。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署导致通信成为性能瓶颈，现有通信库未能充分利用硬件中的多种互连资源。

Challenges: 如何有效聚合NVLink、PCIe和RDMA等异构链路并实现负载均衡，避免高速链路受制于低速链路，同时保持与现有API的兼容性。

Contributions: 提出FlexLink，首个系统性整合多种异构互连的集合通信框架，采用两阶段自适应负载均衡策略，实现通信流量的动态分配，并作为兼容NCCL API的无损即插即用替代方案。

Results: 在8-GPU H800服务器上，AllReduce和AllGather的带宽分别提升最多26%和27%，通过将2%-22%的通信流量卸载到原本未充分利用的PCIe和RDMA NIC上实现性能增益。

Conclusion: FlexLink有效提升了多GPU系统中的通信效率，为大规模语言模型的部署提供了高性能、易集成的通信解决方案。

Related Work: 现有的工作主要依赖单一互连（如NCCL使用NVLink），未能充分利用多类型互连资源，缺乏对异构链路的系统性整合与动态负载均衡设计。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [6] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 本文提出了一种架构无关的测试方案，用于分析消费级NVIDIA GPU中矩阵乘法器的数值特性，涵盖多种输入输出精度格式。实验结果表明，RTX-3060与数据中心级A100 GPU的数值特性一致，且该方法可扩展至未来NVIDIA GPU架构和新型精度格式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分析GPU矩阵乘法器数值特性时依赖设备特定常量或进行穷举搜索，缺乏通用性和可扩展性。本文旨在提出一种不依赖硬件特性的通用测试方法，适用于消费级GPU及多种混合精度格式。

Challenges: 主要挑战包括：设计不依赖设备特定常量的测试向量生成方法；支持多种浮点格式（如binary16、TF32、bfloat16等）的灵活适配；确保方法在不同GPU架构（如Ampere、Ada Lovelace）上的可移植性；准确提取累加器的舍入、归一化和内部精度等数值特性。

Contributions: 1. 提出一种架构无关的矩阵乘法器数值特性分析方法；2. 实现无需穷举搜索或硬编码设备参数的测试向量生成方案；3. 首次在消费级GPU（RTX-3060、Ada RTX-1000）上验证该方法，并揭示其与数据中心GPU（A100）的数值一致性；4. 方法可扩展至Hopper、Blackwell等未来架构及8位浮点格式。

Results: 在RTX-3060（Ampere架构）和Ada RTX-1000（Ada Lovelace架构）上成功提取了binary16、TF32、bfloat16输入格式与binary16、binary32输出格式下的矩阵乘法器数值特性；实验证明RTX-3060与A100 GPU在数值行为上完全一致；所提方法无需修改即可适用于新型GPU架构和精度格式。

Conclusion: 本文提出的测试方案有效克服了传统方法的局限性，实现了对消费级GPU矩阵乘法器数值特性的精确分析，并验证了其与数据中心级GPU的一致性，表明该方法具有良好的通用性、可扩展性和未来适用性。

Related Work: 相关工作主要集中在数据中心GPU（如NVIDIA A100）矩阵乘法器的数值特性分析，通常采用穷举搜索或依赖设备特定常量的测试方法。本文扩展了这些方法，使其适用于消费级GPU并提升通用性，填补了消费级GPU低层次数值行为研究的空白。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [7] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+ 是首个针对消费级区域化闪存存储设计的仿真器，扩展了块接口支持以提升可用性，允许用户研究内部架构并集成优化方案，同时验证了其准确性并进行了多个案例研究。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和优化消费级区域化闪存存储的软硬件设计，需要一个能够准确模拟其资源限制和架构特征的仿真平台。

Challenges: 原始ConZone缺乏原地更新能力，无法挂载文件系统（如F2FS所需的元数据区），限制了其可用性。

Contributions: 提出了ConZone+，扩展了对块接口的支持，提供了部署脚本和多项改进，增强了仿真器的可用性和功能性，支持用户集成优化并研究系统设计。

Results: 验证了ConZone+在模拟消费级区域化闪存存储硬件架构方面的准确性，并通过多个案例研究揭示了当前文件系统在该架构下的不足。

Conclusion: ConZone+ 是一个高效、可用的仿真平台，有助于推动消费级区域化闪存存储的软硬件协同设计与优化。

Related Work: 现有工作缺乏对消费级设备中典型资源约束（如有限映射缓存、写缓冲区）和混合闪存管理的综合建模，ConZone系列填补了这一空白。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [8] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s是一个开源框架，提供RISC-V RV32I架构的实用微架构路线图，从单周期核心逐步演进到具备完整功能的五级流水线核心，并在FPGA上实现验证，所有设计资源在MIT许可下公开，支持开源硬件教育与复现。


<details>
  <summary>Details</summary>
Motivation: 弥合计算机体系结构教学中理论知识与硬件实现之间的鸿沟，为RISC-V架构提供一个可实践、可复现的教学与开发平台。

Challenges: 如何在保持教学清晰性的同时实现完整的流水线功能（如数据转发、动态分支预测和异常处理），并确保设计在真实FPGA平台上的可验证性和性能表现。

Contributions: 1) 提供了一个从单周期到五级流水线RISC-V核心的渐进式微架构设计框架；2) 实现了包含UART的完整SoC并在Xilinx Artix-7 FPGA上验证；3) 全面公开RTL代码、逻辑框图和开发日志，促进开源硬件教育。

Results: 最终核心在50 MHz下实现1.09 DMIPS/MHz的性能，成功运行在FPGA上并通过UART进行通信验证，所有设计资料已开源。

Conclusion: BASIC_RV32s为RISC-V架构提供了一个可复现、可教学的开源微架构实现路径，有效连接了体系结构理论与硬件实践。

Related Work: 基于Patterson和Hennessy的经典计算机体系结构设计方法，延续了从简单到复杂的处理器设计教学范式，与RISC-V开源硬件社区中的其他开源核心（如PicoRV32、VexRiscv）相比，更注重教学演进过程的完整记录与公开。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [9] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 本文提出了一种无需新增指令或修改缓存一致性协议的轻量级硬件事务内存（HTM）设计，仅通过扩展Load-Linked和Store-Conditional指令语义，并将实现限制在L1数据缓存中，显著降低了硬件复杂度。该设计适用于读写集较小的事务场景，且通过两种机制保障前向进展。实验表明，在低争用和多节点并发场景下性能良好，尤其在原子fetch-and-increment操作中优于TTS锁。


<details>
  <summary>Details</summary>
Motivation: 现有的HTM方案因硬件复杂度高、需扩展指令集架构（ISA）或修改缓存一致性协议而难以普及，本文旨在设计一种更简单、易集成的HTM方案以促进其实际应用。

Challenges: 如何在不增加新指令、不修改标准缓存一致性协议的前提下实现高效的HTM；如何简化硬件实现以降低复杂度；如何保证事务的前向进展，避免无限重试。

Contributions: 1. 提出一种仅通过扩展Load-Linked和Store-Conditional语义实现HTM的方法，无需新增ISA指令；2. HTM实现完全限定在L1数据缓存中，不修改一致性协议；3. 限制事务读写集大小以简化设计；4. 提出两种基于重试检测的前向进展保障机制；5. 在Gem5上验证了设计有效性，并实现了多种并发数据结构。

Results: 实验表明，大多数并发数据结构的事务读写集不超过8个字（cache lines）；在低争用场景下，事务中止率低，性能优于TTS锁；在多节点并发且争用分散时具有良好的可扩展性。

Conclusion: 本文证明了在不修改ISA和缓存一致性协议的前提下，通过简化设计和限制事务规模，仍可实现高效、实用的HTM，为HTM的广泛应用提供了可行路径。

Related Work: 本文与以往需要新增指令（如LL/SC扩展、专用事务指令）或修改MESI等缓存一致性协议的HTM工作不同，最相关的是基于LL/SC的事务内存研究，但本文进一步简化了硬件实现并避免协议修改。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [10] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 该论文探讨了通过3D集成光学与逻辑技术实现的高带宽、低功耗互连（如3D CPO）在训练超大规模混合专家模型（MoE）中的关键作用，表明其可将扩展能力提升8倍，训练时间减少2.7倍。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的持续增长，传统半导体缩放放缓，迫切需要通过新型互连技术实现计算、内存和互连性能的协同提升，以支持万亿参数级大模型的训练。

Challenges: 铜互连受限于约1米的最大传输距离，难以跨机架扩展；现有互连技术在带宽、功耗和扩展性方面无法满足前沿大模型的训练需求。

Contributions: 提出并评估了基于3D晶圆级集成的共封装光学（3D CPO）互连架构（Passage），展示了其在大规模GPU集群中实现高带宽、低延迟通信的能力，并量化了其在训练超大规模MoE模型时的性能增益。

Results: 3D CPO技术使扩展能力提升8倍，支持更高维度的并行训练，训练时间减少2.7倍，显著提升了大模型训练效率。

Conclusion: 3D CPO为代表的光子互连技术是突破当前AI系统扩展瓶颈的关键，能够支持跨多机架的高效GPU集群，推动万亿参数级模型的实用化发展。

Related Work: 相关工作包括高带宽电互连、硅光子学、共封装光学（CPO）以及用于AI训练的多芯片模块和晶圆级集成技术。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [11] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: 本文提出了DiffPlace，一种基于条件去噪扩散模型的芯片布局框架，能够泛化到未见过的电路网表而无需重新训练，有效解决了传统方法在硬约束和重训练上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的芯片布局方法依赖于解析优化或强化学习，难以处理复杂的布局约束，且需为每个新电路设计进行耗时的在线训练，限制了其在实际VLSI设计中的可扩展性和效率。

Challenges: 主要挑战包括：在满足严格布局约束（如无重叠、布通率）的同时实现高质量布局；使模型具备跨不同电路网表的泛化能力；在庞大的布局空间中高效搜索全局最优解。

Contributions: 1) 首次将扩散模型引入芯片布局问题，提出DiffPlace框架；2) 通过条件生成建模实现无需微调的跨网表泛化；3) 结合能量引导采样与约束流形扩散，确保布局合法性；4) 在多种实验场景下实现极低重叠率和高质量结果。

Results: 实验表明，DiffPlace在多个基准电路上实现了极低的模块重叠率，同时保持优异的性能、功耗和可布线性指标，优于传统优化和现有学习方法，且无需针对新设计重新训练。

Conclusion: DiffPlace成功融合了优化与学习方法的优势，为现代VLSI设计提供了一条通往自动化、高质量芯片布局的实用路径。

Related Work: 相关工作包括基于梯度的解析优化方法（如模拟退火、力导向布局）和基于强化学习的布局方法（如DeepPlace、Reinforcement Learning for Placement），但这些方法通常难以泛化或需大量在线训练。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [12] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 本文提出了意图驱动的存储系统（IDSS），利用大语言模型（LLM）从非结构化信号中推断工作负载和系统意图，实现跨层参数自适应配置，提升存储系统性能与自主性。


<details>
  <summary>Details</summary>
Motivation: 现有的存储系统缺乏对工作负载意图的理解，导致依赖脆弱的启发式策略和孤立的优化方法，难以适应现代大规模数据密集型应用的需求。

Challenges: 如何从非结构化信号中准确提取工作负载意图；如何将大语言模型安全、高效地集成到存储控制回路中；如何在多目标竞争下做出符合策略约束的优化决策。

Contributions: 提出了意图驱动存储系统（IDSS）的新范式；总结了将大语言模型融入存储控制的四项设计原则；设计了相应的系统架构；通过实验验证了IDSS在IOPS上最高可提升2.45倍。

Results: 在FileBench工作负载上的初步结果显示，IDSS通过解析意图并生成可执行配置，最高可将IOPS提升2.45倍，表明在策略约束和结构化流程中，大语言模型可作为高效的高层语义优化器。

Conclusion: IDSS展示了在策略约束下，大语言模型能够有效桥接应用目标与底层系统控制，推动存储系统向更智能、自适应和自主的方向发展。

Related Work: 相关工作包括自动化存储优化、基于机器学习的系统调优、语义感知计算以及大语言模型在系统控制中的初步应用，但现有研究缺乏对意图理解的深入建模和跨层协同优化。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [13] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 本文提出了一种基于大语言模型（LLM）的新型框架VeriPPA，用于芯片设计中的功耗-性能-面积（PPA）优化和Verilog代码生成。该框架采用两阶段方法，第一阶段提升生成代码的语法和功能正确性，第二阶段优化PPA指标。在RTLLM和VerilogEval数据集上，VeriPPA在语法和功能正确性方面均优于现有最先进方法，并能有效优化电路设计的PPA，展示了LLM在复杂技术领域自动化芯片设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在内容生成方面的快速发展，将其应用于高度复杂且专业的芯片设计领域具有重要意义。现有方法在Verilog代码生成的正确性和PPA优化方面仍存在不足，亟需一种能同时保证功能正确性与设计性能的自动化解决方案。

Challenges: 主要挑战包括：1）确保LLM生成的Verilog代码在语法和功能上的正确性；2）在满足电路功能的前提下优化PPA指标；3）将自然语言指令或高级描述准确转化为可综合、高性能的硬件描述代码；4）处理芯片设计中复杂的约束条件和优化目标。

Contributions: 1）提出了一种名为VeriPPA的新型框架，首次将LLM应用于PPA优化与Verilog代码生成的联合任务；2）设计了两阶段优化流程，分别提升代码正确性与设计性能；3）在多个基准数据集上实现了优于当前最先进方法的代码生成准确率；4）验证了LLM在复杂硬件设计自动化中的可行性与优势。

Results: 在RTLLM数据集上，VeriPPA实现了81.37%的语法正确率和62.06%的功能正确率；在VerilogEval数据集上，语法正确率达到99.56%，功能正确率为43.79%，均超过现有SOTA方法（分别为92.11%和33.57%）。此外，该框架能够有效优化电路设计的PPA指标，显著提升设计质量。

Conclusion: VeriPPA框架展示了大语言模型在芯片设计自动化中的巨大潜力，特别是在Verilog代码生成和PPA优化方面表现优异。研究结果表明，LLM不仅可用于代码生成，还能结合专业优化目标实现高质量硬件设计，为未来EDA工具的智能化发展提供了新方向。

Related Work: 相关工作主要包括：1）基于深度学习的RTL代码生成方法；2）使用程序合成技术进行硬件描述代码生成；3）传统的PPA优化流程与EDA工具；4）近期将大语言模型应用于代码生成的研究（如CodeGen、PanGu-Coder等）；5）RTLLM和VerilogEval等用于评估硬件代码生成能力的数据集。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [14] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 提出了一种面向可配置IP的功能验证的全自动框架，通过需求驱动的自动化流程显著减少验证工作量，提高覆盖率和开发效率。


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧，需在保证质量与可靠性的同时降低芯片成本，而可配置IP的功能验证因其复杂性和资源消耗成为开发成本的主要来源。

Challenges: 可配置IP的功能验证过程复杂、耗时且容易出错，传统方法难以满足高效、高覆盖率和可扩展性的需求。

Contributions: 提出并实现了一个全自动化的、需求驱动的功能验证框架，集成了vPlan生成、测试平台创建、回归测试执行和报告生成功能，并将其与需求管理工具集成。

Results: 该框架显著减少了验证工作量，加快了开发周期，降低了人为错误，提高了验证覆盖率，具备良好的可扩展性和效率。

Conclusion: 该自动化框架为可配置IP的功能验证提供了一种高效、可扩展的解决方案，有助于降低开发成本并提升验证质量。

Related Work: 相关工作主要集中在功能验证自动化和验证流程管理，但缺乏与需求管理工具的深度集成和端到端的自动化支持。

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [15] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: FVDebug是一个自动化根因分析系统，通过结合波形、RTL代码和设计规范等多源数据，将形式验证失败的复杂反例转化为可操作的洞察，并提供具体的RTL修复建议。


<details>
  <summary>Details</summary>
Motivation: 形式验证失败的调试是现代硬件设计中最耗时的瓶颈之一，现有工具无法有效应对设计意图与实现逻辑之间的复杂交互，亟需智能化的自动化解决方案。

Challenges: 需要从跨多个周期的复杂反例中提取根因，整合不同抽象层次的数据（如波形、RTL代码、设计规范），并生成准确且可解释的因果分析。

Contributions: 提出FVDebug系统，包含三个核心组件：因果图构建、基于批量大模型分析的图扫描器、以及基于智能体叙事探索的洞察生成器，并集成了RTL修复生成模块。

Results: 在公开基准测试中表现出高假设质量和强Pass@k修复率，并在两个专有的工业级案例中验证了其在实际生产环境中的有效性。

Conclusion: FVDebug能够有效支持从学术研究到工业级设计的形式验证调试，显著提升调试效率。

Related Work: 现有工作主要依赖手动波形查看或简单的自动化工具，缺乏对设计意图与实现逻辑之间复杂关系的推理能力。

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [16] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 本文提出了一种基于解析延迟公式的数字集成电路符号时序分析新方法，能够计算电路内部信号转换时间的闭式表达式，支持无需仿真的逐转换时序分析，并可进行输入信号和门参数对时序特性影响的解析研究。


<details>
  <summary>Details</summary>
Motivation: 为了实现更高效、精确的符号时序分析，避免传统仿真方法的高开销，同时支持对时序特性与输入信号及门参数之间依赖关系的解析研究。

Challenges: 如何在固定信号转换顺序下，为复杂电路中的所有内部信号建立依赖于输入转换时间和门参数的闭式解析延迟表达式，并确保其在实际电路中的可计算性和可应用性。

Contributions: 1) 提出基于Ferdowsi等人延迟公式的符号时序分析框架；2) 实现闭式解析延迟表达式的自动生成；3) 支持灵敏度分析等解析研究；4) 在c17基准电路上验证了方法可行性。

Results: 在c17电路的NOR门版本上实现了所提方法，成功生成了符号延迟表达式，并展示了其在无需仿真情况下进行时序和灵敏度分析的能力。

Conclusion: 所提方法为数字电路提供了高效的符号时序分析手段，不仅可用于快速时序评估，还支持对电路行为的深入解析研究，具有理论与应用价值。

Related Work: Ferdowsi等人在NAHS 2025中提出的针对2输入NOR、NAND和Muller-C门的解析延迟公式是本工作的基础。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [17] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: 本文提出了Belenos，通过对生物力学有限元模拟的工作负载进行表征，揭示了当前硬件和软件栈的瓶颈，并强调了面向领域专用加速器的架构感知协同设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的硬件和软件架构在生物力学有限元仿真中存在效率低下问题，尤其影响迭代任务（如材料参数识别）的性能和可扩展性，导致在实际应用中不得不牺牲模型精度。因此，需要探索更高效的计算架构来提升仿真效率。

Challenges: 主要挑战包括前端停顿（特别是小规模工作负载）和后端瓶颈（大规模工作负载中后端受限周期高达82.2%以上），以及流水线、内存和分支预测器配置不当可能导致性能下降达37.1%。

Contributions: 提出了Belenos，结合FEBio模拟器、gem5敏感性分析和VTune分析，系统地表征了生物力学有限元仿真工作负载；揭示了不同规模工作负载的性能瓶颈；为领域专用加速器（DSA）提供了最优硬件配置建议。

Results: VTune分析显示小规模工作负载前端停顿约为13.1%，而大规模工作负载后端受限周期在59.9%到82.2%以上；gem5敏感性研究表明次优硬件配置可能导致性能下降最高达37.1%。

Conclusion: 生物力学仿真工作负载对硬件架构高度敏感，必须通过架构感知的协同设计来优化领域专用加速器，以克服当前性能瓶颈并提升仿真效率。

Related Work: 相关工作包括有限元分析软件FEBio的应用、基于gem5的处理器敏感性研究以及使用Intel VTune进行性能剖析的技术。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [18] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: 本文提出了一种名为SoCks的灵活且可扩展的构建框架，通过将SoC映像划分为称为“块”的高层单元来降低异构SoC设备开发的复杂性，实现封装式独立构建，减少依赖，并通过标准化接口促进模块重用和CI/CD集成，实验表明其构建速度比现有工具快达三倍。


<details>
  <summary>Details</summary>
Motivation: 随着现代异构SoC设备复杂性的增加，开发工具也变得复杂且缺乏支持，导致学习曲线陡峭和调试困难，因此需要一种能够简化开发流程、提升构建效率的解决方案。

Challenges: 如何在保证SoC各固件和软件组件无缝集成的前提下，降低构建系统的复杂性，减少模块间依赖，并支持高效、可复用和可自动化的开发流程。

Contributions: 提出了SoCks框架，引入了基于‘块’的模块化构建方法，实现了构建过程的封装性和独立性；定义了标准化接口以管理模块间通信；支持快速替换和重用模块（如根文件系统）；并推动了去中心化和部分自动化的CI/CD开发流程。

Results: 实验结果显示，SoCks构建完整SoC映像的速度比现有工具快达三倍，验证了其在提升构建效率方面的有效性。

Conclusion: SoCks通过模块化、低依赖和标准化接口的设计，显著降低了异构SoC系统的构建复杂性，提升了开发效率和可维护性，同时支持高效的自动化开发流程。

Related Work: 现有的SoC构建工具（如Yocto、Buildroot）通常整体性强、依赖复杂、构建耗时长，缺乏良好的模块封装和独立构建能力，而SoCks通过块化结构和接口标准化在这些方面实现了改进。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [19] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: 本文提出了TeLLMe，首个基于查找表的三值化大语言模型加速器，专为低功耗边缘FPGA设计，支持1.58位权重和8位激活，兼顾预填充和自回归解码阶段，显著提升了边缘设备上大模型推理的能效。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和其他嵌入式系统的兴起，将大语言模型（LLMs）部署到边缘平台成为迫切需求。然而，由于LLMs的高计算和内存需求，边缘部署面临挑战，尤其是资源受限、功耗限制以及预填充阶段的长延迟问题。

Challenges: 主要挑战包括：1）边缘设备的计算和内存资源有限；2）低比特量化虽压缩了模型，但预填充阶段延迟仍高；3）在低功耗约束下实现高吞吐和低延迟；4）高效利用FPGA片上存储资源。

Contributions: 1）提出基于查找表的三值矩阵乘法（TLMM）引擎，结合分组激活和在线预计算；2）设计基于URAM的细粒度权重缓冲管理方案；3）采用融合浮点逐元素操作与线性计算的流式数据流架构以隐藏延迟；4）提出反向重排序的预填充注意力机制；5）设计资源高效的专用解码注意力模块。

Results: 在5W功耗预算下，TeLLMe实现了最高25 tokens/s的解码吞吐量，以及0.45--0.96秒的首token时间（TTFT），适用于64--128 token的输入提示，显著提升了边缘FPGA上LLM推理的能效。

Conclusion: TeLLMe是首个全面支持预填充和自回归解码的低比特LLM边缘加速器，在低功耗FPGA上实现了高效、低延迟的大模型推理，为边缘智能设备部署LLMs提供了可行方案。

Related Work: 相关工作包括BitNet和DeepSeek等低比特量化方法，这些方法将权重压缩至1.58比特并保持较低精度损失，但未充分解决边缘部署中的资源、功耗和预填充延迟问题。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [20] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 本文提出了一种名为Kelle的软硬件协同设计解决方案，用于在基于eDRAM的边缘设备上高效部署大语言模型（LLM），通过细粒度内存驱逐、重计算和刷新控制算法，显著降低了KV缓存开销，实现了3.9倍的速度提升和4.5倍的能耗节约。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行大语言模型（LLM）对于降低延迟、提升实时处理能力和增强隐私保护至关重要。然而，由于KV缓存随序列长度线性增长，导致内存占用和访问成本高，而边缘设备资源有限，难以支持大规模缓存，因此亟需高效解决方案。

Challenges: 主要挑战包括：KV缓存随输入长度线性增长带来的巨大内存开销；边缘设备内存和计算能力有限；eDRAM虽密度高但需周期性刷新，带来较高功耗。

Contributions: 提出Kelle，一种面向eDRAM边缘系统的软硬件协同设计框架；引入细粒度内存驱逐、重计算机制和刷新控制算法，优化KV缓存管理；在保证性能的同时显著降低能耗。

Results: 与现有基线方案相比，Kelle实现了3.9倍的加速和4.5倍的能耗节省。

Conclusion: Kelle有效缓解了在资源受限的边缘设备上部署LLM时由KV缓存带来的性能与功耗瓶颈，为高效边缘LLM推理提供了可行路径。

Related Work: 相关工作集中在KV缓存压缩、内存层级优化、以及在SRAM或DRAM上进行LLM推理的硬件加速器设计。Kelle不同于以往工作之处在于专门针对eDRAM特性进行软硬件协同优化，兼顾高密度存储与能效管理。

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [21] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS致力于设计和评估非常规硬件加速器，以解决人工智能在功耗、效率和可扩展性方面的瓶颈，特别关注国防应用场景。本文介绍了其系统架构、软件栈以及用于早期全系统原型仿真的工具。


<details>
  <summary>Details</summary>
Motivation: 应对AI在功耗、效率和可扩展性方面的挑战，尤其是在国防领域（如自动驾驶车辆、监控无人机、海空平台）中对高效计算的迫切需求。

Challenges: 集成多种非传统硬件加速器（如光电、存内计算、神经形态计算）面临架构兼容性、能效优化和系统可扩展性等技术难题。

Contributions: 提出了一套完整的系统架构与软件堆栈，支持多种新型加速器的集成，并开发了用于早期系统级仿真的软件工具。

Results: 尚未在摘要中展示具体实验结果，但提出了一个可用于全系统及组件早期仿真的仿真软件框架。

Conclusion: ARCHYTAS为下一代AI加速器提供了一个集成化的设计与评估平台，有望显著提升国防相关AI应用的性能与能效。

Related Work: 与存内计算、神经形态计算和光电计算相关的硬件加速器研究，以及面向AI的专用架构设计工作。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [22] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 本研究提出了一种适用于孟加拉国等发展中国家城市背景的智能交通信号系统，利用RTSP视频流、树莓派4B和基于YOLO的模型检测非车道化异构交通，并通过NSGA-II优化信号配时，显著提升了达卡Palashi五岔路口的交通效率。


<details>
  <summary>Details</summary>
Motivation: 发展中国家城市如达卡交通密度高、拥堵严重，且交通模式为非车道化、异质性强，现有智能交通信号系统多针对结构化交通设计，难以适用，因此需要开发符合本地情境的解决方案。

Challenges: 达卡城市交通具有非车道化、车辆类型多样、行为不规范等异质性特点，传统交通检测与信号控制方法难以有效应对；同时系统需在低资源硬件上实现实时处理与优化。

Contributions: 1）构建了一个基于RTSP流和树莓派4B的低资源智能交通信号测试平台；2）采用在NHT-1071数据集上训练的YOLO模型实现异构交通目标检测；3）结合NSGA-II多目标优化算法生成最优信号配时方案；4）在真实五岔路口验证了系统的可行性与有效性。

Results: 在达卡Palashi五岔路口的实际测试表明，该系统能有效检测并分类异构交通流，通过优化信号配时显著减少车辆等待时间并提高通行量，验证了其在复杂交通环境下的应用潜力。

Conclusion: 本研究展示了一种面向发展中国家高密度、异构交通环境的低成本、可部署智能交通信号系统方案，为类似城市提供了可行的技术路径，推动了更具情境适应性的智能交通系统发展。

Related Work: 已有研究多集中于发达国家结构化车道交通的智能信号控制，使用高清摄像头与高性能计算设备，且较少考虑多类型车辆混行与非规则驾驶行为，缺乏对发展中国家复杂交通场景的适配性。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [23] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: 本文提出了一种名为smaRTLy的新优化技术，用于RTL逻辑综合中的多路复用器优化，通过创新策略减少冗余多路复用器树并重构剩余结构，显著降低门电路数量，在多个基准测试中优于Yosys。


<details>
  <summary>Details</summary>
Motivation: 传统工具如Yosys在优化多路复用器时未能充分利用信号间的逻辑关系和结构优化潜力，因此需要更高效的优化方法。

Challenges: 如何识别并消除多路复用器树中的冗余结构，同时重构逻辑以减少整体门数，同时保持功能正确性。

Contributions: 提出了新的逻辑推断和结构重构技术，有效去除冗余多路复用器树并优化结构，实现了比Yosys更优的面积缩减。

Results: 在IWLS-2005和RISC-V基准测试中，相比Yosys实现了8.95%的AIG面积减少；在百万门级工业基准上，多减少了47.2%的AIG面积。

Conclusion: smaRTLy通过逻辑推断与结构重建显著提升了RTL综合中的多路复用器优化效果，有助于实现更高效的硬件设计。

Related Work: 传统方法如Yosys通过遍历多路复用器树并监控控制端口值进行优化，但未充分挖掘信号间的逻辑关联和结构优化空间。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [24] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 本文提出并分析了基于MPI的并行Bootstrap算法，通过局部统计量聚合和同步伪随机数生成策略，显著降低了通信开销和内存使用，实现了大规模系统上的可扩展并行Bootstrap。


<details>
  <summary>Details</summary>
Motivation: 由于传统Bootstrap方法在大数据集或高重采样次数下计算成本过高，亟需高效的并行化方案以提升其可扩展性。

Challenges: 分布式环境下并行Bootstrap面临高通信开销和内存受限两大挑战，尤其当单个进程无法存储完整数据集时。

Contributions: 提出了两种新策略：1）局部统计量聚合，减少通信量；2）同步伪随机数生成，支持分布式重采样。同时建立了通信与计算复杂度的分析模型。

Results: 理论分析表明，所提方法相比朴素基线方法显著减少了通信量和内存使用，具备良好的可扩展性。

Conclusion: 所提出的并行Bootstrap策略有效解决了通信和内存瓶颈，为大规模数据下的统计推断提供了可行的高性能计算方案。

Related Work: 相关工作包括经典的Bootstrap方法、分布式统计计算以及基于MPI的并行算法设计。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [25] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: 提出了一种名为MeCeFO的内存和计算高效的容错优化算法，用于大规模语言模型训练中的节点故障恢复，具有低开销和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着分布式优化规模扩大，硬件故障在大模型训练中变得不可忽视，现有容错方法通常带来显著的计算或内存开销。

Challenges: 如何在不增加显著计算和内存负担的前提下，实现对训练节点故障的快速恢复和持续稳定训练。

Contributions: 提出了MeCeFO算法，包含跳连连接、重计算和低秩梯度近似三项关键技术，在保持收敛速度的同时大幅降低容错开销。

Results: 理论证明MeCeFO保持O(1/√(nT))的收敛速率；实验显示在高故障率下仅吞吐量下降4.18%，容错能力比现有SOTA方法提升5.0×至6.7×。

Conclusion: MeCeFO在不牺牲训练效率的前提下，显著提升了分布式大模型训练的容错能力，适合大规模部署应用。

Related Work: 现有工作多依赖冗余备份或完整重算实现容错，导致资源消耗大；部分近似方法未充分考虑内存与计算的联合优化。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [26] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: 本文提出了一种名为FourierCompress的新型层感知激活压缩框架，利用大语言模型（LLM）激活在频域中的稀疏性，通过快速傅里叶变换（FFT）实现高效、近无损的协同推理压缩，显著降低了边缘设备上LLM推理的通信开销，并在压缩比、重构精度和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 协同大语言模型推理在边缘设备上实现低延迟、隐私保护的AI服务具有重要意义，但其面临的主要瓶颈是客户端与边缘服务器之间传输高维中间激活带来的通信开销，尤其是在自回归解码过程中，通信量随输出长度线性增长。现有压缩方法难以同时兼顾高压缩比、低重构误差和高计算效率，因此亟需一种更优的激活压缩方案。

Challenges: 主要挑战包括：1）如何在不显著影响模型推理性能的前提下大幅压缩高维中间激活；2）如何应对自回归生成过程中激活传输的线性通信开销增长；3）现有压缩方法（如Top-k、QR、SVD）在压缩率、重构误差和计算延迟之间难以平衡；4）压缩方法需支持硬件加速以适应资源受限的边缘设备。

Contributions: 1）提出FourierCompress，一种基于频域稀疏性的层感知激活压缩框架；2）理论证明第一Transformer层的激活具有低频能量集中特性，适合FFT进行近无损压缩；3）设计利用共轭对称性的信号重建方法，支持在DSP和FPGA等硬件上高效加速；4）在Llama 3和Qwen2.5模型上进行广泛实验，验证了方法在压缩效率、精度保持和速度方面的优越性。

Results: 实验结果表明：1）FourierCompress平均减少7.6倍激活数据大小；2）在10个常识推理数据集上，平均准确率损失小于0.3%，性能接近未压缩基线；3）相比Top-k等方法，在保持更高精度的同时，压缩时间减少超过32倍；4）在Llama 3和Qwen2.5模型上均表现出优越的压缩-精度-速度权衡。

Conclusion: FourierCompress通过利用LLM激活在频域的低频集中特性，实现了通信高效、近无损且快速的协同推理压缩，有效解决了边缘设备上LLM推理的通信瓶颈问题。该方法在压缩比、重构精度和硬件友好性方面均优于现有技术，为资源受限场景下的大模型部署提供了可行路径。

Related Work: 相关工作主要包括：1）大语言模型的协同推理架构；2）激活压缩技术，如Top-k稀疏化、QR分解、SVD低秩近似等；3）基于变换域的压缩方法，如DCT、小波变换等；4）边缘计算中的低延迟模型推理优化技术。本文方法区别于传统空间域压缩，首次系统性利用FFT在频域进行激活压缩，并结合层特性设计压缩策略。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [27] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一种面向机器学习工作负载的领域专用RDMA传输协议，通过移除传统RDMA中的重传和有序交付机制，利用ML对数据丢失的容忍性，显著降低尾部延迟、减少硬件资源占用并提升容错能力。


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习扩展到数千个GPU，传统RDMA协议因严格的可靠性和有序传输要求导致尾延迟成为瓶颈，影响系统性能。

Challenges: 如何在保证机器学习训练正确性的同时，降低RDMA传输中的尾延迟和硬件开销，并提升系统在高并发和故障情况下的可扩展性与鲁棒性。

Contributions: 提出Celeris，一种新的RDMA传输架构，首次将重传和有序交付从NIC卸载，采用尽力而为传输，结合软件层机制与ML管道内的恢复技术（如Hadamard变换），实现低延迟、高容错的通信。

Results: 实验表明，Celeris将99百分位延迟降低最多2.3倍，BRAM使用减少67%，NIC对故障的恢复能力接近翻倍。

Conclusion: Celeris通过重新定义RDMA的可靠性模型，为大规模机器学习集群提供了更高效、更具弹性的通信传输方案。

Related Work: RoCE、IRN和SRNIC等传统RDMA设计依赖重传和包排序来确保可靠性，适用于通用场景但不适应ML工作负载的容错特性。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [28] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 本文提出了一种基于C++ Noarr库扩展的MPI新抽象，遵循Noarr的首类布局和遍历抽象范式，实现了MPI应用的布局无关设计，并通过分布式GEMM核的案例研究验证了其可用性和性能，性能与当前最先进的MPI C++绑定相当。


<details>
  <summary>Details</summary>
Motivation: MPI作为分布式高性能计算的成熟技术，其纯C接口缺乏现代语言（如C++）的类型检查和泛型设计等特性，限制了灵活性和易用性。

Challenges: 如何在保持MPI高性能的同时，引入现代C++特性实现类型安全和布局无关的抽象设计。

Contributions: 1. 提出并实现了一种基于Noarr库的MPI新抽象；2. 实现了布局无关的分布式GEMM核作为案例；3. 验证了该抽象在保持高性能的同时提升了设计灵活性。

Results: 所提出的抽象在性能上与当前最先进的MPI C++绑定相当，同时支持更灵活的分布式应用设计。

Conclusion: 该工作成功将现代C++特性引入MPI，实现了类型安全和布局无关的编程模型，在不牺牲性能的前提下提升了开发效率和代码可维护性。

Related Work: 相关工作包括MPI的C++绑定、Noarr库在数据布局和遍历抽象方面的研究，以及高性能计算中类型安全通信接口的探索。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [29] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 本文提出了一种用于互连多核系统的集成容错架构，通过构建稳定性指标和周期性诊断，实现永久性故障隔离与自适应任务调度，无需额外硬件，相比传统TMR降低约30%任务负载，显著提升可靠性和能效。


<details>
  <summary>Details</summary>
Motivation: 传统双相三重模块冗余（TMR）在无故障时虽可节能，但无法应对永久性故障，限制了关键系统的可靠性；现有改进方案如R-TMR依赖额外硬件，增加复杂性且在多模块失效时容错能力下降。

Challenges: 如何在不增加硬件开销的前提下，同时有效应对瞬时和永久性故障，并实现高精度的故障隔离与系统的高可靠性。

Contributions: 提出一种无需额外硬件的集成容错架构，引入稳定性指标和周期性诊断机制，实现永久性故障的自动隔离与自适应任务调度，提升了系统在多核环境下的可靠性和能效。

Results: 实验结果表明，与基线TMR相比，该方法将任务负载降低了约30%，并实现了更高的故障覆盖率和隔离精度。

Conclusion: 所提架构在不增加硬件复杂性的前提下，有效提升了多核系统的容错能力、可靠性和能源效率，适用于对可靠性要求较高的关键系统。

Related Work: 相关工作包括双相TMR和R-TMR，前者在永久性故障下失效，后者虽能处理永久性故障但依赖额外硬件且在多点故障时容错能力受限。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [30] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 该论文探讨了最新的推理模型在生成优化CUDA代码方面的能力，评估了大语言模型（LLM）在无需人工干预下进行代码优化和并行模式生成的表现，并研究了通过提示中的详细指导（“辅导”）是否能提升其性能。结果表明，尽管LLM本身具备较强的编码能力，但在达到并行计算专家水平的优化效果时仍需辅导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在编程工具中的广泛应用，探索其在高性能计算领域（特别是CUDA代码优化）中的潜力成为重要课题。作者希望明确当前LLM在自动生成高效并行代码方面的能力边界，并探索提升其表现的方法。

Challenges: 主要挑战包括：LLM是否能自主掌握复杂的CUDA优化技术（如内存共存、线程调度、并行模式选择）；如何设计有效的提示（prompt）来引导模型生成更优代码；以及如何评估生成代码在性能和正确性上的表现。

Contributions: 1）系统评估了最新推理型LLM在生成优化CUDA代码方面的性能；2）提出并验证了‘辅导式提示’对提升代码质量的有效性；3）引入交互式会话机制，允许模型在会话中修正错误；4）结合自动评估（正确性和加速比）与人工代码审查，提供多维度分析。

Results: 实验结果表明：LLM能够生成基本正确的CUDA代码，但在未经辅导时往往无法达到专家级优化水平；通过提供详细优化提示（辅导），生成代码的性能显著提升；交互式修正机制有助于改进错误；整体上，辅导显著缩小了LLM与专家代码之间的性能差距。

Conclusion: 尽管当前大语言模型已具备较强的CUDA编码能力，但要生成高度优化的并行代码仍需外部指导（如详细提示）。辅导式提示和交互式修正能有效提升代码质量，未来的研究可进一步探索自动化提示优化和闭环调试机制。

Related Work: 相关工作包括利用LLM进行代码生成（如GitHub Copilot）、在HPC领域中的代码辅助研究、以及通过提示工程提升模型在特定任务上的表现。本文在此基础上聚焦于CUDA代码的性能优化，并引入辅导与交互机制进行深入评估。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [31] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 提出了一种基于eBPF的遥测系统，用于统一监控GPU工作负载，实现主机指标与GPU内部事件的关联，具备高诊断准确率、快速检测和根因分析能力，且CPU开销低，适用于分布式学习负载和多租户GPU基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏细粒度的GPU尾延迟尖峰根因分析能力，难以保障云和高性能计算基础设施中的性能可预测性和资源利用率。

Challenges: 在多租户环境中实现低开销、高精度的GPU尾延迟监控与根因定位，同时关联主机侧指标与GPU内部事件存在技术挑战。

Contributions: 设计并实现了一个基于eBPF的统一主机侧GPU工作负载监控系统，支持跨层关联分析，能够在低CPU开销下实现快速、准确的尾延迟尖峰检测与根因分析。

Results: 系统在100Hz采样下仅引入1.21%的CPU开销，可在5秒内检测到延迟尖峰，在6-8秒内完成根因分析，诊断准确率达到81%-88%，并在分布式学习工作负载上成功识别出NIC争用、PCIe压力和CPU干扰等根本原因。

Conclusion: 该eBPF-based系统为多租户GPU基础设施提供了高效、无需全集群插桩的运维调试能力，显著提升了系统可观测性与故障排查效率。

Related Work: 相关工作包括GPU性能监控工具（如NVIDIA Nsight、DCGM）和基于eBPF的系统可观测性技术，但现有方法缺乏对主机与GPU跨层事件关联的细粒度分析支持。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [32] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 本文提出了一种训练语言模型的方法，使其在推理过程中能够与性能工具交互，从而提升代码性能优化任务的效果，特别是在GPU内核优化方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在软件工程任务中表现出色，但在涉及代码性能优化等依赖环境和硬件信息的任务上仍表现不佳，因此需要让模型能够理解环境与代码性能之间的关系。

Challenges: 代码性能优化依赖于源代码之外的复杂环境和硬件数据，传统语言模型难以获取和利用这些信息，限制了其在性能相关任务中的表现。

Contributions: 提出了一种新的训练方法，使语言模型在推理过程中可以与性能分析工具交互，从而获取执行反馈并优化决策；并将该方法应用于GPU内核优化，实现了先进的性能提升。

Results: 基于该方法训练的模型在GPU内核优化任务中表现出色，显著优于传统方法，验证了模型通过与性能工具交互来提升优化能力的有效性。

Conclusion: 通过让语言模型在推理过程中与性能工具交互，能够有效提升其在复杂性能优化任务中的表现，为语言模型在系统级优化中的应用提供了新方向。

Related Work: 已有研究利用链式思维（chain-of-thought）提升代码建模能力，但在结合外部工具进行性能优化方面仍属空白；本文工作填补了语言模型与性能分析工具协同推理的空白。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [33] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文研究了分布式图算法中的轮次消除固定点是否是证明下界问题的通用技术。作者通过引入三值输入的新技术，解决了之前已知的障碍——同态问题，并证明了这些问题是轮次消除固定点。然而，作者也发现了一类新的障碍：某些带输入的问题需要Ω(log n)轮，但无法通过非平凡轮次消除固定点的松弛来证明下界，表明轮次消除在带输入问题中不是通用的。此外，作者提出了首个适用于任意问题（无论是否有输入）的一般性下界定理。


<details>
  <summary>Details</summary>
Motivation: 研究轮次消除固定点是否能作为证明分布式计算中局部可验证问题下界的通用方法，特别是针对需要至少Ω(log n)轮的问题。解决当前理论中存在的障碍，推动分布式计算复杂性理论的可判定性发展。

Challenges: 主要挑战包括：1）如何处理已知依赖于Marks技术证明的同态问题；2）是否存在不依赖于非平凡轮次消除固定点的Ω(log n)下界问题；3）扩展轮次消除技术以适用于更广泛的问题类别，尤其是带有输入的问题。

Contributions: 1）提出基于三值输入（tripotent inputs）的新技术，系统化构建轮次消除下界；2）证明同态问题存在基于轮次消除固定点的下界证明，消除了此前的主要障碍；3）发现并证明一类带输入问题无法通过非平凡固定点松弛获得下界，揭示了轮次消除的非通用性；4）建立首个适用于任意轮次消除固定点问题的通用下界定理。

Results: 1）同态问题可以通过新提出的三值输入方法用轮次消除技术证明其Ω(log n)下界；2）存在一类带输入问题，尽管需要Ω(log n)轮，但不存在基于非平凡轮次消除固定点的证明；3）提出并证明了一个适用于所有类型问题（含输入或不含输入）的通用下界定理，超越了以往仅适用于受限输入的成果。

Conclusion: 轮次消除固定点技术不能作为所有带输入问题的通用下界证明工具，因此在一般情形下不具备完全的普适性。然而，对于无输入问题，其可能仍是通用的。尽管如此，本文通过新方法扫清了此前主要障碍，并建立了更强大的通用下界框架，推动了该领域理论的发展。

Related Work: 本文与STOC 2016、PODC 2019中的轮次消除技术，STOC 2022、ITCS 2022、PODC 2020中关于分布式图算法与下界的研究密切相关。特别是ITCS 2022中提出的同态问题作为关键障碍，以及Marks在J.AMS 2016中提出的不可计算性技术，构成了本文工作的主要背景和出发点。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>
