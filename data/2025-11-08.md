<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了五分鐘規則，整合了主機成本、DRAM帶寬/容量和SSD性能與成本的物理模型，提出了一個考慮限制和工作負載的框架，並展示了現代AI平台中DRAM到閃存緩存閾值的顯著降低。


<details>
  <summary>Details</summary>
Motivation: 由於現代AI平台的發展，特別是GPU主機與高IOPS SSD的結合，傳統的五分鐘規則已不再適用，需要一個更全面的分析框架。

Challenges: 如何整合主機成本、DRAM容量與帶寬限制，以及SSD的性能與成本模型，並在考慮實際限制和工作負載的情況下提供可行的配置建議。

Contributions: 提出了考慮限制和工作負載的分析框架，引入了MQSim-Next模擬器，並展示了兩個軟體系統設計的案例研究。

Results: 現代AI平台中的DRAM到閃存緩存閾值從幾分鐘降低到幾秒，NAND閃存記憶體成為活躍的數據層。

Conclusion: 本文將經典的五分鐘規則轉變為可行的分析和配置框架，為AI時代的記憶體層次結構研究奠定了基礎。

Related Work: Jim Gray和Gianfranco Putzolu於1987年提出的五分鐘規則及其後續研究。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [2] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: 本文提出了一种基于3D堆叠芯片的大型语言模型推理加速器，采用非易失性存内计算处理单元和硅光互连技术，有效缓解通信瓶颈，并通过映射策略和CCPG方案显著提升性能与能效。


<details>
  <summary>Details</summary>
Motivation: 为解决大型语言模型推理中由于传统架构通信瓶颈导致的性能和能效限制，本文旨在设计一种高效、可扩展的加速器架构。

Challenges: 主要挑战包括芯片间通信延迟高、功耗大，以及在3D堆叠芯片架构下如何高效映射和调度大规模语言模型的计算任务。

Contributions: 1) 提出基于3D堆叠芯片和硅光互连的LLM推理加速器架构；2) 设计非易失性存内计算处理单元与IPCN；3) 开发优化的LLM映射方案；4) 引入芯片集群与电源门控（CCPG）机制以提升能效和可扩展性。

Results: 仿真结果显示，相比Nvidia A100，在未启用CCPG时实现3.95倍加速和30倍能效提升；启用CCPG后，相比H100在相似吞吐下实现57倍能效提升。

Conclusion: 该架构通过创新的硬件设计和调度优化，显著提升了LLM推理的能效和可扩展性，为未来高性能AI加速器提供了可行方案。

Related Work: 相关工作包括基于存内计算的AI加速器、3D堆叠芯片设计、光互连技术在芯片间的应用，以及针对LLM的硬件调度与映射方法。

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [3] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 本文综述了硬件解聚的动机和最新进展，讨论了相关研究挑战与机遇，并指出其有望重塑数据中心生态系统。


<details>
  <summary>Details</summary>
Motivation: 推动数据中心资源从传统服务器集群转变为统一资源池，提升资源利用率和灵活性。

Challenges: 包括技术实现难度、资源调度复杂性、硬件配置动态管理、冷却与供电系统优化等问题。

Contributions: 概述了硬件解聚的最新进展，识别了研究中较少关注的关键问题，并通过数值研究展示了主要挑战。

Results: 研究表明硬件解聚在多个方面具有显著潜力，但需克服系统设计与跨层优化的难题。

Conclusion: 硬件解聚有望深刻影响数据中心的架构与运营，未来研究应聚焦跨领域协同优化。

Related Work: 工业界和学术界在硬件解聚架构、资源池化和调度策略方面已有较多探索。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [4] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: 本文提出了一种软硬件协同设计的架构级IR-drop缓解方案AIM，用于高性能SRAM存内计算（PIM），通过工作负载调控与动态V-f调节，在7nm工艺下实现最高69.2%的IR-drop缓解，能效提升2.29倍，速度提升1.152倍。


<details>
  <summary>Details</summary>
Motivation: 高性能SRAM存内计算（PIM）因追求高计算密度和能效，导致电路复杂度和工作频率上升，引发严重的IR-drop问题，影响芯片性能与可靠性，传统电路级缓解方法代价高昂且影响PPA（功耗、性能、面积）。

Challenges: 如何在不牺牲功率、性能和面积（PPA）的前提下，有效缓解高频率PIM架构中的IR-drop问题；如何建立PIM工作负载与IR-drop之间的关联，并实现动态调节与软硬件协同优化。

Contributions: 1. 提出Rtog和HR，建立PIM工作负载与IR-drop的直接关联；2. 设计LHR和WDS，实现架构级IR-drop缓解的同时保持计算精度；3. 开发IR-Booster，结合软件HR信息与硬件监测，动态调整V-f对；4. 提出HR感知的任务映射方法，实现软硬件协同优化。

Results: 在7nm 256-TOPS PIM芯片上的后布局仿真结果显示，AIM实现了最高69.2%的IR-drop缓解，能效提升2.29倍，性能提升1.152倍。

Conclusion: AIM通过软硬件协同设计，有效缓解了高性能PIM中的IR-drop问题，在不牺牲PPA的前提下显著提升了能效与性能，为未来高密度PIM架构提供了可行的优化路径。

Related Work: 相关工作主要集中在电路级IR-drop缓解技术，如后端优化、电源网格设计等，但这些方法通常资源消耗大且难以适应动态工作负载，缺乏从架构与软件协同角度进行系统性优化的研究。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: 本文提出了OMPILOT，一种专门用于将C++代码转换为OpenMP的领域特定编码器-解码器Transformer模型，通过函数级语义建模和定制预训练目标提升代码翻译的准确性与鲁棒性，并提出OMPBLEU指标评估并行结构的正确性与质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码翻译中取得了进展，但现有方法多局限于循环级并行化，缺乏对函数级语义上下文的充分建模，且缺乏针对OpenMP等并行构造的专用评估指标。

Challenges: 如何有效建模OpenMP并行语义；在缺乏大规模标注数据的情况下提升翻译鲁棒性；设计能准确评估并行代码质量的指标。

Contributions: 1) 提出OMPILOT，首个专用于C++到OpenMP翻译的领域特定模型，支持函数级并行化；2) 设计结合无监督与有监督学习的定制预训练目标；3) 提出OMPBLEU，一种评估OpenMP代码翻译质量的新复合指标。

Results: 实验表明，OMPILOT在功能正确性和并行结构生成质量上优于现有基线模型，OMPBLEU指标与人工评估具有更高相关性，验证了方法的有效性。

Conclusion: OMPILOT通过领域定制化建模和新型评估指标，在C++到OpenMP的代码翻译任务中实现了更准确、更鲁棒的函数级并行化支持，推动了自动化并行编程的发展。

Related Work: 先前研究主要基于规则或通用LLM进行代码翻译，部分工作聚焦于循环级OpenMP指令插入，缺乏对整体函数上下文的理解和系统性评估方法。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [6] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 本文提出了一种基于马尔可夫链的随机建模方法，用于分析边缘计算中的电源状态转换，验证了AI驱动的预测性电源调节相比传统反应式方法在节能和系统响应性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 由于边缘设备分布广泛且能源资源有限，如何在保证低延迟的同时有效管理功耗成为边缘计算中的关键挑战。

Challenges: 边缘计算中电源管理的主要挑战包括设备分布性、能源资源有限、工作负载动态变化以及多节点间能耗不均。

Contributions: 1. 提出基于马尔可夫链的随机模型分析电源状态转换；2. 引入AI驱动的预测性电源调节策略；3. 通过蒙特卡洛仿真验证模型有效性；4. 进行敏感性分析以优化能效。

Results: 实验结果表明，AI驱动的电源管理策略能有效减少不必要的状态转换，降低设备间能耗差异，提升整体能效和系统响应速度，且理论模型与仿真结果高度一致。

Conclusion: AI-based power management通过预测工作负载需求和优化状态转换，显著提升了边缘计算系统的能源效率和自适应协调能力。

Related Work: 相关工作包括基于阈值的动态电压频率调节（DVFS）、反应式电源管理策略以及马尔可夫模型在系统行为预测中的应用。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [7] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 本文提出了一种新的并行生成策略，用于解决高性能计算中MPI应用的动态资源管理问题，显著降低了扩展和收缩操作的开销，提升了系统适应性和整体性能。


<details>
  <summary>Details</summary>
Motivation: 动态资源管理在高性能计算中愈发重要，能够减少作业等待时间、降低任务完成时间并提高系统利用率。然而，现有方法在扩展和收缩过程中存在重配置开销大、无法完全释放多余资源等问题，因此需要更高效的支持应用弹性（malleability）的方法。

Challenges: 主要挑战包括：1）在资源扩展时重新启动整个应用开销大；2）在资源收缩时无法完全释放不再需要的进程，因部分跨节点的通信仍保持活跃；3）需在异构和共享资源环境下保持高效性。

Contributions: 1）提出一种新的并行生成策略，所有进程在资源重分配前协同参与生成过程，减少执行时间；2）克服了现有方法在收缩时的资源释放限制，实现更彻底的节点回收；3）在保持扩展操作低开销（最多1.25倍开销）的同时，将收缩操作成本降低至少20倍；4）该策略在同构和异构系统以及共享资源环境中均经过验证。

Results: 实验结果表明，该方法在扩展操作中保持了最多1.25倍的开销，同时将收缩操作的成本降低了至少20倍，显著提升了资源适应性和系统效率，并在多种系统环境下验证了其有效性。

Conclusion: 所提出的并行生成策略有效解决了MPI应用在动态资源管理中的扩展与收缩难题，显著降低了重配置成本，提升了资源利用率和系统性能，具有广泛的应用前景。

Related Work: 已有工作主要分为两类：一类是通过重新启动整个应用来实现资源调整，开销较大；另一类尝试重用原有进程，但在收缩时无法完全释放节点，受限于跨节点通信的存在。本文方法在两者基础上实现了更高效的资源重配置。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [8] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 提出了一种在低比特量化下实现动态稀疏推理的技术，通过zigzag量化布局、专用GEMV内核和紧凑运行时机制，在保持精度的同时显著提升了解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在终端设备上部署大语言模型因资源受限面临挑战，尤其是内存和计算能力有限，而现有的量化方法与动态稀疏激活不兼容。

Challenges: 动态稀疏性与分组量化不兼容，导致难以在资源受限设备上高效执行大模型；同时需在保持精度的同时提升计算效率和内存局部性。

Contributions: 提出了zigzag-patterned量化布局、针对该布局优化的GEMV内核，以及低开销的稀疏索引收集机制，实现了量化与动态稀疏性的高效协同。

Results: 在多种模型规模和硬件配置下，解码吞吐量最高提升1.55倍，且精度与密集量化推理相当。

Conclusion: 结构化稀疏性与低比特量化可在通用GPU上有效共存，为终端设备上的高效大模型推理提供了可行方案。

Related Work: 相关工作包括大模型量化、动态稀疏激活、GEMV优化及稀疏张量计算，但大多未解决稀疏性与量化布局之间的冲突。

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [9] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 本文提出了一种基于MAPE-K架构的自保护分布式系统，并引入了一种新的概率性移动拜占庭故障（MBF）模型，用于捕捉动态演变攻击的行为，通过理论分析和仿真验证了系统在不同感染与自恢复速率下的安全状态演化。


<details>
  <summary>Details</summary>
Motivation: 由于现代分布式系统面临日益增长的安全威胁，传统的拜占庭容错模型难以准确反映现实中的动态攻击行为，因此需要一种更贴近实际、具备自适应防护能力的系统模型。

Challenges: 主要挑战在于如何准确建模动态变化的攻击过程，平衡拜占庭故障传播速率与系统自恢复能力之间的关系，并确保模型能够在真实系统中有效集成并驱动自保护策略。

Contributions: 提出了一个基于MAPE-K架构的自保护分布式系统框架；设计了一种新的概率性移动拜占庭故障（MBF）模型，可嵌入到分析组件中；进行了数学分析以评估系统进入不安全状态或恢复至安全状态所需的时间；并通过仿真展示了系统在不同参数下的行为特征。

Results: 数学分析结果表明，系统的安全性强烈依赖于拜占庭感染传播速率与自恢复速率之间的相对大小；仿真结果显示，当自恢复速率高于感染传播速率时，系统能够较快恢复到安全状态，反之则可能迅速恶化。

Conclusion: 所提出的概率性MBF模型能有效刻画动态攻击的演化过程，结合MAPE-K架构可实现对分布式系统的动态防护，为构建具备自我保护能力的系统提供了理论基础和实践路径。

Related Work: 相关工作主要包括经典的拜占庭容错机制、自适应安全防护框架（如自愈系统）、以及基于控制论的自主计算模型（如MAPE-K环），本文在这些基础上引入了时间动态性和概率特性以增强模型的现实适用性。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [10] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 本文提出了动态并发的概念，即根据当前系统状态动态检测冲突，并仅在必要时使用强同步原语，从而提升分布式计算的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算中可扩展性的主要障碍，传统方法在处理并发操作时容易产生不必要的冲突开销，因此需要一种能根据系统状态动态调整的冲突检测机制。

Challenges: 如何准确判断并发操作在特定系统状态下是否真正发生冲突，并在不牺牲正确性的前提下减少同步开销。

Contributions: 定义了“动态并发”的概念，并提出了一种支持动态并发的通用构造方法，能够根据运行时状态决定是否进行强同步。

Results: 实现了一个动态并发的通用构造，能够在实际运行中减少不必要的同步操作，提高系统并发性能。

Conclusion: 动态并发能有效降低分布式系统中的同步开销，提升可扩展性，为构建高效并发算法提供了新思路。

Related Work: 以往的研究多采用静态冲突判断或基于锁的同步机制，缺乏对运行时状态的感知能力；本文的方法与乐观并发控制和软件事务内存有一定关联，但更强调状态依赖的动态仲裁。

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>
