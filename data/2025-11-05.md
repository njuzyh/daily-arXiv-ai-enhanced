<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NI](#cs.NI) [Total: 5]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention](https://arxiv.org/abs/2511.00295)
*Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 本文提出了一种名为H-FA的硬件优化方法，通过结合浮点和对数域定点计算来加速FlashAttention的硬件实现，在28nm工艺下实现了面积和功耗的显著降低，同时保持性能不受影响。


<details>
  <summary>Details</summary>
Motivation: 由于传统FlashAttention在长序列上的计算存在性能瓶颈，且纯浮点硬件实现开销大，因此需要一种更高效的硬件加速方案。

Challenges: 如何在不损失精度和性能的前提下，降低FlashAttention硬件实现的面积和功耗是主要挑战。此外，融合softmax和矩阵乘法操作对硬件设计提出了高要求。

Contributions: 提出了H-FA方法，首次将对数域定点计算引入FlashAttention硬件实现，用加减法替代乘除法，并省略指数运算，有效降低了硬件开销。

Results: 在28nm工艺下，H-FA相比纯浮点架构平均减少26.5%的面积和23.4%的功耗，且未影响性能。

Conclusion: H-FA通过混合浮点与对数域定点计算，显著提升了FlashAttention的硬件效率，为Transformer模型的高效硬件加速提供了新方向。

Related Work: 相关工作包括FlashAttention的GPU优化、注意力机制的硬件加速研究，以及对数域计算在深度学习中的应用。

Abstract: Transformers have significantly advanced AI and machine learning through
their powerful attention mechanism. However, computing attention on long
sequences can become a computational bottleneck. FlashAttention mitigates this
by fusing the softmax and matrix operations into a tiled computation pattern
that decouples performance from sequence length. Though designed for GPUs, its
simplicity also makes it well suited for direct hardware acceleration. To
improve hardware implementation, we compute FlashAttention using a mixture of
floating-point and fixed-point logarithm domain representations. Floating-point
is used to compute attention scores from query and key matrices, while
logarithmic computation simplifies the fused computation of softmax
normalization and the multiplication with the value matrix. This
transformation, called H-FA, replaces vector-wide floating-point multiplication
and division operations by additions and subtractions implemented efficiently
with fixed-point arithmetic in the logarithm domain. Exponential function
evaluations are effectively omitted and fused with the rest operations, and the
final result is directly returned to floating-point arithmetic without any
additional hardware overhead. Hardware implementation results at 28nm
demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction
in power, on average, compared to FlashAttention parallel hardware
architectures built solely with floating-point datapaths, without hindering
performance.

</details>


### [2] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 本文提出了一种基于CXL的近内存处理（PNM）KV缓存管理方案，用于支持百万级Token上下文的大型语言模型推理，通过将页面选择卸载至CXL内存中的PNM加速器，显著提升了吞吐量、能效和成本效益。


<details>
  <summary>Details</summary>
Motivation: 随着大模型上下文窗口扩展至百万级Token，KV缓存的增长导致GPU内存与计算资源面临严重瓶颈，现有CXL方案仍因频繁调入非驻留KV数据而产生高昂传输开销。

Challenges: 如何在长上下文场景下高效管理不断增长的KV缓存；如何减少GPU与外部存储间的数据迁移开销；如何在CXL架构下实现计算与内存的协同扩展。

Contributions: 提出了将Token页面选择卸载到CXL内存中PNM加速器的新架构；设计了PNM-only（PNM-KV）和GPU-PNM混合（PnG-KV）两种方案；引入混合并行策略与稳态Token选择机制以提升计算效率。

Results: 在高达405B参数、1M-Token上下文的LLM上实现了最高21.9倍的吞吐提升，单位Token能耗降低60倍，总成本效率提升7.3倍。

Conclusion: CXL支持的多PNM架构可作为未来长上下文大模型推理的可扩展基础，有效突破GPU内存与计算限制。

Related Work: 基于CXL的非逐出式KV缓存管理框架；近内存计算在AI加速中的应用；大模型推理中的KV缓存压缩与分页技术。

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.00294)
*Lucas Almeida,Maycon Peixoto*

Main category: cs.DC

TL;DR: Tetris是一种面向边缘-云连续体的应用放置策略，通过启发式算法高效分配计算服务，优先考虑SLA紧急程度和资源效率，显著减少SLA违规，提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 在边缘-云连续体中，如何有效分配应用模块以兼顾低延迟和高计算能力是一个关键挑战，需要在满足用户需求的同时优化资源利用。

Challenges: 应用放置需平衡边缘的低延迟优势与云的高算力特性，同时避免系统过载，满足SLA要求并提升资源效率。

Contributions: 提出Tetris，一种基于启发式算法的应用放置策略，能够根据SLA紧急程度和资源效率进行服务优先级排序和资源分配。

Results: 实验结果表明，与基线方法相比，Tetris将SLA违规减少了约76%，显著提升了服务质量（QoS）。

Conclusion: Tetris为边缘-云连续体环境下的延迟敏感型应用提供了一种高效的放置方案，有效增强了系统的服务质量。

Related Work: 相关工作主要集中在边缘计算中的任务卸载、资源调度和云边协同管理，但较少综合考虑SLA紧急程度与资源效率的动态权衡。

Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a
flexible and scalable infrastructure. This paradigm can minimize latency by
processing data closer to the source at the edge while leveraging the vast
computational power of the cloud for more intensive tasks. In this context,
module application placement requires strategic allocation plans that align
user demands with infrastructure constraints, aiming for efficient resource
use. Therefore, we propose Tetris, an application placement strategy that
utilizes a heuristic algorithm to distribute computational services across edge
and cloud resources efficiently. Tetris prioritizes services based on SLA
urgencies and resource efficiency to avoid system overloading. Our results
demonstrate that Tetris reduces SLA violations by approximately 76% compared to
the baseline method, which serves as a reference point for benchmarking
performance in this scenario. Therefore, Tetris offers an effective placement
approach for managing latency-sensitive applications in Edge-Cloud Continuum
environments, enhancing Quality of Service (QoS) for users.

</details>


### [4] [EPARA: Parallelizing Categorized AI Inference in Edge Clouds](https://arxiv.org/abs/2511.00603)
*Yubo Wang,Yubo Cui,Tuo Shi,Danyang Li,Wenxin Li,Lide Suo,Tao Wang,Xin Xie*

Main category: cs.DC

TL;DR: EPARA是一种面向边缘计算的端到端AI并行推理框架，通过任务分类和资源感知调度，提升边缘AI服务的吞吐量，实验表明其在生产负载下相较现有框架最高可提升2.1倍goodput。


<details>
  <summary>Details</summary>
Motivation: 随着大模型和计算机视觉等AI应用的普及，边缘端的推理计算需求不断上升，如何利用现有硬件提升任务处理能力成为边缘云的关键挑战。

Challenges: 边缘环境中资源受限，不同AI任务对延迟、频率和GPU资源的需求差异大，难以统一高效调度；同时需兼顾请求级和服务级的资源分配。

Contributions: 提出了EPARA框架，包含任务分类并行分配器、分布式请求处理器和状态感知调度器三个核心组件，实现了基于任务特性的细粒度资源分配与动态服务部署。

Results: 在包含边缘服务器、嵌入式设备和微计算机的测试平台上，EPARA在LLM和分割任务的案例研究中，相比先前框架最高实现了2.1倍的goodput提升，并展现出对多种边缘AI推理任务的良好适应性。

Conclusion: EPARA通过任务感知的并行策略和动态调度机制，有效提升了边缘AI推理系统的处理能力，为边缘环境下高效部署多样化AI应用提供了可行方案。

Related Work: 相关工作包括边缘AI推理优化、模型分割、任务调度框架以及资源感知的微服务部署，但现有方法缺乏对任务敏感度和资源需求的综合分类与协同调度。

Abstract: With the increasing adoption of AI applications such as large language models
and computer vision AI, the computational demands on AI inference systems are
continuously rising, making the enhancement of task processing capacity using
existing hardware a primary objective in edge clouds. We propose EPARA, an
end-to-end AI parallel inference framework in edge, aimed at enhancing the edge
AI serving capability. Our key idea is to categorize tasks based on their
sensitivity to latency/frequency and requirement for GPU resources, thereby
achieving both request-level and service-level task-resource allocation. EPARA
consists of three core components: 1) a task-categorized parallelism allocator
that decides the parallel mode of each task, 2) a distributed request handler
that performs the calculation for the specific request, and 3) a state-aware
scheduler that periodically updates service placement in edge clouds. We
implement a EPARA prototype and conduct a case study on the EPARA operation for
LLMs and segmentation tasks. Evaluation through testbed experiments involving
edge servers, embedded devices, and microcomputers shows that EPARA achieves up
to 2.1$\times$ higher goodput in production workloads compared to prior
frameworks, while adapting to various edge AI inference tasks.

</details>


### [5] [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)
*Ran Yan,Youhe Jiang,Tianyuan Wu,Jiaxuan Gao,Zhiyu Mei,Wei Fu,Haohui Mai,Wei Wang,Yi Wu,Binhang Yuan*

Main category: cs.DC

TL;DR: AReaL-Hex 是一种面向异构GPU的异步强化学习训练系统，通过两阶段调度器优化资源分配，在保持训练稳定性的同时显著提升训练吞吐量并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 为了在异构GPU上高效、低成本地进行大语言模型的强化学习训练，提升训练吞吐量和资源利用率，推动该技术的普及。

Challenges: RL训练包含生成、奖励计算和策略更新三个阶段，各阶段计算强度、内存需求和通信模式差异大；在异构硬件上协调这些阶段并控制数据陈旧性是一大挑战。

Contributions: 提出了AReaL-Hex系统，采用两阶段调度器（MILP约束搜索与图划分）实现异构资源的高效分配；支持将HBM/I-O密集型与计算密集型任务匹配到合适的硬件，平衡生产-消费关系，保证低陈旧性。

Results: 在1.5B至14B模型上实验表明：在相同预算下，吞吐量最高提升1.50倍；在相同吞吐量下，训练成本最高降低1.46倍。

Conclusion: AReaL-Hex有效实现了异构GPU上的高效异步RL训练，显著提升了成本效益和训练效率，适用于大规模语言模型的强化学习场景。

Related Work: 基于全异步RL架构的最新研究，支持阶段解耦与跨硬件池并行，为异构部署提供了基础。

Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is
essential to democratize this advanced technique. One promising but challenging
approach is to deploy such a computational workflow over heterogeneous GPUs.
Unlike conventional large-scale LLM pretraining, RL training generally
decomposes into three coupled stages, i.e., rollout generation, reward
computation, and policy/value updates, which exhibit markedly different compute
intensities, memory footprints, and communication patterns. Recent research
shows that fully asynchronous RL training can disaggregate these stages across
disjoint hardware pools without sacrificing training stability, creating a
great opportunity for real-world heterogeneous deployment. To this end, we
present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that
effectively schedules how to execute rollout generation and policy model
training over heterogeneous GPUs while enforcing data staleness bounds.
Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to
select per-stage parallelization strategies and workload assignments given a
resource budget, and (ii) a graph-partitioning step that allocates
heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built
atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound
generation and compute-bound optimization to more cost-efficient resources and
balances their producer-consumer interactions to avoid both idleness and stale
rollout trajectories. On the mathematical reasoning task with various model
scales (1.5B, 7B, and 14B), compared to homogeneous deployments of
state-of-the-art asynchronous RL systems: (i) When maintaining the same total
budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When
achieving the same training throughput, AReaL-Hex results in up to 1.46x
reduction in training cost.

</details>


### [6] [Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver](https://arxiv.org/abs/2511.01001)
*Johansell Villalobos,Daniel Caviedes-Voullième,Silvio Rizzi,Esteban Meneses*

Main category: cs.DC

TL;DR: 本研究对SERGHEI-SWE求解器在四种前沿异构高性能计算系统上的性能进行了全面评估，展示了其在多GPU环境下的良好扩展性与性能可移植性，同时指出内存带宽是主要瓶颈，未来可通过Kokkos团队和架构特定参数优化进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化带来的复杂多尺度动力学挑战，水文模型对高分辨率、实时模拟的需求日益增长，推动了GPU加速平台和性能可移植编程框架（如Kokkos）的应用。

Challenges: 气候系统的复杂性和多尺度动态给数值建模带来巨大挑战；在异构HPC系统上实现高效、可扩展且可移植的水文模拟仍存在性能瓶颈和优化难题。

Contributions: 1. 在四类先进异构HPC系统上对SERGHEI-SWE求解器进行了全面的性能研究；2. 评估了强扩展性和弱扩展性，验证了其在多达2048个GPU上的可扩展性；3. 通过Roofline分析识别出内存带宽为主要性能瓶颈；4. 使用多种指标评估性能可移植性，并提出优化方向。

Results: SERGHEI-SWE在最多1024个GPU上实现了32倍加速比，大多数测试范围内的扩展效率超过90%；弱扩展性测试扩展至2048个GPU仍保持良好性能；Roofline分析显示关键求解器内核处于内存受限区域；性能可移植性分析表明，在调整问题规模后可在不同设备间实现较好可移植性（<70%差异），但仍有优化空间。

Conclusion: SERGHEI-SWE是一个在当前异构HPC架构下具有高可扩展性、鲁棒性和可移植性的大规模地学物理模拟工具，具备进一步优化潜力，特别是在内存访问和架构特定调优方面。

Related Work: 相关工作包括基于GPU加速的浅水方程求解器开发、使用Kokkos等性能可移植框架的科学计算应用、以及在Frontier、JUWELS Booster等大型异构系统上的性能分析研究。

Abstract: Current climate change has posed a grand challenge in the field of numerical
modeling due to its complex, multiscale dynamics. In hydrological modeling, the
increasing demand for high-resolution, real-time simulations has led to the
adoption of GPU-accelerated platforms and performance portable programming
frameworks such as Kokkos. In this work, we present a comprehensive performance
study of the SERGHEI-SWE solver, a shallow water equations code, across four
state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS
Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We
assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,
demonstrating consistent scalability with a speedup of 32 and an efficiency
upwards of 90\% for most almost all the test range. Roofline analysis reveals
that memory bandwidth is the dominant performance bottleneck, with key solver
kernels residing in the memory-bound region. To evaluate performance
portability, we apply both harmonic and arithmetic mean-based metrics while
varying problem size. Results indicate that while SERGHEI-SWE achieves
portability across devices with tuned problem sizes (<70\%), there is room for
kernel optimization within the solver with more granular control of the
architecture specifically by using Kokkos teams and architecture specific
tunable parameters. These findings position SERGHEI-SWE as a robust, scalable,
and portable simulation tool for large-scale geophysical applications under
evolving HPC architectures with potential to enhance its performance.

</details>


### [7] [Scalable Maxflow Processing for Dynamic Graphs](https://arxiv.org/abs/2511.01235)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DC

TL;DR: 本文提出了一种新的GPU并行最大流算法，能够对动态图在批量边更新后增量地重新计算最大流，同时设计了高效的静态图初始最大流计算算法，并引入了一系列针对CUDA的优化技术以提升GPU平台上的性能、可扩展性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 由于最大流问题在多个领域有广泛应用，且图结构常动态变化，需要高效地处理动态图的最大流问题。现有的GPU并行算法多集中于静态图，缺乏对动态更新的有效支持，因此需要一种能高效处理边批量更新的增量式GPU算法。

Challenges: 主要挑战包括如何在GPU上高效实现增量式最大流计算，如何处理动态图中的批量边更新，以及如何优化内存访问和线程调度以提升并行效率和可扩展性。

Contributions: 本文的主要贡献包括：1）提出一种新的GPU并行增量最大流算法，支持动态图的批量边更新；2）设计高效的静态图最大流GPU算法；3）实现一系列CUDA特定优化，提升性能和内存效率。

Results: 实验结果表明，所提出的算法在多种真实和合成数据集上均显著优于现有方法，具有更高的吞吐量和更好的可扩展性，尤其在处理动态更新时表现出优异的增量计算效率。

Conclusion: 本文展示了通过精心设计的GPU并行策略和CUDA优化，可以高效解决静态和动态图的最大流问题，为大规模图数据的实时流优化提供了可行方案。

Related Work: 相关工作主要包括经典的Push-Relabel算法、其他GPU加速的最大流算法以及动态图上的增量图算法，本文在这些基础上进一步优化了GPU上的增量计算性能。

Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and
combinatorial optimization, aiming to determine the largest possible flow from
a designated source node to a sink node within a capacitated flow network. It
has extensive applications across diverse domains such as computer networking,
transportation systems, and image segmentation. The objective is to maximize
the total throughput while respecting edge capacity constraints and maintaining
flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the
Push--Relabel algorithm is particularly notable for its efficiency and
suitability for parallelization, owing to its localized vertex-based
operations. This property has motivated extensive research into GPU-accelerated
Max-Flow computation, leveraging the high degree of parallelism inherent to
modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of
incrementally recomputing the maximum flow of a dynamic graph following a batch
of edge updates. In addition, we introduce a high-performance static GPU
algorithm designed for efficiently computing the initial Max-Flow on static
graphs. We further describe a series of CUDA-specific implementation
optimizations that enhance performance, scalability, and memory efficiency on
GPU platforms.

</details>


### [8] [Design of quasi phase matching crystal based on differential gray wolf algorithm](https://arxiv.org/abs/2511.01255)
*He Chen,ZiHua Zheng,JingHua Sun*

Main category: cs.DC

TL;DR: 本文提出了一种结合hwsda混合优化算法和GPU并行加速技术的新方案，用于解决非周期极化晶体性能优化中的高维离散组合难题，显著提升了优化效率和晶体性能控制精度。


<details>
  <summary>Details</summary>
Motivation: 为了突破传统算法在非周期极化晶体优化中收敛慢、易陷入局部最优以及CPU串行计算效率低的问题。

Challenges: 晶体性能优化属于高维离散组合的NP难问题，传统算法收敛慢且易陷入局部最优，启发式算法受限于CPU串行计算效率低下。

Contributions: 提出了DE与GWO融合的混合优化算法，并结合GPU并行计算实现高效优化，实现了全局与局部搜索的平衡，显著提高了优化效率和精度。

Results: 相比传统CPU串行计算，准相位匹配设计效率提升了数百至数千倍，有效解决了高维离散空间优化难题。

Conclusion: 该方案为复杂非线性光学器件的设计提供了新范式，有助于推动量子光学和激光加工等领域器件的性能突破和产业化应用。

Related Work: 传统优化算法如遗传算法在晶体设计中的应用受限于计算效率，而本文提出的混合算法结合GPU加速显著超越了这些方法。

Abstract: This paper focuses on the key problem in the development of nonlinear optical
technology, the performance optimization of aperiodically polarized crystals.
The performance of the crystal depends on the precise control of the micro
distribution of crystal domains, but its optimization belongs to the
high-dimensional discrete combination "NP hard" problem. The traditional
algorithm has the bottleneck of slow convergence and easy to fall into local
optimization, while the heuristic methods such as genetic algorithm are limited
by the CPU serial calculation and inefficient. In order to solve the above
challenges, this paper proposes the fusion scheme of hwsda hybrid optimization
algorithm and GPU parallel acceleration technology: the differential evolution
algorithm (DE) is used to realize the global search, and the gray wolf
optimization algorithm (GWO) is used to strengthen the local search and
convergence speed, and the two coordinate to balance the global and local
optimization requirements; At the same time, it relies on GPU multi-core
architecture to realize thread level parallel computing and improve
optimization efficiency. This scheme effectively breaks through the
optimization problem of high-dimensional discrete space, improves the accuracy
of crystal domain control, improves the efficiency of quasi phase matching
design by hundreds to thousands of times compared with traditional CPU serial
computing, provides a new paradigm for the design of complex nonlinear optical
devices, and helps promote the performance breakthrough and industrial
application of related devices in the fields of quantum optics and laser
processing.

</details>


### [9] [Adaptive Multidimensional Quadrature on Multi-GPU Systems](https://arxiv.org/abs/2511.01573)
*Melanie Tonarelli,Simone Riva,Pietro Benedusi,Fabrizio Ferrandi,Rolf Krause*

Main category: cs.DC

TL;DR: 提出了一种基于多GPU架构的分布式自适应数值积分方法，通过层次化域分解和动态负载重分配策略，提高了高维积分的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在多GPU架构上高效求解高维自适应数值积分问题，克服传统方法在高维和不规则被积函数下的效率与可扩展性瓶颈。

Challenges: 自适应过程中子域划分导致严重的负载不平衡；多GPU间通信与计算的协调困难；高维积分中误差估计与收敛效率低下。

Contributions: 提出了一种基于层次化域分解的分布式自适应积分方法；设计了基于循环轮询的去中心化负载重分配机制；实现了非阻塞、CUDA-aware MPI通信以重叠通信与计算。

Results: 相比现有GPU优化积分包，该方法在高维情况下效率更高，对被积函数的正则性和目标精度具有更强的鲁棒性。

Conclusion: 该方法有效解决了多GPU环境下自适应积分的负载不平衡问题，显著提升了高维数值积分的性能和可扩展性。

Related Work: 现有工作主要集中在单GPU或CPU上的自适应积分，缺乏对多GPU间动态负载平衡的有效支持，且在高维场景下性能受限。

Abstract: We introduce a distributed adaptive quadrature method that formulates
multidimensional integration as a hierarchical domain decomposition problem on
multi-GPU architectures. The integration domain is recursively partitioned into
subdomains whose refinement is guided by local error estimators. Each subdomain
evolves independently on a GPU, which exposes a significant load imbalance as
the adaptive process progresses. To address this challenge, we introduce a
decentralised load redistribution schemes based on a cyclic round-robin policy.
This strategy dynamically rebalance subdomains across devices through
non-blocking, CUDA-aware MPI communication that overlaps with computation. The
proposed strategy has two main advantages compared to a state-of-the-art
GPU-tailored package: higher efficiency in high dimensions; and improved
robustness w.r.t the integrand regularity and the target accuracy.

</details>


### [10] [LARK -- Linearizability Algorithms for Replicated Keys in Aerospike](https://arxiv.org/abs/2511.01843)
*Andrew Goodng,Kevin Porter,Thomas Lopatic,Ashish Shinde,Sunil Sayyaparaju,Srinivasan Seshadri,V. Srinivasan*

Main category: cs.DC

TL;DR: LARK是一种同步复制协议，通过引入分区可用性条件（PAC）在保证线性一致性的同时显著降低延迟和基础设施成本，并在独立故障下比传统基于日志的共识协议（如Raft、Paxos）提供更高的可用性。


<details>
  <summary>Details</summary>
Motivation: 传统共识协议（如Raft、Paxos）依赖有序日志和固定副本集，在节点故障时需暂停提交以重建副本，导致可用性降低；LARK旨在消除这些限制，提升系统在故障期间的持续提交能力和恢复效率。

Challenges: 如何在不牺牲安全性的情况下消除有序日志、实现高可用性；如何在仅维护f+1个副本的情况下容忍f个故障并继续提交；如何确保分区在领导者变更后立即就绪。

Contributions: 提出LARK协议，引入基于全局集群状态的分区可用性条件（PAC）；消除有序日志，实现领导者变更后的即时分区就绪；支持在f+1副本配置下持续提交，优于传统协议的停写重建机制；实现零停机滚动重启。

Results: 在容忍1个和2个故障时，LARK的分区可用性分别提升约3倍和10倍；实验和形式化验证表明其在相同存储预算下显著优于Raft等协议，支持故障期间持续提交和快速恢复。

Conclusion: LARK通过创新的PAC机制和无日志设计，在保证安全性和线性一致性的同时，显著提升了复制系统的可用性、降低了延迟和运维成本，尤其适用于高可用和低延迟需求的场景。

Related Work: Raft、Paxos、Viewstamped Replication等基于日志的共识协议，以及传统的多数派（quorum）机制。

Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a
synchronous replication protocol that achieves linearizability while minimizing
latency and infrastructure cost, at significantly higher availability than
traditional quorum-log consensus. LARK introduces Partition Availability
Conditions (PAC) that reason over the entire database cluster rather than fixed
replica sets, improving partition availability under independent failures by
roughly 3x when tolerating one failure and 10x when tolerating two. Unlike
Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,
enabling immediate partition readiness after leader changes -- with at most a
per-key duplicate-resolution round trip when the new leader lacks the latest
copy. Under equal storage budgets -- where both systems maintain only f+1 data
copies to tolerate f failures -- LARK continues committing through data-node
failures while log-based protocols must pause commits for replica rebuilding.
These properties also enable zero-downtime rolling restarts even when
maintaining only two copies. We provide formal safety arguments and a TLA+
specification, and we demonstrate through analysis and experiments that LARK
achieves significant availability gains.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Mist-Assisted Federated Learning for Intrusion Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2511.00271)
*Saadat Izadi,Shakib Komasi,Ali Salimi,Alireza Rezaei,Mahmood Ahmadi*

Main category: cs.NI

TL;DR: 提出了一种雾辅助的分层联邦学习框架，用于物联网入侵检测，能够在数据异构和非独立同分布条件下实现高效、隐私保护的高精度检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备的快速增长扩大了攻击面，且设备分布广泛、资源受限，传统集中式入侵检测方法难以应对数据异构和隐私保护问题，需要一种分布式、隐私安全的检测机制。

Challenges: 物联网环境中数据来源多样导致数据异构，客户端数据分布非独立同分布（non-IID），且设备资源有限，对模型效率和通信开销提出了挑战。

Contributions: 提出了一种四层雾辅助分层框架，结合Mist层特征统一与轻量模型、边缘层客户端选择、雾层FedProx聚合和云层全局模型整合，有效提升了异构环境下的训练稳定性与检测性能。

Results: 在TON-IoT数据集上的实验表明，该框架在异构和大规模场景下仍能达到98-99%的准确率，PR-AUC超过0.97，并表现出稳定的收敛性，同时保持高效和隐私保护。

Conclusion: 所提出的分层联邦学习框架能有效应对物联网环境中数据异构、non-IID分布和资源限制等挑战，在保证隐私的前提下实现了高性能的入侵检测，适用于大规模物联网部署。

Related Work: 相关工作包括联邦学习在边缘计算中的应用、non-IID数据下的FedProx优化方法，以及基于物联网的入侵检测数据集（如TON-IoT）的使用。

Abstract: The rapid growth of the Internet of Things (IoT) offers new opportunities but
also expands the attack surface of distributed, resource-limited devices.
Intrusion detection in such environments is difficult due to data heterogeneity
from diverse sensing modalities and the non-IID distribution of samples across
clients. Federated Learning (FL) provides a privacy-preserving alternative to
centralized training, yet conventional frameworks struggle under these
conditions. To address this, we propose a Mist-assisted hierarchical framework
for IoT intrusion detection. The architecture spans four layers: (i) Mist,
where raw data are abstracted into a unified feature space and lightweight
models detect anomalies; (ii) Edge, which applies utility-based client
selection; (iii) Fog, where multiple regional aggregators use FedProx to
stabilize training; and (iv) Cloud, which consolidates and disseminates global
models. Evaluations on the TON-IoT dataset show the framework achieves 98-99%
accuracy, PR-AUC> 0.97, and stable convergence under heterogeneous and
large-scale settings, while maintaining efficiency and preserving privacy.

</details>


### [12] [Reinforcement Learning for Resource Allocation in Vehicular Multi-Fog Computing](https://arxiv.org/abs/2511.00276)
*Mohammad Hadi Akbarzadeh,Mahmood Ahmadi,Mohammad Saeed Jahangiry,Jae Young Hur*

Main category: cs.NI

TL;DR: 本研究探讨了强化学习在多雾计算环境中的资源分配问题，通过将问题建模为马尔可夫决策过程，应用Q-learning、DQN和Actor-Critic等算法，有效降低了延迟并提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 由于物联网设备、智能车辆和延迟敏感型应用的快速增长，对高效分布式计算范式的需求日益迫切，而传统优化方法难以适应多雾计算中动态变化的资源需求。

Challenges: 多雾计算中的资源分配面临动态车辆移动性、异构资源和波动工作负载等挑战，导致传统方法适应能力差。

Contributions: 本文将资源分配问题建模为马尔可夫决策过程，并系统评估了多种强化学习算法在降低延迟、平衡负载和提升任务成功率方面的有效性，突出了强化学习在应对 vehicular 计算挑战中的潜力。

Results: 实验结果表明，采用Q-learning、Deep Q-Networks和Actor-Critic等强化学习算法显著改善了延迟、工作负载均衡性和任务成功率达到预期目标。

Conclusion: 强化学习为多雾计算环境下的自适应资源分配提供了有效解决方案，具有良好的应用前景。

Related Work: 相关工作主要集中在雾/边缘计算中的资源管理和优化，以及使用传统优化方法进行任务调度，但缺乏对高度动态环境的适应能力。

Abstract: The exponential growth of Internet of Things (IoT) devices, smart vehicles,
and latency-sensitive applications has created an urgent demand for efficient
distributed computing paradigms. Multi-Fog Computing (MFC), as an extension of
fog and edge computing, deploys multiple fog nodes near end users to reduce
latency, enhance scalability, and ensure Quality of Service (QoS). However,
resource allocation in MFC environments is highly challenging due to dynamic
vehicular mobility, heterogeneous resources, and fluctuating workloads.
Traditional optimization-based methods often fail to adapt to such dynamics.
Reinforcement Learning (RL), as a model-free decision-making framework, enables
adaptive task allocation by continuously interacting with the environment. This
paper formulates the resource allocation problem in MFC as a Markov Decision
Process (MDP) and investigates the application of RL algorithms such as
Q-learning, Deep Q-Networks (DQN), and Actor-Critic. We present experimental
results demonstrating improvements in latency, workload balance, and task
success rate. The contributions and novelty of this study are also discussed,
highlighting the role of RL in addressing emerging vehicular computing
challenges.

</details>


### [13] [Impact of Antenna Arrays Misalignment on the Near Field Distance in Terahertz Communications](https://arxiv.org/abs/2511.00502)
*Peng Zhang,Vitaly Petrov,Emil Björnson*

Main category: cs.NI

TL;DR: 本文研究了太赫兹通信中由于空间错位对近场距离计算的影响，推导了在任意错位情况下ULA和UPA配置下的近场边界精确表达式与简化近似，并通过仿真验证了模型的有效性，为实际部署太赫兹系统提供了重要指导。


<details>
  <summary>Details</summary>
Motivation: 由于太赫兹通信波长极短，其辐射近场区域显著扩展，传统基于理想对齐假设的远场条件（如夫琅禾费距离）不再适用，实际中由移动性或机械误差引起的收发器错位问题亟需解决。

Challenges: 现有近场边界公式假设收发器完全对齐，忽略了实际应用中的空间错位影响，导致近场区域估计不准确，影响系统设计与性能。

Contributions: 1. 推导了在任意空间错位下ULA-ULA和UPA-UPA配置的近场边界精确解析表达式；2. 提出了简化的近似公式；3. 通过数值仿真验证了理论模型，并量化了错位对近场区域的影响。

Results: 仿真结果表明，空间错位会显著改变近场边界，错位越大，近场区域扩展越明显，传统公式在错位存在时会产生较大误差。

Conclusion: 考虑空间错位的近场模型更符合实际场景，对太赫兹系统的部署和优化具有重要意义，建议在系统设计中引入错位感知的近场建模方法。

Related Work: 已有研究主要基于理想对齐假设定义近场边界（如Fraunhofer距离），未充分考虑实际错位影响，本文在此基础上扩展了更通用的非对齐场景模型。

Abstract: The extremely short wavelength of terahertz (THz) communications leads to an
extended radiative near-field region, in which some canonical far-field
assumptions fail. Existing near-field boundary formulations (Fraunhofer
distance) for uniform linear/planar array (ULA/UPA) configurations assume ideal
alignment between transceivers, overlooking practical misalignments caused by
mobility or mechanical imperfections. This paper addresses this critical gap by
analyzing the impact of spatial misalignment on near-field distance
calculations in THz systems. We derive exact analytical expressions and
simplified approximations for the near-field boundary in both ULA--ULA and
UPA--UPA configurations under arbitrary misalignment offsets. Through numerical
simulations, we validate our theoretical models and quantify how misalignment
reshapes the near-field region. These findings provide essential guidelines for
optimizing THz system deployment in realistic scenarios.

</details>


### [14] [Power Control Based on Multi-Agent Deep Q Network for D2D Communication](https://arxiv.org/abs/2511.00767)
*Shi Gengtian,Takashi Koshimizu,Megumi Saito,Pan Zhenni,Liu Jiang,Shigeru Shimamoto*

Main category: cs.NI

TL;DR: 本文提出了一种基于强化学习的自适应功率控制算法，用于提升D2D通信系统中的频谱资源利用率并减少对蜂窝用户的干扰，仿真结果表明该算法在系统吞吐量方面优于传统LTE算法。


<details>
  <summary>Details</summary>
Motivation: 在D2D通信中，若不控制D2D用户产生的干扰，会降低蜂窝用户的服务质量（QoS）和系统性能，因此需要有效的功率控制机制来平衡资源利用与干扰管理。

Challenges: 主要挑战在于如何在提升系统频谱利用率的同时，有效控制D2D用户对蜂窝用户的干扰，确保系统整体性能和QoS。

Contributions: 提出了一种基于强化学习的自适应功率控制算法，能够动态调整发射功率以减少干扰，提升系统吞吐量。

Results: 仿真结果表明，所提出的算法在系统吞吐量方面优于传统的LTE功率控制算法，验证了其在干扰管理和资源利用上的有效性。

Conclusion: 该强化学习算法能有效实现D2D通信中的自适应功率控制，显著提升系统性能，具有在实际LTE系统中应用的潜力。

Related Work: 相关工作主要集中在D2D通信中的资源分配与功率控制，特别是基于传统优化方法的干扰管理机制，而本文采用强化学习实现了更灵活和高效的自适应控制。

Abstract: In device-to-device (D2D) communication under a cell with resource sharing
mode the spectrum resource utilization of the system will be improved. However,
if the interference generated by the D2D user is not controlled, the
performance of the entire system and the quality of service (QOS) of the
cellular user may be degraded. Power control is important because it helps to
reduce interference in the system. In this paper, we propose a reinforcement
learning algorithm for adaptive power control that helps reduce interference to
increase system throughput. Simulation results show the proposed algorithm has
better performance than traditional algorithm in LTE (Long Term Evolution).

</details>


### [15] [3D Gaussian Radiation Field Modeling for Integrated RIS-FAS Systems: Analysis and Optimization](https://arxiv.org/abs/2511.01373)
*Kaining Wang,Bo Yang,Yusheng Lei,Zhiwen Yu,Xuelin Cao,Liang Wang,Bin Guo,George C. Alexandropoulos,Mérouane Debbah,Zhu Han*

Main category: cs.NI

TL;DR: 本文提出了一种基于三维高斯辐射场建模的场信息驱动优化方法，用于快速衰落信道下流体天线系统（FAS）与可重构智能表面（RIS）的联合实时优化，显著提升了频谱预测精度、收敛速度和最低可达速率。


<details>
  <summary>Details</summary>
Motivation: 在快速衰落信道条件下，传统方法难以实时高效地联合优化FAS的天线位置和RIS的相位配置，因此需要一种低开销、高精度的实时优化方案。

Challenges: 快速衰落信道下信道状态变化剧烈，传统迭代优化方法计算复杂、耗时长，难以满足实时性要求；同时获取精确信道信息需要大量导频开销。

Contributions: 1）提出基于三维高斯辐射场建模的场信息驱动方法，通过将障碍物视为虚拟发射机，分别学习幅度和相位变化，快速生成高精度信道信息；2）设计无需大量导频和复杂计算的交替优化框架，实现FAS位置与RIS相位的联合优化。

Results: 仿真结果表明，所提方法在频谱预测精度、收敛速度和最小可达速率方面均显著优于现有方法，尤其在快速衰落场景下表现出更强的适应性和实用性。

Conclusion: 该方法有效解决了FAS-RIS系统在动态信道环境中的实时联合优化难题，具备高精度、低延迟和低开销的优势，具有良好的实际应用前景。

Related Work: 相关工作主要集中在RIS相位优化和FAS天线选择的独立研究上，部分工作开始探索RIS与FAS的联合设计，但大多依赖传统迭代算法，缺乏对动态信道的快速响应能力。

Abstract: The integration of reconfigurable intelligent surfaces (RIS) and fluid
antenna systems (FAS) has attracted considerable attention due to its
tremendous potential in enhancing wireless communication performance. However,
under fast-fading channel conditions, rapidly and effectively performing joint
optimization of the antenna positions in an FAS system and the RIS phase
configuration remains a critical challenge. Traditional optimization methods
typically rely on complex iterative computations, thus making it challenging to
obtain optimal solutions in real time within dynamic channel environments. To
address this issue, this paper introduces a field information-driven
optimization method based on three-dimensional Gaussian radiation-field
modeling for real-time optimization of integrated FAS-RIS systems. In the
proposed approach, obstacles are treated as virtual transmitters and, by
separately learning the amplitude and phase variations, the model can quickly
generate high-precision channel information based on the transmitter's
position. This design eliminates the need for extensive pilot overhead and
cumbersome computations. On this framework, an alternating optimization scheme
is presented to jointly optimize the FAS position and the RIS phase
configuration. Simulation results demonstrate that the proposed method
significantly outperforms existing approaches in terms of spectrum prediction
accuracy, convergence speed, and minimum achievable rate, validating its
effectiveness and practicality in fast-fading scenarios.

</details>
