<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.NI](#cs.NI) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文综述了已部署和正在使用的负载调度器解决方案，并根据其架构和设计将其划分为多个层次类别。重点分析影响吞吐量和可扩展性的关键设计因素及改进，并特别关注Google的Borg系统。


<details>
  <summary>Details</summary>
Motivation: 为了系统地理解现有负载调度器的设计特点及其对性能的影响，特别是吞吐量和可扩展性方面的表现。

Challenges: 不同调度器架构之间的差异较大，难以统一比较；同时，实际部署中的性能受多种因素影响，提取关键设计因素具有挑战性。

Contributions: 提出了一种新的分类法，基于架构和设计对负载调度器进行分层；重点分析了影响性能的关键设计因素及其演进；深入探讨了Google Borg系统的设计优势。

Results: 明确了不同架构在吞吐量和可扩展性方面的优劣；总结了提升调度器性能的增量改进方法；突出了Borg系统在工业级调度器中的领先地位。

Conclusion: 架构设计对调度器的性能有显著影响，未来的工作应继续优化关键设计因素以提升大规模系统的效率。

Related Work: 已有其他分类法研究负载调度器，但本文更聚焦于影响吞吐量和可扩展性的核心设计要素及其演进路径。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [2] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 本报告阐述了德国达姆施塔特FAIR研究中心自2028年起的计算基础设施规划，旨在构建一个联邦式、集中协调的计算架构，以支持多样化的科研需求，并具备应对未来数据挑战的可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 为了支持FAIR中心多样化的科研项目，并应对未来日益增长的数据处理和存储需求，需要建立一个高效、灵活且可持续发展的计算基础设施。

Challenges: 主要挑战包括整合不同研究团队的异构计算需求、实现数据与服务的开放共享、确保系统的高可扩展性与灵活性，以及在分布式环境中实现集中协调管理。

Contributions: 提出了面向2028年及以后的FAIR计算模型，涵盖计算与存储政策、开放数据与软件服务架构，以及分阶段实施的模块化方案，为大型科研设施的信息化建设提供了系统性设计蓝图。

Results: 报告确立了一个联邦式、由中心协调的计算基础设施框架，支持‘第一科学（增强）’阶段及后续模块化FAIR运行，明确了各阶段的技术路线与政策体系。

Conclusion: 该计算基础设施设计方案能够有效支撑FAIR多学科研究需求，具备应对未来数据密集型科学挑战的能力，是实现开放科学和协作研究的重要基础。

Related Work: 相关工作包括国际大型科研机构（如CERN、ESRF等）的计算基础设施设计，以及在开放科学、数据治理和云计算架构方面的先进实践。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [3] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 本文讨论了云计算在AI/ML驱动下的发展如何重塑计算资源格局，并分析了当前云成本模型对科学研究支持的不足，探讨了未来可能的改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML成为现代计算的主要驱动力，云计算资源和成本模型逐渐向其倾斜，导致科学计算面临资源竞争和访问困难，因此需要重新审视云成本模型对科研的支持能力。

Challenges: 当前的云成本和资源模型主要服务于AI/ML需求，难以满足科学计算对高性能、低延迟和大规模并行处理的需求；科研用户在资源竞争中处于劣势，可能导致科学工作负载在非优化环境中运行。

Contributions: 本文回顾了计算发展的历史演变，分析了当前云计算资源分配的单极化趋势，并提出了为支持科学发现而改进云成本模型的可能路径。

Results: 文章指出，若不调整现有云资源分配和成本模型，科学计算将面临更大的资源获取障碍，影响科研效率和创新。

Conclusion: 为了持续支持科学探索，需要设计更加多元、公平的云资源成本模型，以平衡AI/ML与科学计算之间的资源需求。

Related Work: 相关研究包括科学计算的资源调度优化、云计算中的多租户资源分配机制，以及针对高性能计算（HPC）与AI融合的架构设计。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [4] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: 本论文提出了EdgeReasoning，系统研究了在边缘GPU上部署用于推理任务的大语言模型的延迟-精度权衡，提供了在严格延迟约束下优化部署的指导方案。


<details>
  <summary>Details</summary>
Motivation: 由于边缘智能在自主系统中的需求增加，将大语言模型部署在边缘设备上面临延迟和资源限制的挑战，亟需系统性指导以平衡推理性能与资源消耗。

Challenges: 在边缘GPU上部署大语言模型进行推理任务时，面临严格的延迟限制、计算资源有限、模型架构选择、模型规模、令牌预算分配以及运行时扩展策略之间的权衡等挑战。

Contributions: 1）系统评估了不同大语言模型架构和规模下的延迟-精度权衡；2）评估了基于提示和模型微调的推理令牌压缩技术；3）分析了不同并行度下的运行时扩展方法；4）绘制了边缘推理部署的帕累托前沿，提供了系统性部署指南。

Results: 研究结果表明，通过合理选择模型架构、压缩推理路径并采用并行测试时扩展策略，可以在满足严格延迟要求的同时显著提升推理精度，实现了延迟与精度之间的有效权衡。

Conclusion: EdgeReasoning为在边缘设备上部署推理型大语言模型提供了系统性的分析框架和实践指导，有助于在资源受限条件下实现高效、准确的智能决策。

Related Work: 相关工作包括边缘智能中的模型压缩、低延迟推理优化、大语言模型的提示工程与微调方法，以及测试时扩展策略的研究。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [5] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文研究了基于多功能核心的多核处理器结构分析及效率指标（可靠性、容错性、生存性和灵活性）评估模型，利用逻辑概率方法建立了核心可靠性、容错性、最短路径、灵活性及性能条件等模型，并分析了双核和四核处理器的结构特性与效率提升趋势。


<details>
  <summary>Details</summary>
Motivation: 为了提高多核处理器在复杂环境下的可靠性与性能，需要建立有效的结构分析与效率评估模型，以支持可变结构多核处理器的设计与优化。

Challenges: 如何准确建模多核处理器在不同状态下的可靠性、容错性、灵活性和性能；如何综合考虑多功能核心的多状态特性对系统整体效率的影响。

Contributions: 提出了基于逻辑概率方法的多核处理器可靠性、容错性、灵活性和性能评估模型；建立了考虑所有可能工作状态的处理器寿命估计模型；给出了双核与四核处理器的结构分析结果及效率提升趋势。

Results: 通过结构分析，验证了所提模型的有效性，结果显示增加核心数量和功能多样性可显著提升多核处理器的可靠性、容错性和灵活性；同时揭示了效率指标随结构变化的趋势。

Conclusion: 所提出的模型能够有效支持可变结构多核处理器的设计与评估，为提升系统整体效率提供了理论依据和方法支持。

Related Work: 已有研究主要集中在固定结构多核处理器的可靠性分析，而本文针对具有多功能核心和可变结构的多核处理器，拓展了结构灵活性与多状态建模的研究。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [6] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLR是一种基于记忆化的加速方法，用于提升ADMM-FFT在层析成像重建中的性能，通过减少重复的FFT计算并结合变量卸载技术，显著降低计算时间和内存消耗，在2Kx2Kx2K的大规模问题上实现了最高65.4%的性能提升。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT虽然重建精度高，但计算时间长、内存消耗大，限制了其在大规模层析成像问题中的应用。

Challenges: 如何在不牺牲精度的前提下减少ADMM-FFT中重复的FFT计算开销，并有效管理内存使用以支持更大规模的问题求解。

Contributions: 提出了mLR方法，利用记忆化技术避免重复FFT计算；设计了一系列优化技术使记忆化在ADMM-FFT中高效且可扩展；引入变量卸载机制以减少CPU内存占用并支持跨节点多GPU扩展。

Results: 在2Kx2Kx2K的输入问题上成功扩展了ADMM-FFT，这是目前该方法在有限内存下处理的最大规模问题；相比原始ADMM-FFT，mLR平均性能提升52.8%，最高达65.4%。

Conclusion: mLR有效解决了ADMM-FFT在计算效率和内存使用方面的瓶颈，显著提升了其可扩展性和实际应用能力。

Related Work: ADMM-FFT是目前层析成像重建中精度较高的迭代方法，已有工作主要关注算法收敛性，但在计算效率和内存优化方面研究较少。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [7] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 本文提出了“三大税收”分析框架，用以识别大规模语言模型在多GPU分布式执行中的性能瓶颈，并通过细粒度编程模式和Iris for Triton等库实现消除这些瓶颈，显著提升了执行效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的扩大，传统批量同步并行（BSP）模型在分布式GPU执行中引入了显著的性能低效，亟需更高效的执行范式。

Challenges: 主要挑战包括Bulk Synchronous Tax（同步开销）、Inter-Kernel Data Locality Tax（数据局部性差）和Kernel Launch Overhead Tax（内核启动开销），这些因素共同限制了分布式LLM的性能。

Contributions: 提出“三大税收”分析框架；利用Iris for Triton实现细粒度编程模式；设计了消除三大税收的新型执行方式，如tile-level生产者-消费者流水线和细粒度数据流同步。

Results: 在All-Gather+GEMM和Flash Decode等关键算子上实现了10-20%的端到端延迟降低。

Conclusion: 通过摆脱传统的BSP模型并采用细粒度同步与通信，可以显著提升分布式LLM工作负载的效率和可编程性。

Related Work: 与分布式训练中的BSP优化、异步并行、通信库（如NCCL）以及Triton编译器相关的研究工作密切相关。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [8] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 本文提出了一种基于算子级自动扩展的框架，通过在更细粒度上优化资源分配、批处理和放置，显著提升了大模型推理服务的效率，在满足SLO的同时减少了40%的GPU使用和35%的能耗，或在固定资源下实现1.6倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型服务方案采用静态资源配置或模型级自动扩展，将模型视为整体，导致对动态推理流量适应性差，资源利用率低或性能下降。因此需要更细粒度的资源管理方法。

Challenges: 生成式模型内部由多个异构算子组成，不同算子在计算和内存开销上差异大，且对批大小、序列长度和流量速率等参数敏感，传统模型级扩展难以有效应对这种异构性和动态性。

Contributions: 1. 揭示了生成式模型中算子级异构性及其对资源和负载的敏感性；2. 提出了算子级自动扩展框架，实现细粒度的资源分配、批处理和放置优化；3. 在真实生产轨迹上验证了该方法在资源节省和吞吐提升方面的显著优势。

Results: 实验结果显示，相比现有方法，该框架在满足SLO的前提下最多减少40%的GPU使用和35%的能耗；在固定资源下可实现1.6倍的吞吐量提升并减少5%的能耗。

Conclusion: 算子而非整个模型是扩展大型生成式工作负载更有效的基本单位，算子级自动扩展为高效服务大模型提供了新的方向。

Related Work: 相关工作主要包括模型级自动扩展（如Kubernetes部署、动态批处理）和静态资源分配策略，但这些方法缺乏对模型内部结构的细粒度考虑，无法有效应对推理负载的动态变化。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [9] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出两种新的调度算法，通过优化二元批张量收缩的顺序来提高输入/中间张量的重用，从而减少GPU上的数据流量并降低峰值内存使用，最终加速格点量子色动力学（LQCD）中的关联函数计算。


<details>
  <summary>Details</summary>
Motivation: 在LQCD模拟中，关联函数的计算涉及大量大张量的二元批量收缩，这些操作在GPU上执行时面临内存重用不足和数据传输开销大的问题，因此需要高效的调度策略以提升性能。

Challenges: 主要挑战在于如何调度大量的大内存张量收缩操作，以最大化张量重用、减少数据迁移，并在有限的GPU内存下最小化峰值内存使用。

Contributions: 提出了两种新颖的快速调度算法，利用二元收缩特性和收缩树内的局部性来优化内存使用；算法集成到LQCD软件Redstar中，显著提升了计算效率。

Results: 相比现有方法，新调度器最高实现2.1倍的峰值内存优化，减少最多4.2倍的内存驱逐和1.8倍的数据传输，关联函数计算速度最高提升1.9倍。

Conclusion: 所提出的调度算法有效提升了LQCD中张量收缩的执行效率，显著降低了内存开销和数据传输，加快了物理可观测量的提取过程。

Related Work: 相关工作包括张量收缩优化、高性能计算中的内存调度算法以及LQCD中的批处理计算技术。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [10] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: 本文提出了FedAttn，一种将联邦学习范式融入自注意力机制的分布式大语言模型推理框架，可在保护隐私的同时实现通信与计算高效性，并通过理论分析揭示了局部计算与参与者间异构性对误差传播的影响，实验证明其在边缘场景中具有良好的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备上的协同推理面临隐私泄露、通信开销和计算瓶颈等挑战，亟需一种兼顾隐私保护与系统效率的新型分布式推理框架。

Challenges: 主要挑战包括：如何在不暴露私有输入的前提下进行协作推理；如何减少参与者之间的通信开销；如何在异构设备上实现高效的本地计算；以及如何平衡响应质量与资源消耗。

Contributions: 1) 提出FedAttn框架，实现隐私保护的分布式LLM推理；2) 发现FedAttn中上下文优化与联邦学习参数更新之间的结构对偶性；3) 建立误差传播的理论模型；4) 揭示响应质量与通信/计算效率之间的权衡关系；5) 验证稀疏注意力与自适应KV聚合的优化潜力。

Results: 实验结果验证了理论分析的正确性，表明FedAttn在不同同步间隔和参与方数量下能有效控制误差传播，同时结合稀疏注意力和自适应KV聚合可显著提升通信与计算效率，且对输出质量影响较小。

Conclusion: FedAttn为边缘环境下大语言模型的协同推理提供了一个高效、安全的解决方案，通过融合联邦学习思想与Transformer架构，实现了隐私、效率与性能的统一，具备实际部署潜力。

Related Work: 相关工作主要包括联邦学习（Federated Learning）、分布式推理、Transformer的自注意力机制优化、以及边缘AI中的隐私保护技术。本文不同于传统FL中的梯度或模型聚合，创新性地将联邦思想应用于注意力机制中的KV矩阵交换。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [11] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 该论文初步评估了Kokkos、OpenMP、RAJA和OCCA四种性能可移植框架在N体模拟和结构化网格模拟中的求解时间表现，实验基于Polaris超算单节点的四个NVIDIA A100 GPU，结果显示各框架性能差异显著，未来工作将聚焦于优化算法、通信与扩展性分析。


<details>
  <summary>Details</summary>
Motivation: 为应对异构计算架构下科学计算对跨平台高效执行的需求，研究旨在评估主流性能可移植框架在典型科学计算应用中的实际表现，推动实现无需大量代码修改即可在不同硬件上高效运行的目标。

Challenges: 不同性能可移植框架在GPU上的执行效率存在显著差异；OCCA虽在小规模问题上表现优异但缺乏优化的归约算法；OpenMP在结构化网格模拟中因数据同步与通信低效导致性能不佳；如何提升各框架的扩展性与通信效率仍具挑战。

Contributions: 提供了Kokkos、OpenMP、RAJA和OCCA在两类典型科学计算应用上的初步性能对比结果；揭示了各框架在实际使用中的优势与瓶颈，为后续优化提供方向；明确了未来在归约算法、通信机制和内存管理方面的改进重点。

Results: 在NVIDIA A100 GPU上，OCCA在小规模验证问题中执行速度最快，可能得益于JIT编译；但在大规模模拟中因缺乏优化的归约算法而受限；OpenMP在结构化网格模拟中表现最差，推测源于节点间数据同步与通信效率低下；四种框架整体表现出显著的性能差异。

Conclusion: 当前性能可移植框架在实际应用中表现不一，需进一步优化以充分发挥其跨平台潜力；特别是在归约操作、数据通信和内存管理方面仍有较大提升空间；未来需开展系统性的扩展性研究与统计分析以全面评估框架性能。

Related Work: 相关工作主要集中在性能可移植性框架的发展与应用，如Kokkos、RAJA、OpenMP和OCCA等，这些库致力于通过抽象化编程模型使高性能计算应用能在CPU和GPU等多种硬件平台上高效运行，已有研究比较了它们在特定场景下的性能表现，但系统性评估仍不足。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [12] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: 本文提出了EPaxos*，一种更简单且正确的Egalitarian Paxos变体，解决了原协议复杂、模糊和存在错误的问题，并通过简化的故障恢复算法实现了严格正确性证明，同时将协议推广到最优的容错阈值范围。


<details>
  <summary>Details</summary>
Motivation: Egalitarian Paxos虽然提供了去中心化的状态机复制方案并提升了容错性和性能，但其协议复杂、规范不明确且存在非平凡的bug，因此需要一个更简洁、正确且可证明安全的替代方案。

Challenges: 主要挑战在于设计一个既能保持Egalitarian Paxos高可用和低延迟优点，又能简化协议逻辑、明确规范并确保在各种故障场景下正确恢复的协议；特别是处理并发命令的依赖关系与失败恢复的一致性问题。

Contributions: 1) 提出了EPaxos*，一个更简单且正确的Egalitarian Paxos变体；2) 设计了一个简化的、经过严格正确性证明的故障恢复算法；3) 将协议推广至覆盖更广的容错阈值f和e，满足n ≥ max{2e+f−1, 2f+1}，并证明该进程数为最优。

Results: EPaxos*在保持原有性能优势的同时显著降低了协议复杂性，提供了形式化正确性证明，并支持更灵活的容错配置，适用于不同系统需求下的部署。

Conclusion: EPaxos*成功改进了Egalitarian Paxos的复杂性和正确性缺陷，提供了一个更可靠、更易实现的状态机复制协议，为后续基于去领导者架构的系统设计奠定了坚实基础。

Related Work: 最相关的工作是Egalitarian Paxos，它首次实现了去领导者的状态机复制，允许副本协作排序命令，并在最多f个进程崩溃时仍保持吞吐量，且在特定条件下实现2条消息延迟的快速执行。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 本文提出了针对下一代解聚式GPU架构中非统一内存访问（NUMA）问题的Swizzled Head-first Mapping调度策略，通过将注意力头与GPU的NUMA域对齐，显著提升了多头注意力机制的性能和L2缓存命中率。


<details>
  <summary>Details</summary>
Motivation: 随着多芯片设计成为扩展AI计算能力的主流，传统假设内存访问均匀的GPU调度策略在面对显著变化的内存延迟和带宽时表现不佳，尤其是在大规模注意力工作负载中，NUMA效应严重影响了性能。

Challenges: 主要挑战在于如何在多芯片架构下有效管理内存访问的非均匀性，避免因跨芯片数据传输导致的性能下降，并最大化局部性和缓存复用。

Contributions: 提出了一种空间感知的调度策略——Swizzled Head-first Mapping，首次将注意力头的映射与GPU的NUMA域结构对齐，从而提升缓存利用率和整体性能。

Results: 在AMD MI300X架构上，该方法相比现有最先进的注意力算法性能提升高达50%，并实现了80%-97%的持续高L2缓存命中率。

Conclusion: NUMA感知的调度已成为在下一代解聚式GPU上实现高效AI训练与推理的关键，本文为解决大规模注意力机制中的性能瓶颈提供了可行路径。

Related Work: 相关工作包括传统的GPU调度策略、多头注意力优化方法以及NUMA-aware系统设计，但这些工作大多未充分考虑解聚式GPU中芯片间的内存访问差异。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [14] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本论文提出在通用粗粒度线性阵列（CGLA）加速器IMAX上实现Whisper的核心计算内核，通过硬件/软件协同设计，在FPGA原型上评估并预测28nm ASIC性能，结果显示其在能效上显著优于Jetson AGX Orin和RTX 4090，为低功耗边缘设备上的可持续语音识别提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在自动语音识别（ASR）等任务中的广泛应用带来了严重的能耗问题，而专用集成电路（ASIC）虽然高效但缺乏适应算法演进的可编程性，因此需要一种兼顾能效与灵活性的加速方案。

Challenges: 如何在保持高能效的同时提供足够的可编程性以适应不断演进的ASR算法，特别是在资源受限的边缘设备上实现可持续的高性能计算。

Contributions: 1）首次在CGRA（IMAX）上实现并评估Whisper的核心计算内核；2）通过硬件/软件协同设计方法，结合FPGA原型验证与ASIC性能预测；3）证明CGLA在ASR任务中相较主流GPU具有显著能效优势。

Results: 在28nm ASIC预测下，IMAX比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍（Q8_0模型），展现出卓越的能源效率表现。

Conclusion: CGLA架构在保持可编程性的同时实现了优异的能效，是面向低功耗边缘设备可持续ASR应用的有前景平台。

Related Work: 相关工作主要集中在基于ASIC或GPU的ASR加速方案，前者高效但缺乏灵活性，后者灵活但能效较低；本文提出的基于CGRA的方案填补了能效与可编程性之间的空白。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [15] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus 是一个三阶段框架，通过增强大语言模型在Verilog代码生成中的推理聚焦能力，显著提升了生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在生成Verilog代码方面表现出潜力，但确保功能正确性仍具挑战。现有方法未能有效引导模型关注代码中关键的决策点。

Challenges: 主要挑战在于如何提升生成代码的功能正确性，同时有效识别并聚焦于代码生成过程中的关键决策部分。

Contributions: 提出VFocus框架，包含密度引导过滤、自一致性聚类和推理增强的细化提示，系统性地优化了Verilog代码生成的推理过程。

Results: 在VerilogEval-Human基准上，VFocus显著提高了多个推理型大模型的pass@1正确率，验证了其在复杂硬件设计任务中的有效性。

Conclusion: VFocus通过聚焦关键推理步骤，有效提升了大语言模型生成Verilog代码的准确性和可靠性，为硬件设计自动化提供了新思路。

Related Work: 相关工作包括利用自一致性、模拟反馈和代码筛选机制来优化LLM生成代码的正确性，但缺乏对推理过程的结构化引导。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [16] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 本文提出了一种基于SoC FPGA和DPU的多线程独立面部表情识别系统，通过使用DenseBox进行人脸检测和CNN进行表情识别，有效利用FPGA资源并提升系统吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统Haar Cascade检测器在侧脸和光照变化条件下准确率低，以及专用硬件加速器难以扩展的问题，本文旨在设计一种高效、资源利用率高的面部表情识别系统。

Challenges: 主要挑战包括在FPGA资源有限的情况下实现高精度的人脸检测与表情识别，同时提升系统整体吞吐量和DPU利用率。

Contributions: 1. 在同一DPU上运行DenseBox（人脸检测）和CNN（表情识别），提高FPGA资源利用率；2. 提出多线程技术，提升系统吞吐量和DPU使用效率；3. 实现25 FPS的整体系统吞吐量，能效比提升2.4倍。

Results: 系统实现了25 FPS的处理速度，吞吐量每瓦特提升2.4倍，验证了在资源受限环境下使用通用DPU进行多任务深度学习推理的有效性。

Conclusion: 该方法在保持小电路规模的同时，显著提升了面部表情识别系统的性能和能效，适用于嵌入式和边缘计算场景。

Related Work: 先前的工作使用CPU运行Haar Cascade进行人脸检测，精度较低；同时采用专用电路加速器，难以扩展支持多个DNN推理任务。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [17] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了一种基于数字递归算法的基数-4定点除法单元，通过冗余算术、在线商转换和操作数缩放等硬件优化技术，显著降低了延迟、面积和功耗开销，在多种定点配置下实现了超过80%的能耗降低和迭代次数的大幅减少。


<details>
  <summary>Details</summary>
Motivation: 由于定点数在硬件实现上的复杂性，其除法操作仍然具有挑战性，而现有的浮点数替代方案在效率和精度上存在局限，因此需要一种高效且低功耗的定点除法实现方法。

Challenges: 定点除法在硬件实现中面临高延迟、大面积和高功耗的问题，同时传统算法迭代次数多，难以满足高性能计算需求。

Contributions: 1) 首次将基数-4数字递归算法应用于定点除法；2) 引入冗余算术、在线商转换和操作数缩放等硬件优化技术；3) 实现了低延迟、低功耗和小面积开销的定点除法单元。

Results: 综合评估结果显示，与现有方法相比，能耗降低超过80%，面积开销小，并显著减少了迭代次数，验证了所提方法在性能和能效上的优势。

Conclusion: 本文提出的基于基数-4数字递归算法的定点除法单元通过多项硬件优化，在能效和性能方面显著优于现有方法，展示了其在提升定点算术单元效率方面的巨大潜力。

Related Work: 相关工作主要集中在IEEE 754浮点表示及其除法实现，以及早期的定点算术研究，但缺乏针对定点除法的高效硬件优化设计。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [18] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在通用粗粒度可重构阵列（CGRA）加速器IMAX3上实现了stable-diffusion.cpp图像生成框架的核心计算内核，并进行了深入评估，展示了其在FPGA原型上的性能和功耗效率，预测了其在ASIC实现中的潜力，为未来AI专用的粗粒度线性阵列（CGLA）加速器设计提供了指导，推动了高效、设备端、多模态AI平台的发展。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA架构IMAX3在复杂AI工作负载（如图像生成）中的性能和能效，探索其在下一代AI加速器中的潜力。

Challenges: 在保持架构通用性的同时，高效执行stable-diffusion.cpp中的计算密集型核心；在FPGA上实现并评估性能，并合理外推至ASIC实现的性能潜力。

Contributions: 首次在CGRA上实现并评估stable-diffusion.cpp的核心计算；提供了IMAX3在FPGA上的性能基线和ASIC下的性能预测；为未来AI专用CGLA加速器的设计提供了实践指导。

Results: IMAX3尽管是通用架构，在FPGA原型上已展现出有前景的性能和能效；在预测的ASIC实现中，性能和能效将进一步提升。

Conclusion: IMAX3作为一个通用计算平台，能够有效支持复杂的图像生成任务，证明了其作为未来AI专用加速器基础的可行性，为开发节能的设备端多模态AI平台奠定了基础。

Related Work: 与stable-diffusion.cpp框架、CGRA/CGLA架构设计、AI加速器（尤其是FPGA和ASIC实现）相关的研究工作。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM](https://arxiv.org/abs/2511.02171)
*Rodrigo Nunes,André Melo,Rafael Albarello,Reinaldo Gomes,Cesar Marcondes,Lourenço Pereira Jr*

Main category: cs.NI

TL;DR: 本文比较了两种符合当前监管框架的分布式无人机交通管理（UTM）架构，即InterUSS平台和基于Hyperledger Fabric的私有账本，发现基于区块链的系统需要专门针对航空性能约束设计的架构。


<details>
  <summary>Details</summary>
Motivation: 由于无人机交通管理（UTM）是安全关键且高度监管的领域，系统不仅需要高性能和高安全性，还必须符合标准和监管要求，因此有必要评估现有分布式架构的适用性。

Challenges: 区块链系统在满足航空领域严格的性能约束（如延迟、吞吐量）的同时，还需确保合规性、安全性和可扩展性，这对架构设计提出了重大挑战。

Contributions: 本文对两种符合监管框架的分布式UTM架构进行了基准测试，揭示了区块链技术在航空应用中的性能瓶颈，并强调了为航空环境定制架构的必要性。

Results: 研究发现，基于区块链的系统在当前设计下难以满足航空性能要求，必须进行针对性优化；相比之下，InterUSS平台在合规性和性能之间表现出更好的平衡。

Conclusion: 尽管区块链在UTM中具有潜力，但其应用必须结合航空领域的特殊性能和监管需求进行专门架构设计，不能直接套用通用方案。

Related Work: 相关工作包括InterUSS平台的设计与部署，以及将区块链技术应用于空中交通管理的研究，这些工作为本文的架构选择和评估提供了基础。

Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation
authorities to propose distributed Uncrewed Traffic Management (UTM)
architectures. Several studies have advocated blockchain as a promising
technology to meet these requirements. However, since UTM is a safety-critical
and highly regulated domain, compliance with standards and regulatory
frameworks is as crucial as performance and security. This work benchmarks two
distributed architectures aligned with current regulatory frameworks: the Linux
Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger.
Our findings reveal that blockchain-based systems require architectures
specifically designed for aeronautical performance constraints.

</details>


### [20] [Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains](https://arxiv.org/abs/2511.02368)
*Rushi Moliya,Dhaval K. Patel,Brijesh Soni,Miguel López-Benítez*

Main category: cs.NI

TL;DR: 本文提出了一种多无人机协同感知系统，通过结合遗传算法（GA）与粒子群优化（PSO）的分层启发式框架，在复杂地形下实现高效的视线（LoS）连接与能量优化部署，显著提升了检测概率并降低了悬停能耗。


<details>
  <summary>Details</summary>
Motivation: 在非平坦地形中，地形遮挡导致的视线（LoS）阻塞严重影响无人机对目标的检测性能，亟需一种能够兼顾空间、方向与能耗约束的协同部署方法。

Challenges: 1) 地形感知下的视线阻塞问题；2) 多目标优化问题的非凸性；3) 空间、方向与安全约束的耦合；4) 方向性天线带来的朝向敏感性。

Contributions: 1) 引入二值视线指示器与基于包围体层次结构（BVH）的自适应评估方法以高效处理地形遮挡；2) 构建最大化协同检测概率与最小化悬停能耗的双目标优化模型；3) 提出结合GA探索与PSO精细化的分层启发式求解框架，并采用基于惩罚项的适应度函数确保约束满足。

Results: 在真实地形数据上的蒙特卡洛仿真表明，与仅使用PSO的基线相比，所提GA+PSO框架在2架和3架无人机场景下检测概率分别提升37.02%和36.5%，平均多余悬停能耗降低45.0%和48.9%；相较于非优化方案，检测概率提高59.5%和54.2%，多余能耗降低59.8%和65.9%。

Conclusion: 所提出的GA+PSO分层优化框架在复杂地形下能有效平衡检测性能与能量消耗，适用于小规模多无人机系统的协同感知部署。

Related Work: 相关工作包括基于粒子群优化的无人机部署、遗传算法在路径规划中的应用、以及利用BVH加速三维场景可见性判断的技术，本文结合GA与PSO并引入BVH加速LoS评估，在协同感知优化中实现了性能突破。

Abstract: In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative
sensing system where UAVs are deployed to sense multiple targets in
terrain-aware line of sight (LoS) conditions in uneven terrain equipped with
directional antennas. To mitigate terrain-induced LoS blockages that degrade
detection performance, we incorporate a binary LoS indicator and propose a
bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS
evaluation. We formulate a bi-objective problem that maximizes the probability
of cooperative detection with minimal hover energy constraints governing
spatial, orientational, and safety constraints. To address the problem, which
is inherently non-convex, we propose a hierarchical heuristic framework that
combines exploration through a genetic algorithm (GA) with per-UAV refinement
via particle swarm optimization (PSO), where a penalty-based fitness evaluation
guides solutions toward feasibility, bounded within constraints. The proposed
methodology is an effective trade-off method of traversing through a complex
search space and maintaining terrain-aware LoS connectivity and energy aware
deployment. Monte Carlo simulations on real-world terrain data show that the
proposed GA+PSO framework improves detection probability by 37.02% and 36.5%
for 2 and 3 UAVs, respectively, while reducing average excess hover energy by
45.0% and 48.9% compared to the PSO-only baseline. Relative to the
non-optimized scheme, it further achieves 59.5% and 54.2% higher detection
probability with 59.8% and 65.9% lower excess hover energy, thereby showing its
effectiveness with a small number of UAVs over uneven terrain.

</details>


### [21] [Janus: Leveraging Incremental Computation for Efficient DNS Verification](https://arxiv.org/abs/2511.02559)
*Yao Wang,Kexin Yu,Wenyun Xu,Kaiqiang Hu,Ziyi Wang,Lizhao You,Qiang Su,Dong Guo,Haizhou Du,Wanjian Feng,Qingyu Song,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: Janus是一种高效的DNS配置验证工具，通过将域名服务器处理查询的过程转化为匹配动作表的匹配过程，实现了对DNS配置的快速和增量验证。


<details>
  <summary>Details</summary>
Motivation: 现有的DNS配置验证工具存在效率低下和缺乏对增量验证支持的问题。

Challenges: 如何高效地验证DNS配置，并支持增量验证以减少计算开销。

Contributions: 提出了Janus，一种新的DNS验证工具，包括高效的查询空间划分数据结构、符号执行算法以及支持增量验证的机制。

Results: 实验结果显示，Janus在真实世界数据集上实现了显著的速度提升，最高可达255.7倍的加速比，并且LEC数量最多减少了6046倍。

Conclusion: Janus能够有效解决现有DNS配置验证工具的不足，提供了一种更高效、支持增量验证的解决方案。

Related Work: 分布式数据平面验证的最新进展及其与DNS配置之间的相似性。

Abstract: Existing DNS configuration verification tools face significant issues (e.g.,
inefficient and lacking support for incremental verification). Inspired by the
advancements in recent work of distributed data plane verification and the
resemblance be- tween the data plane and DNS configuration, we tackle the
challenge of DNS misconfiguration by introducing Janus, a DNS verification
tool. Our key insight is that the process of a nameserver handling queries can
be transformed into a matching process on a match-action table. With this
insight, Janus consists of (1) an efficient data structure for partition query
space based on the behaviors, (2) a symbolic execution algorithm that specifies
how a single nameserver can efficiently cover all possible queries and ensure
the accuracy of verification, (3) a mechanism to support incremental
verification with less computational effort. Extensive experiments on
real-world datasets (with over 6 million resource records) show that Janus
achieves significant speedups, with peak improvements of up to 255.7x and a
maximum 6046x reduction in the number of LECs.

</details>
