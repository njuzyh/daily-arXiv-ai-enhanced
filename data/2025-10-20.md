<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文指出Messor论文中关于Ramulator 2.0和DAMOV的模拟结果存在严重错误，包括配置错误、使用了不相关的性能统计指标以及缺乏可复现性，并纠正了这些错误，强调了正确验证模拟结果和开源协作的重要性。


<details>
  <summary>Details</summary>
Motivation: Messor论文声称其新基准能更好评估内存系统性能，但其模拟结果存在问题，影响结论可靠性，因此有必要进行独立验证和纠错。

Challenges: 发现并复现Messor论文中Ramulator 2.0和DAMOV模拟器的配置与使用错误；确保模拟结果准确反映真实系统行为；提高研究可复现性和方法严谨性。

Contributions: 纠正了Messor论文中Ramulator 2.0的错误配置，证明其实际能较好拟合真实系统特性；指出DAMOV模拟中使用的错误统计指标；揭示其 artifact 缺乏完整源码导致不可完全复现；呼吁加强模拟器使用的验证流程和社区协作。

Results: 正确配置后，Ramulator 2.0的模拟结果与真实系统表现一致，反驳了Messor论文的核心主张；DAMOV模拟所用统计量与DRAM性能无关；Messor的artifact无法支持全部结果复现。

Conclusion: Messor论文的关键贡献基于错误的模拟结果，其结论不成立；必须严格验证模拟配置并与开发者协作以确保准确性；当前评审和artifact评估流程可能存在漏洞。

Related Work: Messor论文本身是本研究最直接的相关工作，此外涉及Ramulator 2.0、DAMOV等内存模拟器的正确使用与验证方法的研究也密切相关。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: 本文提出了一种名为Hive的高性能GPU哈希表，通过缓存对齐的桶布局、 warp级并发协议和动态负载感知扩容策略，在高负载和高并发场景下显著优于现有GPU哈希表。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表在并发更新、高负载因子和不规则内存访问方面存在性能瓶颈，难以满足数据密集型应用的需求。

Challenges: 如何在GPU上实现高效的并发控制、避免ABA问题、支持动态扩容而不进行全局重哈希，并维持高负载下的高性能。

Contributions: 1) 缓存对齐的紧凑桶结构，支持单CAS原子操作；2) warp同步的并发协议（WABC和WCME），减少原子操作竞争；3) 基于线性哈希的动态扩容策略，按warp并行批量调整容量；4) 四步插入策略（替换、声明提交、有限 cuckoo 驱逐、溢出暂存）应对高竞争。

Results: 在NVIDIA RTX 4090上，Hive哈希表在混合插入-删除-查找工作负载下吞吐量比现有方案高1.5-2倍，支持高达95%的负载因子；在均衡负载下达到35亿次更新/秒和近40亿次查找/秒。

Conclusion: Hive哈希表通过创新的内存布局、并发控制和动态扩容机制，实现了高性能、高负载和可扩展的GPU哈希表，适用于现代数据密集型应用。

Related Work: Slab-Hash、DyCuckoo 和 WarpCore 是当前最先进的GPU哈希表，但在高负载或高并发插入场景下性能受限。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [3] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一种新的区块链执行引擎，结合乐观并发控制（OCC）和对象数据模型，通过四项核心技术改进高争用场景下的性能，显著减少冗余计算并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法效率的提升，执行层成为新的性能瓶颈，尤其是在高争用工作负载下，现有并行执行框架（OCC和PCC）性能下降明显，亟需更高效的执行引擎。

Challenges: 在高争用场景下，乐观并发控制（OCC）因频繁冲突导致大量重执行，悲观并发控制（PCC）则因过度串行化限制了并行性，两者均难以实现高效执行。

Contributions: 提出NEMO，包含四项核心创新：(i) 针对仅使用自有对象的事务采用贪婪提交规则；(ii) 精细化依赖处理以减少重执行；(iii) 使用静态可推导的读写提示指导执行；(iv) 基于优先级的调度器，优先执行能解除阻塞的事务。

Results: 实验表明，NEMO显著减少了冗余计算，在16个工作线程下，吞吐量比最先进的OCC方案Block-STM最高提升42%，比PCC基线高出61%。

Conclusion: NEMO通过结合OCC与对象模型及多项优化机制，在高争用场景下有效提升了区块链执行层的性能，是解决当前执行瓶颈的有效方案。

Related Work: 相关工作主要包括基于乐观并发控制（如Block-STM）和悲观并发控制的区块链执行框架，这些方法在高争用环境下均存在性能局限。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [4] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 本文提出了一种基于Kubernetes的Charm++应用运行方案，并设计了一个基于优先级的弹性作业调度器，能够动态调整HPC作业规模以提高集群资源利用率并减少高优先级作业响应时间，实验表明该调度器相比传统静态调度器具有显著性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着HPC应用在云环境中的广泛采用，按需付费的模式要求更高效的资源利用，而传统并行编程模型（如MPI）缺乏对自动伸缩的良好支持，因此需要一种能原生支持动态扩展的编程模型和相应的调度机制。

Challenges: 主要挑战包括如何在Kubernetes环境中有效运行Charm++应用程序，如何实现低开销的动态作业伸缩，以及如何在提升资源利用率的同时保证高优先级作业的响应时间。

Contributions: 1) 提出并实现了在Kubernetes集群上运行Charm++应用的Operator；2) 设计并实现了一个基于优先级的弹性作业调度器，支持根据集群状态动态调整作业规模；3) 实验验证了该调度器在提升资源利用率和降低响应时间方面的有效性。

Results: 实验结果表明，所提出的弹性调度器在动态调整HPC作业时开销极低，相比传统静态调度器，在集群资源利用率和高优先级作业响应时间方面均有显著改进。

Conclusion: 本文展示了Charm++结合Kubernetes在云环境中运行HPC应用的可行性，并通过弹性调度器实现了高效的资源管理和性能优化，为云上HPC调度提供了新的解决方案。

Related Work: 相关工作包括MPI在云环境中的扩展支持、HPC作业调度器的设计、Kubernetes在高性能计算中的应用，以及Charm++的可迁移对象模型在动态负载均衡中的已有研究。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [5] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan是一种新型控制器，能够使大语言模型基础设施主动调节输出长度以应对系统负载变化，在H100 GPU测试平台上显著降低推理延迟并节省能耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用通常忽视底层基础设施状态，导致在高负载时推理延迟增加，影响用户体验。因此需要一种机制让基础设施能与应用协同调整行为。

Challenges: 如何在不牺牲生成质量的前提下，实现动态调节输出长度以应对系统负载波动；同时确保控制机制的实时性和有效性。

Contributions: 提出了beLLMan控制器，首次实现LLM基础设施向第一方应用反馈负载状态并动态调节输出长度；在真实H100 GPU测试环境中验证了其性能优势。

Results: 在拥塞期间，端到端延迟最高降低8倍，能耗减少25%，同时服务请求量提升19%。

Conclusion: beLLMan有效实现了LLM应用与基础设施之间的协同优化，显著提升了推理效率和资源利用率。

Related Work: 相关工作包括动态资源调度、LLM推理优化以及反馈控制机制在AI系统中的应用，但缺乏应用与基础设施间的直接输出调控机制。

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [6] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了在嵌入式AI系统开发中，本地与基于云的仿真环境之间的权衡，重点关注可扩展性与隐私之间的平衡，并提出通过优化计算基础设施设置来提升远程仿真的信任度和虚拟原型技术的采用。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统的快速发展和AI算法的日益复杂，亟需一种基于虚拟原型技术的高效硬件/软件协同设计方法，以应对开发中的性能与安全挑战。

Challenges: 主要挑战包括在仿真环境中平衡可扩展性与数据隐私、确保远程计算环境下的数据安全，以及适应多样化的模拟解决方案和基础设施选择。

Contributions: 本文系统分析了本地与云端仿真环境的优劣，提出了影响执行性能和数据安全的关键因素，并强调了高效仿真在嵌入式AI开发流程中的核心作用，旨在增强对远程仿真的信任。

Results: 研究揭示了不同基础设施配置对仿真性能和数据安全的影响，为开发者选择合适的仿真策略提供了指导，并推动了虚拟原型技术在行业中的更广泛应用。

Conclusion: 合理选择和配置本地或云-based仿真环境，能够在保障数据安全的同时实现良好的可扩展性，从而有效支持嵌入式AI系统的开发与优化。

Related Work: 相关工作集中在虚拟原型技术、硬件/软件协同设计、云计算在工程仿真中的应用以及嵌入式AI开发流程的自动化与优化等方面。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [7] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 本文研究了基于匹配的离散迭代负载均衡方法，提出了一种适用于任意图的通用局部均衡方案，通过匹配节点平均分配令牌，并在奇数情况下随机分配多余令牌。主要结果表明，在高概率下，该方法可在渐近匹配连续负载均衡谱界的时间内将负载差异降至3，优于以往工作，且适用于任意图而非仅限于正则图，证明了在所考虑的通用模型中，离散负载均衡不比连续负载均衡更难。


<details>
  <summary>Details</summary>
Motivation: 负载均衡在分布式系统中至关重要，现有研究多集中于连续或正则图场景，缺乏对任意图上离散均衡机制的有效分析与性能保证，因此需要一种更通用且高效的离散负载均衡方法。

Challenges: 主要挑战包括：在任意图上设计有效的局部均衡策略；处理离散令牌导致的负载无法完全均分的问题；分析离散过程的收敛速度并与其连续 counterpart 进行比较；实现小常数差异而非依赖大常数或非显式界限。

Contributions: 1. 提出并分析了一类适用于任意图的通用离散负载均衡方案，涵盖三种主流模型；2. 证明该方案在高概率下可在渐近最优时间内将差异降至3，首次在非正则图上实现小常数差异；3. 改进了先前工作，统一并简化了对不同模型的分析；4. 表明在所考虑模型中，离散均衡的难度不高于连续均衡。

Results: 主要结果为：在任意图上，该离散均衡方案以高概率在O(log(nΦ)/λ)轮内将差异降至3，其中Φ为初始负载差异，λ为图的谱间隙，该轮数与连续均衡的谱界一致，实现了紧的收敛界。

Conclusion: 本文展示了基于匹配的离散负载均衡在任意图上的高效性与理论可行性，通过精细分析实现了小常数差异的快速收敛，弥合了离散与连续均衡之间的理论差距，表明离散方法在一般模型中同样具有最优性能。

Related Work: 相关工作包括连续负载均衡中的谱图理论与扩散模型、基于匹配的离散均衡早期研究（如匹配模型与平衡电路模型）、异步负载均衡模型以及先前关于离散过程收敛性的分析，但多数结果局限于正则图或仅能保证较大的常数差异。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [8] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 本文提出了一种名为用户加权公平队列（UWFQ）的新型调度器，旨在解决Apache Spark在大规模数据处理中用户级公平性和平均响应时间之间的权衡问题。UWFQ通过模拟虚拟公平队列并基于作业预计完成时间进行调度，在保证用户间资源公平分配的同时显著降低作业响应时间。此外，引入运行时分区机制以应对任务倾斜和优先级反转问题。实验表明，与现有调度器相比，UWFQ可将小作业的平均响应时间减少高达74%。


<details>
  <summary>Details</summary>
Motivation: Spark内置的调度器（如FIFO和公平调度）在工业级分析环境中难以同时保证用户级别的公平性和低平均响应时间，尤其在长期运行的共享应用中表现不佳。现有方案多关注作业级公平性，导致提交更多作业的用户获得不公平优势，且缺乏对动态工作负载的适应能力。

Challenges: 主要挑战包括：1）实现用户级而非作业级的公平性；2）在动态工作负载下保持高性能；3）减少因任务倾斜导致的优先级反转；4）在不显著增加系统开销的前提下提升小作业响应速度。

Contributions: 1）提出UWFQ调度器，基于虚拟公平队列模型实现用户加权公平调度；2）引入运行时分区机制，动态调整任务粒度以缓解任务倾斜；3）在Spark框架内实现了该调度器，并通过合成多用户负载和Google集群轨迹验证其有效性。

Results: 实验结果显示，UWFQ相较于Spark内置调度器及最先进的公平调度算法，可将小作业的平均响应时间减少最多74%，同时有效改善用户间的资源分配公平性，并提升整体系统性能。

Conclusion: UWFQ在保持良好公平性的同时显著降低了作业响应时间，特别是在多用户共享的长期运行环境中表现出优越性能，是Spark调度机制的一个有效改进方案。

Related Work: 相关工作主要包括Spark的FIFO与公平调度器、传统的公平队列算法（如WFQ）、以及近年来提出的改进型调度策略（如Dominant Resource Fairness, Delay Scheduling等），但这些方法大多未考虑用户级公平性或缺乏对动态任务执行特征的适应能力。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [9] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文研究了分布式量子计算中的Lovász局部引理（LLL）问题，提出了在量子-LOCAL模型中LLL问题的复杂度下界为$2^{\Omega(\log^* n)}$，并针对sinkless orientation这一特例在更强的随机在线-LOCAL模型中获得了相同下界。这是首次在多种模型中为sinkless orientation和分布式LLL提供超常数下界，解决了近期提出的开放问题，并提出了一种新的下界证明技术，有望成为证明后量子时代局部性问题下界的通用工具。


<details>
  <summary>Details</summary>
Motivation: 近年来量子计算在多个顶会（如STOC'24、STOC'25）取得进展，分布式LLL问题成为研究热点。然而，在量子模型中仍缺乏对sinkless orientation等基本问题的超常数下界结果，若干开放问题亟待解决。

Challenges: 在量子-LOCAL及更强模型中建立LLL及其特例sinkless orientation的超常数下界；设计适用于后量子分布式计算的通用下界证明技术。

Contributions: 1. 给出了量子-LOCAL模型中分布式LLL的$2^{\Omega(\log^* n)}$复杂度下界；2. 在更强的随机在线-LOCAL模型中为sinkless orientation建立了相同下界；3. 将结果推广至多种计算模型；4. 提出一种全新的下界证明技术，具有成为通用工具的潜力。

Results: 获得了sinkless orientation和分布式LLL在多个模型中的首个超常数下界$2^{\Omega(\log^* n)}$，显著推进了对这些问题的复杂度理解。

Conclusion: 本工作解决了近期提出的开放问题，首次在多种模型中实现了对sinkless orientation和分布式LLL的超常数下界证明，并为后量子分布式计算复杂性研究提供了新的技术路径。

Related Work: 近期STOC'24与STOC'25上关于量子分布式计算的工作，特别是涉及Lovász局部引理与sinkless orientation的研究，构成了本文的主要相关工作。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [10] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: 本文提出了Funky，一种面向云原生应用的全栈FPGA感知编排引擎，解决了现有编排系统在FPGA虚拟化、隔离和抢占支持方面的不足。


<details>
  <summary>Details</summary>
Motivation: 由于FPGA缺乏虚拟化、隔离和抢占机制，且现有编排器以CPU为中心设计，导致FPGA在云原生环境中难以被有效编排，限制了其可扩展性、灵活性和容错性。

Challenges: 主要挑战包括实现轻量级FPGA虚拟化、支持任务抢占与检查点恢复、在保持性能的同时提供强隔离，并与现有云原生标准（如CRI/OCI）兼容。

Contributions: 1) 实现了轻量级沙箱的FPGA虚拟化；2) 提出FPGA状态管理机制，支持任务抢占和检查点；3) 设计了符合CRI/OCI标准的FPGA感知编排组件。

Results: 在四台搭载Alveo U50 FPGA卡的x86服务器上实现并评估Funky，仅需修改3.4%的源代码即可迁移23个OpenCL应用，OCI镜像大小比AMD的容器小28.7倍，性能开销仅为7.4%。大规模集群实验表明其具备良好的可扩展性、容错性和调度效率。

Conclusion: Funky为FPGA在云原生环境中的高效编排提供了完整解决方案，显著提升了FPGA资源的利用率和系统弹性，同时兼容工业标准，具有实际部署价值。

Related Work: 相关工作包括FPGA虚拟化技术（如部分重配置）、硬件资源管理、以及云环境中GPU/加速器的编排系统（如NVIDIA Kubernetes Device Plugin），但这些工作大多未深入结合云原生标准或缺乏对FPGA特性的全面支持。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Uno: A One-Stop Solution for Inter- and Intra-Datacenter Congestion Control and Reliable Connectivity](https://arxiv.org/abs/2510.15802)
*Tommaso Bonato,Sepehr Abdous,Abdul Kabbani,Ahmad Ghalayini,Nadeen Gebara,Terry Lam,Anup Agarwal,Tiancheng Chen,Zhuolong Yu,Konstantin Taranov,Mahmoud Elhaddad,Daniele De Sensi,Soudeh Ghorbani,Torsten Hoefler*

Main category: cs.NI

TL;DR: Uno是一个统一的系统，用于同时处理数据中心内和数据中心间的流量，通过集成快速拥塞响应、公平速率控制和结合纠删码与自适应路由的负载均衡方案，显著提升了两类流量的完成时间。


<details>
  <summary>Details</summary>
Motivation: 云计算和AI工作负载导致数据中心内外通信需求激增，但现有方案未能有效解决因往返时延差异导致的拥塞管理和流量调度问题，尤其是内部与跨数据中心流量之间的速率不公平和跨数据中心流量恢复缓慢的问题。

Challenges: 1. 数据中心内外流量共存且RTT差异大，导致拥塞管理复杂；2. 内部流量响应快，与跨数据中心流量竞争时造成速率不公平；3. 跨数据中心流量损失恢复慢，需要高可靠性；4. 现有方案采用分离的控制机制，缺乏统一性。

Contributions: 1. 提出Uno，首个统一管理数据中心内外流量的系统；2. 设计支持快速拥塞响应和公平速率控制的传输协议；3. 结合纠删码与自适应路由的负载均衡机制，提升可靠性和效率。

Results: 实验结果表明，与Gemini等最先进方法相比，Uno显著缩短了数据中心内外流量的流完成时间。

Conclusion: Uno通过统一架构有效解决了数据中心内外流量在拥塞控制和路由上的关键挑战，实现了更高效、公平和可靠的通信。

Related Work: 现有研究通常将数据中心内（如DCTCP）和跨数据中心（如SWIFT）的拥塞控制分开处理，或在不同粒度上进行优化，缺乏统一框架来协调两者，导致性能瓶颈和资源利用不均。

Abstract: Cloud computing and AI workloads are driving unprecedented demand for
efficient communication within and across datacenters. However, the coexistence
of intra- and inter-datacenter traffic within datacenters plus the disparity
between the RTTs of intra- and inter-datacenter networks complicates congestion
management and traffic routing. Particularly, faster congestion responses of
intra-datacenter traffic causes rate unfairness when competing with slower
inter-datacenter flows. Additionally, inter-datacenter messages suffer from
slow loss recovery and, thus, require reliability. Existing solutions overlook
these challenges and handle inter- and intra-datacenter congestion with
separate control loops or at different granularities. We propose Uno, a unified
system for both inter- and intra-DC environments that integrates a transport
protocol for rapid congestion reaction and fair rate control with a load
balancing scheme that combines erasure coding and adaptive routing. Our
findings show that Uno significantly improves the completion times of both
inter- and intra-DC flows compared to state-of-the-art methods such as Gemini.

</details>
