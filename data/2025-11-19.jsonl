{"id": "2511.11895", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11895", "abs": "https://arxiv.org/abs/2511.11895", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing", "comment": "9 pages, 8 figures. this is the preprint version of the paper accepted for publication at ICCAD 2025", "summary": "This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments."}
{"id": "2511.11917", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11917", "abs": "https://arxiv.org/abs/2511.11917", "authors": ["Thorben Schey", "Khaled Karoonlatifi", "Michael Weyrich", "Andrey Morozov"], "title": "Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing", "comment": "6 pages, 5 figures, this is the preprint version of the paper accepted for publication at ATS 2025", "summary": "This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing."}
{"id": "2511.12035", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12035", "abs": "https://arxiv.org/abs/2511.12035", "authors": ["Wenxuan Miao", "Yulin Sun", "Aiyue Chen", "Jing Lin", "Yiwu Yao", "Yiming Gan", "Jieru Zhao", "Jingwen Leng", "Mingyi Guo", "Yu Feng"], "title": "TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space", "comment": null, "summary": "The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.\n  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\\% loss on VBench)."}
{"id": "2511.12152", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12152", "abs": "https://arxiv.org/abs/2511.12152", "authors": ["Jianyi Yu", "Yuxuan Wang", "Xiang Fu", "Fei Qiao", "Ying Wang", "Rui Yuan", "Liyuan Liu", "Cong Shi"], "title": "A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation", "comment": null, "summary": "Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications."}
{"id": "2511.11586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11586", "abs": "https://arxiv.org/abs/2511.11586", "authors": ["Ao Zhou", "Jianlei Yang", "Tong Qiao", "Yingjie Qi", "Xinming Wei", "Cenlin Duan", "Weisheng Zhao", "Chunming Hu"], "title": "ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments", "comment": "This paper is accepted by the Journal of IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "summary": "The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph."}
{"id": "2511.12070", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12070", "abs": "https://arxiv.org/abs/2511.12070", "authors": ["Rosario Napoli", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "A Bio-Inspired Leader-based Energy Management System for Drone Fleets", "comment": "Accepted at the 11th European Service-Oriented and Cloud Computing (ESOCC 2025)", "summary": "Drones are embedded systems (ES) used across a wide range of fields, from photography to shipments and even during crisis management for searching, rescuing and damage assessment activities. However, their limited battery life and high energy consumption are very important challenges, especially in networked systems where multiple drones must communicate with a Ground Base Station (GBS). This study addresses these limitations by proposing the implementation of a bio-inspired leader-based energy management system for drone fleets. Inspired by bio-behavioral models, the algorithm dynamically chooses a single drone as a Leader in a cluster to handle long-range communication with the GBS, allowing other drones to preserve their energy. The effectiveness of the proposed bio-inspired algorithm is evaluated by varying network sizes and configurations. The results demonstrate that our approach significantly increases network efficiency and service time by removing useless energy consumption communications."}
{"id": "2511.12286", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12286", "abs": "https://arxiv.org/abs/2511.12286", "authors": ["Khyati Kiyawat", "Zhenxing Fan", "Yasas Seneviratne", "Morteza Baradaran", "Akhil Shekar", "Zihan Xia", "Mingu Kang", "Kevin Skadron"], "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing", "comment": null, "summary": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively."}
{"id": "2511.11598", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11598", "abs": "https://arxiv.org/abs/2511.11598", "authors": ["Van-Vi Vo", "Tien-Dung Nguyen", "Duc-Tai Le", "Hyunseung Choo"], "title": "Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks", "comment": null, "summary": "Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols."}
{"id": "2511.12127", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12127", "abs": "https://arxiv.org/abs/2511.12127", "authors": ["Md Rahat Hasan", "Kazi Ahmed Akbar Munim", "Md. Forkan Uddin"], "title": "Joint Optimization of RU Allocation and C-SR in Multi-AP Coordinated Wi-Fi Systems", "comment": null, "summary": "We formulate an optimization problem for joint RU allocation and C-SR to maximize the throughput of a multi-AP coordinated WiFi system. The optimization problem is found to be a non-linear integer programming problem. We solve the problem for several network scenarios using an optimization tool. The joint design significantly improves throughput compared to a non-coordinated system. To reduce computational complexity, we also provide a heuristic solution to the problem. The proposed heuristic achieves throughput comparable to that of the computationally expensive optimization tool based solution approach."}
{"id": "2511.12349", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12349", "abs": "https://arxiv.org/abs/2511.12349", "authors": ["Divya Kiran Kadiyala", "Alexandros Daglis"], "title": "Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting", "comment": null, "summary": "The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.\n  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x."}
{"id": "2511.11601", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11601", "abs": "https://arxiv.org/abs/2511.11601", "authors": ["Elliott Wen", "Sean Ma", "Ewan Tempero", "Jens Dietrich", "Daniel Luo", "Jiaxing Shen", "Kaiqi Zhao", "Bruce Sham", "Yousong Song", "Jiayi Hua", "Jia Hong"], "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators", "comment": null, "summary": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem."}
{"id": "2511.12501", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12501", "abs": "https://arxiv.org/abs/2511.12501", "authors": ["Jianhang Yao", "Hui Kang", "Geng Sun", "Jiahui Li", "Hongjuan Li", "Jiacheng Wang", "Yinqiu Liu", "Dusit Niyato"], "title": "Collaborative Charging Optimization for Wireless Rechargeable Sensor Networks via Heterogeneous Mobile Chargers", "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Vehicular Technology", "summary": "Despite the rapid proliferation of Internet of Things applications driving widespread wireless sensor network (WSN) deployment, traditional WSNs remain fundamentally constrained by persistent energy limitations that severely restrict network lifetime and operational sustainability. Wireless rechargeable sensor networks (WRSNs) integrated with wireless power transfer (WPT) technology emerge as a transformative paradigm, theoretically enabling unlimited operational lifetime. In this paper, we investigate a heterogeneous mobile charging architecture that strategically combines automated aerial vehicles (AAVs) and ground smart vehicles (SVs) in complex terrain scenarios to collaboratively exploit the superior mobility of AAVs and extended endurance of SVs for optimal energy distribution. We formulate a multi-objective optimization problem that simultaneously addresses the dynamic balance of heterogeneous charger advantages, charging efficiency versus mobility energy consumption trade-offs, and real-time adaptive coordination under time-varying network conditions. This problem presents significant computational challenges due to its high-dimensional continuous action space, non-convex optimization landscape, and dynamic environmental constraints. To address these challenges, we propose the improved heterogeneous agent trust region policy optimization (IHATRPO) algorithm that integrates a self-attention mechanism for enhanced complex environmental state processing and employs a Beta sampling strategy to achieve unbiased gradient computation in continuous action spaces. Comprehensive simulation results demonstrate that IHATRPO achieves a 39% performance improvement over the original HATRPO, significantly outperforming state-of-the-art baseline algorithms while substantially increasing sensor node survival rate and charging system efficiency."}
{"id": "2511.12544", "categories": ["cs.AR", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12544", "abs": "https://arxiv.org/abs/2511.12544", "authors": ["Mukul Lokhande", "Akash Sankhe", "S. V. Jaya Chand", "Santosh Kumar Vishvakarma"], "title": "FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration", "comment": null, "summary": "The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads."}
{"id": "2511.11603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11603", "abs": "https://arxiv.org/abs/2511.11603", "authors": ["Deep Bodra", "Sushil Khairnar"], "title": "Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review", "comment": null, "summary": "Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments."}
{"id": "2511.12772", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12772", "abs": "https://arxiv.org/abs/2511.12772", "authors": ["Stephan Nef", "Bruno Rodrigues"], "title": "CareNet: Linking Home-router Network Traffic to DSM-5 Depressive Behavior Indicators", "comment": null, "summary": "Digital mental-health sensing increasingly depends on mobile or wearable devices that require intrusive permissions and continuous user compliance. We present CareNet, a router-centric system that transforms household network metadata into interpretable behavioral indicators aligned with DSM-5 depressive-symptom domains. All processing occurs locally at the home gateway, preserving privacy while maintaining visibility of temporal routines.\n  The core contribution is the Fuzzy Additive Symptom Likelihood (FASL), a transparent formulation that fuses header-level metrics into daily criterion-level likelihoods using bounded fuzzy memberships and additive aggregation. Combined with a DSM-style temporal gate, FASL integrates short-term traffic fluctuations into persistent, clinically interpretable indicators. Evaluation on realistic multi-day traces shows that CareNet captures characteristic patterns such as delayed sleep timing and attentional instability without payload inspection. The results highlight the feasibility of reproducible, explainable behavioral inference from router-side telemetry."}
{"id": "2511.12616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12616", "abs": "https://arxiv.org/abs/2511.12616", "authors": ["Arya Parameshwara"], "title": "SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration", "comment": "10 pages, 7 figures, conference-style formatting", "summary": "This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures."}
{"id": "2511.11605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11605", "abs": "https://arxiv.org/abs/2511.11605", "authors": ["David Balaban", "Adrian Miclăuş"], "title": "PACE Solver Description: twin_width_fmi", "comment": null, "summary": "In this paper we present \\texttt{twin\\_width\\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.\n  As a baseline, we implement \\texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.\n  Our best-performing component, which we ultimately submitted, is \\texttt{hedom5}. The design of \\texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \\texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination."}
{"id": "2511.12774", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12774", "abs": "https://arxiv.org/abs/2511.12774", "authors": ["Karim Khamaisi", "Pascal Kiechl", "Katharina Müller", "Burkhard Stiller", "Bruno Rodrigues"], "title": "Distributed Pulse-Wave Simulator for DDoS Dataset Generation", "comment": null, "summary": "Pulse-wave Distributed Denial-of-Service (DDoS) attacks generate short, synchronized bursts of traffic that circumvent pattern-based detection and quickly exhaust traditional defense systems. This transient and spatially distributed behavior makes analysis extremely challenging, as no public datasets capture how such attacks evolve across multiple network domains. Since each domain observes only a partial viewpoint of the attack, a correlated, multi-vantage view is essential for comprehensive analysis, early detection, and attribution.\n  This paper presents DPWS, an open-source simulator for generating distributed pulse-wave DDoS datasets. DPWS models multi-AS topologies and produces synchronized packet captures at multiple autonomous systems, showing the distributed structure of coordinated bursts. It enables fine-grained control of traffic parameters through a lightweight YAML interface. DPWS reproduces pulse-wave dynamics across multiple vantage points, exhibits natural fingerprint variability at equal aggregate rates, and scales with MPI in ns-3, providing a reproducible basis for studying pulse-wave behaviour and benchmarking distributed DDoS defenses, while sharing practical insights on ns-3 scalability and synchronization gained during development."}
{"id": "2511.12860", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.12860", "abs": "https://arxiv.org/abs/2511.12860", "authors": ["Yongjoo Jang", "Sangwoo Hwang", "Hojin Lee", "Sangwoo Jung", "Donghun Lee", "Wonbo Shim", "Jaeha Kung"], "title": "Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs", "comment": "This paper is accepted in the 43rd IEEE International Conference on Computer Design (ICCD), 2025", "summary": "The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead."}
{"id": "2511.11608", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11608", "abs": "https://arxiv.org/abs/2511.11608", "authors": ["Mingyu Sung", "Suhwan Im", "Daeho Bang", "Il-Min Kim", "Sangseok Yun", "Jae-Mo Kang"], "title": "Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression", "comment": null, "summary": "Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material."}
{"id": "2511.12888", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12888", "abs": "https://arxiv.org/abs/2511.12888", "authors": ["Amelia Samandari", "Andreas Willig", "Barry Wu", "Philippa Martin"], "title": "Distributed Self-allocated Time Slot Reuse: Multi-hop Communication in Rigid UAV Formations", "comment": null, "summary": "Deployment of Unmanned Aerial Vehicles (UAVs) in autonomous formations necessitates accurate and timely communication of safety information. A communication protocol that supports timely and successful transfer of safety information between UAVs is therefore needed. This paper presents Distributed Self-allocated Time slot Reuse (D-STR). Our D-STR protocol addresses the essential task of communicating safety information in rigid Unmanned Aerial Vehicle (UAV) formations with different network topologies, enabling collision-free deployment of the formation. This is an important step for improving the safety and practicality of UAV formations in application scenarios that span a range of industries."}
{"id": "2511.12930", "categories": ["cs.AR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12930", "abs": "https://arxiv.org/abs/2511.12930", "authors": ["Changhun Oh", "Seongryong Oh", "Jinwoo Hwang", "Yoonsung Kim", "Hardik Sharma", "Jongse Park"], "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration", "comment": null, "summary": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical."}
{"id": "2511.11612", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11612", "abs": "https://arxiv.org/abs/2511.11612", "authors": ["Aasish Kumar Sharma", "Julian Kunkel"], "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems", "comment": "14 pages, 4 figures, 2 tables. Evaluation study on LLM-based reasoning for HPC scheduling. Published in Research in Academic Engineering Journal (RAEJ), 2025", "summary": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers."}
{"id": "2511.11621", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.11621", "abs": "https://arxiv.org/abs/2511.11621", "authors": ["Pedro Antunes", "Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Luís Frazão", "Nuno Costa", "António Pereira"], "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs", "comment": null, "summary": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs."}
{"id": "2511.13139", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13139", "abs": "https://arxiv.org/abs/2511.13139", "authors": ["Zhiteng Chao", "Yonghao Wang", "Xinyu Zhang", "Jiaxin Zhou", "Tenghui Hua", "Husheng Han", "Tianmeng Yang", "Jianan Mu", "Bei Yu", "Rui Zhang", "Jing Ye", "Huawei Li"], "title": "Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT", "comment": "6 pages, 5 figures", "summary": "Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability."}
{"id": "2511.11614", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11614", "abs": "https://arxiv.org/abs/2511.11614", "authors": ["Arturo Urías Jiménez"], "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI", "comment": null, "summary": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.\n  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design."}
{"id": "2511.13343", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13343", "abs": "https://arxiv.org/abs/2511.13343", "authors": ["A Cormier", "David Roqui", "Fabrice Surma", "Martin Labouré", "Jean-Marc Vallet", "Odile Guillon", "N Grozavu", "Ann Bourgès"], "title": "Coliseum project: Correlating climate change data with the behavior of heritage materials", "comment": null, "summary": "Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site."}
{"id": "2511.11617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11617", "abs": "https://arxiv.org/abs/2511.11617", "authors": ["Wendong Xu", "Chujie Chen", "He Xiao", "Kuan Li", "Jing Xiong", "Chen Zhang", "Wenyong Zhou", "Chaofan Tao", "Yang Bai", "Bei Yu", "Ngai Wong"], "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism", "comment": "accpeted paper by Design, Automation and Test in Europe Conference (DATE'26). 8 pages in total with 6 figures and 2 tables", "summary": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload."}
{"id": "2511.13676", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13676", "abs": "https://arxiv.org/abs/2511.13676", "authors": ["Hyunwoo Oh", "KyungIn Nam", "Rajat Bhattacharjya", "Hanning Chen", "Tamoghno Das", "Sanggeon Yun", "Suyeon Jang", "Andrew Ding", "Nikil Dutt", "Mohsen Imani"], "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization", "comment": "Accepted to DATE 2026", "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms."}
{"id": "2511.11619", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11619", "abs": "https://arxiv.org/abs/2511.11619", "authors": ["Yuanjie Liu", "Wenpeng Xing", "Ye Zhou", "Gaowei Chang", "Changting Lin", "Meng Han"], "title": "DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack", "comment": null, "summary": "The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.\n  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.\n  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.\n  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies."}
{"id": "2511.13679", "categories": ["cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13679", "abs": "https://arxiv.org/abs/2511.13679", "authors": ["Hyunwoo Oh", "Hanning Chen", "Sanggeon Yun", "Yang Ni", "Wenjun Huang", "Tamoghno Das", "Suyeon Jang", "Mohsen Imani"], "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention", "comment": "Accepted to DATE 2026", "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups."}
{"id": "2511.11621", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.11621", "abs": "https://arxiv.org/abs/2511.11621", "authors": ["Pedro Antunes", "Ana Rita Ortigoso", "Gabriel Vieira", "Daniel Fuentes", "Luís Frazão", "Nuno Costa", "António Pereira"], "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs", "comment": null, "summary": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs."}
{"id": "2511.11640", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11640", "abs": "https://arxiv.org/abs/2511.11640", "authors": ["Sed Centeno", "Christopher Sprague", "Arnab A Purkayastha", "Ray Simar", "Neeraj Magotra"], "title": "Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications", "comment": "5 pages", "summary": "Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes."}
{"id": "2511.11624", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11624", "abs": "https://arxiv.org/abs/2511.11624", "authors": ["Md Romyull Islam", "Bobin Deng", "Nobel Dhar", "Tu N. Nguyen", "Selena He", "Yong Shi", "Kun Suo"], "title": "Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges", "comment": "Submitted version; 9 pages, 5 figures; presented at IEEE MASS 2025 (online publication pending)", "summary": "Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments."}
{"id": "2511.11628", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11628", "abs": "https://arxiv.org/abs/2511.11628", "authors": ["Xinbo Wang", "Shian Jia", "Ziyang Huang", "Jing Cao", "Mingli Song"], "title": "Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies", "comment": null, "summary": "Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This \"one-policy-fits-all\" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.\n  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable \"expert\" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.\n  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers."}
{"id": "2511.11640", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11640", "abs": "https://arxiv.org/abs/2511.11640", "authors": ["Sed Centeno", "Christopher Sprague", "Arnab A Purkayastha", "Ray Simar", "Neeraj Magotra"], "title": "Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications", "comment": "5 pages", "summary": "Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes."}
{"id": "2511.11660", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11660", "abs": "https://arxiv.org/abs/2511.11660", "authors": ["Zizheng Guo", "Haichuan Liu", "Xizhe Shi", "Shenglu Hua", "Zuodong Zhang", "Chunyuan Zhao", "Runsheng Wang", "Yibo Lin"], "title": "HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support", "comment": "7 pages, 3 figures, to be published in ASP-DAC 2026", "summary": "We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality."}
{"id": "2511.11664", "categories": ["cs.DC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11664", "abs": "https://arxiv.org/abs/2511.11664", "authors": ["Mingyu Sung", "Suhwan Im", "Vikas Palakonda", "Jae-Mo Kang"], "title": "Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks", "comment": null, "summary": "Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance."}
{"id": "2511.11672", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11672", "abs": "https://arxiv.org/abs/2511.11672", "authors": ["Zengyi Qin", "Jinyuan Chen", "Yunze Man", "Shengcao Cao", "Ziqi Pang", "Zhuoyuan Wang", "Xin Sun", "Gen Lin", "Han Fang", "Ling Zhu", "Zixin Xie", "Zibu Wei", "Tianshu Ran", "Haoran Geng", "Xander Wu", "Zachary Bright", "Qizhen Sun", "Rui Wang", "Yuyang Cai", "Song Wang", "Jiace Zhao", "Han Cao", "Yeyang Zhou", "Tianrui Liu", "Ray Pan", "Chongye Yang", "Xiang Ren", "Bo Zhang", "Yutong Ban", "Jitendra Malik", "Brian Anthony", "Pieter Abbeel"], "title": "OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents", "comment": null, "summary": "We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research."}
{"id": "2511.11678", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11678", "abs": "https://arxiv.org/abs/2511.11678", "authors": ["Yuze Liu", "Yunhan Wang", "Tiehua Zhang", "Zhishu Shen", "Cheng Peng", "Libing Wu", "Feng Xia", "Jiong Jin"], "title": "A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems", "comment": null, "summary": "The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM."}
{"id": "2511.11719", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11719", "abs": "https://arxiv.org/abs/2511.11719", "authors": ["Mohammad Mahdi Kamani", "Zhongwei Cheng", "Lin Chen"], "title": "ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation", "comment": null, "summary": "The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework."}
{"id": "2511.11721", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11721", "abs": "https://arxiv.org/abs/2511.11721", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems", "comment": null, "summary": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms."}
{"id": "2511.11729", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11729", "abs": "https://arxiv.org/abs/2511.11729", "authors": ["Ao Xu", "Han Zhao", "Weihao Cui", "Quan Chen", "Yukang Chen", "Shulai Zhang", "Shuang Chen", "Jiemin Jiang", "Zhibin Yu", "Minyi Guo"], "title": "Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.\n  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode."}
{"id": "2511.11733", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11733", "abs": "https://arxiv.org/abs/2511.11733", "authors": ["Jingwei Song", "Wanyi Chen", "Xinyuan Song", "Max", "Chris Tong", "Gufeng Chen", "Tianyi Zhao", "Eric Yang", "Bill Shi", "Lynn Ai"], "title": "Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput", "comment": "6 pages, 2 figures, 2 tables. Uses ICML 2025 style", "summary": "Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes."}
{"id": "2511.11739", "categories": ["cs.DC", "cond-mat.mtrl-sci", "cs.LG", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.11739", "abs": "https://arxiv.org/abs/2511.11739", "authors": ["Christina Schenk", "Miguel Hernández-del-Valle", "Luis Calero-Lumbreras", "Marcus Noack", "Maciej Haranczyk"], "title": "Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows", "comment": "17 pages, 4 figures, 2 tables", "summary": "Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms."}
{"id": "2511.11749", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11749", "abs": "https://arxiv.org/abs/2511.11749", "authors": ["Almond Kiruthu Murimi"], "title": "How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems", "comment": "9 pages, 4 tables, seminar paper", "summary": "This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems."}
{"id": "2511.11843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11843", "abs": "https://arxiv.org/abs/2511.11843", "authors": ["Yiwei Zhao", "Qiushi Lin", "Hongbo Kang", "Guy E. Blelloch", "Laxman Dhulipala", "Charles McGuffey", "Phillip B. Gibbons"], "title": "TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing", "comment": null, "summary": "In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing."}
{"id": "2511.11885", "categories": ["cs.DC", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11885", "abs": "https://arxiv.org/abs/2511.11885", "authors": ["Kausar Patherya", "Ashutosh Dhekne", "Francisco Romero"], "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs", "comment": "12 pages, 5 figures. Under review", "summary": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.\n  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing."}
{"id": "2511.11907", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11907", "abs": "https://arxiv.org/abs/2511.11907", "authors": ["Huawei Zhang", "Chunwei Xia", "Zheng Wang"], "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference", "comment": null, "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.\n  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes."}
{"id": "2511.12009", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12009", "abs": "https://arxiv.org/abs/2511.12009", "authors": ["Guangchao Yao", "Yali Li"], "title": "High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts", "comment": null, "summary": "The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem."}
{"id": "2511.12025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12025", "abs": "https://arxiv.org/abs/2511.12025", "authors": ["Ivan Cao", "Jaromir J. Saloni", "David A. G. Harrison"], "title": "A Quick and Exact Method for Distributed Quantile Computation", "comment": "10 pages, 2 figures. Draft version for testing and feedback", "summary": "Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster."}
{"id": "2511.12031", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12031", "abs": "https://arxiv.org/abs/2511.12031", "authors": ["Arun Ramachandran", "Ramaswamy Govindarajan", "Murali Annavaram", "Prakash Raghavendra", "Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang"], "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding", "comment": null, "summary": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs."}
{"id": "2511.12185", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12185", "abs": "https://arxiv.org/abs/2511.12185", "authors": ["Mills Staylor", "Arup Kumar Sarker", "Gregor von Laszewski", "Geoffrey Fox", "Yue Cheng", "Judy Fox"], "title": "Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications", "comment": "12 pages, 9 figures, 3 tables", "summary": "Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching."}
{"id": "2511.12216", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12216", "abs": "https://arxiv.org/abs/2511.12216", "authors": ["Van Ho-Long", "Nguyen Ho", "Anh-Vu Dinh-Duc", "Ha Manh Tran", "Ky Trung Nguyen", "Tran Dung Pham", "Quoc Viet Hung Nguyen"], "title": "Distributed Seasonal Temporal Pattern Mining", "comment": null, "summary": "The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \\textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets."}
{"id": "2511.12461", "categories": ["cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12461", "abs": "https://arxiv.org/abs/2511.12461", "authors": ["Fangqiang Du", "Sixuan Chong", "Zixuan Huang", "Rui Qin", "Fengnan Mi", "Caibao Hu", "Jiangang Chen"], "title": "Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA", "comment": null, "summary": "Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times."}
{"id": "2511.12486", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12486", "abs": "https://arxiv.org/abs/2511.12486", "authors": ["Duneesha Fernando", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "A Decentralized Root Cause Localization Approach for Edge Computing Environments", "comment": null, "summary": "Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments."}
{"id": "2511.12500", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12500", "abs": "https://arxiv.org/abs/2511.12500", "authors": ["Muhammad Awad", "Muhammad Osama", "Brandon Potter"], "title": "Iris: First-Class Multi-GPU Programming Experience in Triton", "comment": null, "summary": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming."}
{"id": "2511.12667", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12667", "abs": "https://arxiv.org/abs/2511.12667", "authors": ["Sepideh Masoudi", "Mark Edward Michael Daly", "Jannis Kiesel"], "title": "Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines", "comment": null, "summary": "As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures."}
{"id": "2511.12687", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.12687", "abs": "https://arxiv.org/abs/2511.12687", "authors": ["Partha S. Dey", "Aditya S. Gopalan", "Vijay G. Subramanian"], "title": "The Time to Consensus in a Blockchain: Insights into Bitcoin's \"6 Blocks Rule''", "comment": null, "summary": "We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \\emph{honest} and \\emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \\emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation."}
{"id": "2511.13155", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13155", "abs": "https://arxiv.org/abs/2511.13155", "authors": ["Jonathan Bader", "Julius Irion", "Jannis Kappel", "Joel Witzke", "Niklas Fomin", "Diellza Sherifi", "Odej Kao"], "title": "Learning Process Energy Profiles from Node-Level Power Data", "comment": null, "summary": "The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions."}
{"id": "2511.13253", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13253", "abs": "https://arxiv.org/abs/2511.13253", "authors": ["Mordechai Guri"], "title": "Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices", "comment": null, "summary": "This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge."}
{"id": "2511.13313", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13313", "abs": "https://arxiv.org/abs/2511.13313", "authors": ["Sulaiman Muhammad Rashid", "Ibrahim Aliyu", "Jaehyung Park", "Jinsul Kim"], "title": "Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems", "comment": null, "summary": "The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources."}
