{"id": "2511.04682", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04682", "abs": "https://arxiv.org/abs/2511.04682", "authors": ["Eleni Bougioukou", "Theodore Antonakopoulos"], "title": "Efficient Deployment of CNN Models on Multiple In-Memory Computing Units", "comment": "5 pages, 4 figures, 2025 14th International Conference on Modern\n  Circuits and Systems Technologies (MOCAST)", "summary": "In-Memory Computing (IMC) represents a paradigm shift in deep learning\nacceleration by mitigating data movement bottlenecks and leveraging the\ninherent parallelism of memory-based computations. The efficient deployment of\nConvolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use\nof advanced task allocation strategies for achieving maximum computational\nefficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple\nProcessing Units (PUs) for investigating how the deployment of a CNN model in a\nmulti-processing system affects its performance, in terms of processing rate\nand latency. For that purpose, we introduce the Load-Balance-Longest-Path\n(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE\nPUs, for maximizing the processing rate and minimizing latency due to efficient\nresources utilization. We are benchmarking LBLP against other alternative\nscheduling strategies for a number of CNN models and experimental results\ndemonstrate the effectiveness of the proposed algorithm."}
{"id": "2511.04684", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04684", "abs": "https://arxiv.org/abs/2511.04684", "authors": ["Yuchao Qin", "Anjunyi Fan", "Bonan Yan"], "title": "RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression", "comment": "5 pages, 4 figures", "summary": "Data centers handle vast volumes of data that require efficient lossless\ncompression, yet emerging probabilistic models based methods are often\ncomputationally slow. To address this, we introduce RAS, the Range Asymmetric\nNumeral System Acceleration System, a hardware architecture that integrates the\nrANS algorithm into a lossless compression pipeline and eliminates key\nbottlenecks. RAS couples an rANS core with a probabilistic generator, storing\ndistributions in BF16 format and converting them once into a fixed-point domain\nshared by a unified division/modulo datapath. A two-stage rANS update with\nbyte-level re-normalization reduces logic cost and memory traffic, while a\nprediction-guided decoding path speculatively narrows the cumulative\ndistribution function (CDF) search window and safely falls back to maintain\nbit-exactness. A multi-lane organization scales throughput and enables\nfine-grained clock gating for efficient scheduling. On image workloads, our\nRTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a\nPython rANS baseline, reducing average decoder binary-search steps from 7.00 to\n3.15 (approximately 55% fewer). When paired with neural probability models, RAS\nsustains higher compression ratios than classical codecs and outperforms\nCPU/GPU rANS implementations, offering a practical approach to fast neural\nlossless compression."}
{"id": "2511.04687", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04687", "abs": "https://arxiv.org/abs/2511.04687", "authors": ["Teona Bagashvili", "Tarikul Islam Papon", "Subhadeep Sarkar", "Manos Athanassoulis"], "title": "Eliminating the Hidden Cost of Zone Management in ZNS SSDs", "comment": null, "summary": "Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput\nand low-latency storage by eliminating device-side garbage collection. They\nexpose storage as append-only zones that give the host applications direct\ncontrol over data placement. However, current ZNS implementations suffer from\n(a) device-level write amplification (DLWA), (b) increased wear, and (c)\ninterference with host I/O due to zone mapping and management. We identify two\nprimary design decisions as the main cause: (i) fixed physical zones and (ii)\nfull-zone operations that lead to excessive physical writes. We propose\nSilentZNS, a new zone mapping and management approach that addresses the\naforementioned limitations by on-the-fly allocating available resources to\nzones, while minimizing wear, maintaining parallelism, and avoiding unnecessary\nwrites at the device-level. SilentZNS is a flexible zone allocation scheme that\ndeparts from the traditional logical-to-physical zone mapping and allows for\narbitrary collections of blocks to be assigned to a zone. We add the necessary\nconstraints to ensure wear-leveling and state-of-the-art read performance, and\nuse only the required blocks to avoid dummy writes during zone reset. We\nimplement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that\nit eliminates the undue burden of dummy writes by up to 20x, leading to lower\nDLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up\nto 3.7x faster workload execution."}
{"id": "2511.04713", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.04713", "abs": "https://arxiv.org/abs/2511.04713", "authors": ["Mahek Desai", "Rowena Quinn", "Marjan Asadinia"], "title": "SMART-WRITE: Adaptive Learning-based Write Energy Optimization for Phase Change Memory", "comment": null, "summary": "As dynamic random access memory (DRAM) and other current transistor-based\nmemories approach their scalability limits, the search for alternative storage\nmethods becomes increasingly urgent. Phase-change memory (PCM) emerges as a\npromising candidate due to its scalability, fast access time, and zero leakage\npower compared to many existing memory technologies. However, PCM has\nsignificant drawbacks that currently hinder its viability as a replacement. PCM\ncells suffer from a limited lifespan because write operations degrade the\nphysical material, and these operations consume a considerable amount of\nenergy. For PCM to be a practical option for data storage-which involves\nfrequent write operations-its cell endurance must be enhanced, and write energy\nmust be reduced. In this paper, we propose SMART-WRITE, a method that\nintegrates neural networks (NN) and reinforcement learning (RL) to dynamically\noptimize write energy and improve performance. The NN model monitors real-time\noperating conditions and device characteristics to determine optimal write\nparameters, while the RL model dynamically adjusts these parameters to further\noptimize PCM's energy consumption. By continuously adjusting PCM write\nparameters based on real-time system conditions, SMART-WRITE reduces write\nenergy consumption by up to 63% and improves performance by up to 51% compared\nto the baseline and previous models."}
{"id": "2511.05022", "categories": ["cs.NI", "H.3.3; H.3.4; H.m"], "pdf": "https://arxiv.org/pdf/2511.05022", "abs": "https://arxiv.org/abs/2511.05022", "authors": ["Charles Melvin", "N. Rich Nguyen"], "title": "AWARE: Evaluating PriorityFresh Caching for Offline Emergency Warning Systems", "comment": "Preprint version", "summary": "PriorityFresh is a semantic, actionability-first caching policy designed for\noffline emergency warning systems. Within the AWARE system's simulation\nenvironment, PriorityFresh optimizes which alerts to retain and surface under\nconstrained connectivity. Experiments indicate improved actionability-first\nperformance without harming efficiency. A separate Priority Forecasting model\nis used only to synthesize realistic alert sequences for controlled experiments\nand does not influence caching or push decisions."}
{"id": "2511.04853", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04853", "abs": "https://arxiv.org/abs/2511.04853", "authors": ["Nuno dos Santos Fernandes", "Pedro Tomás", "Nuno Roma", "Frank Winklmeier", "Patricia Conde-Muíño"], "title": "Marionette: Data Structure Description and Management for Heterogeneous Computing", "comment": "5 pages, 2 figures. To be published as a short paper accepted by the\n  24th International Symposium on Parallel and Distributed Computing (ISPDC)", "summary": "Adapting large, object-oriented C++ codebases for hardware acceleration might\nbe extremely challenging, particularly when targeting heterogeneous platforms\nsuch as GPUs. Marionette is a C++17 library designed to address this by\nenabling flexible, efficient, and portable data structure definitions. It\ndecouples data layout from the description of the interface, supports multiple\nmemory management strategies, and provides efficient data transfers and\nconversions across devices, all of this with minimal runtime overhead due to\nthe compile-time nature of its abstractions. By allowing interfaces to be\naugmented with arbitrary functions, Marionette maintains compatibility with\nexisting code and offers a streamlined interface that supports both\nstraightforward and advanced use cases. This paper outlines its design, usage,\nand performance, including a CUDA-based case study demonstrating its efficiency\nand flexibility."}
{"id": "2511.04798", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04798", "abs": "https://arxiv.org/abs/2511.04798", "authors": ["Matheus Farias", "Wanghley Martins", "H. T. Kung"], "title": "MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars", "comment": "5 pages, 6 figures", "summary": "Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)\nweight mapping technique for memristive bit-sliced compute-in-memory (CIM)\ncrossbars that reduces parasitic resistance (PR) nonidealities.\n  PR limits crossbar efficiency by mapping DNN matrices into small crossbar\ntiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring\ndigital synchronization before the next layer. At this granularity, designers\neither deploy many small crossbars in parallel or reuse a few sequentially-both\nincreasing analog-to-digital conversions, latency, I/O pressure, and chip area.\n  MDM alleviates PR effects by optimizing active-memristor placement.\nExploiting bit-level structured sparsity, it feeds activations from the denser\nlow-order side and reorders rows according to the Manhattan distance,\nrelocating active cells toward regions less affected by PR and thus lowering\nthe nonideality factor (NF).\n  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and\nimproves accuracy under analog distortion by an average of 3.6% in ResNets.\nOverall, it provides a lightweight, spatially informed method for scaling CIM\nDNN accelerators."}
{"id": "2511.05027", "categories": ["cs.NI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.05027", "abs": "https://arxiv.org/abs/2511.05027", "authors": ["Zhuoling Chen", "Yi Zhong", "Martin Haenggi"], "title": "Cross-link RTS/CTS for MLO mm-Wave WLANs", "comment": "13 pages, 13 figures", "summary": "The directional RTS/CTS mechanism of mm-wave Wi-Fi hardly resolves the hidden\nterminal problem perfectly.This paper proposes cross-link RTS/CTS under\nmulti-link operation (MLO) to address this problem and introduces a novel point\nprocess, named the generalized RTS/CTS hard-core process (G-HCP), to model the\nspatial transceiver relationships under the RTS/CTS mechanism, including the\ndirectional case and the omnidirectional case.Analytical expressions are\nderived for the intensity, the mean interference, an approximation of the\nsuccess probability, and the expected number of hidden nodes for the\ndirectional RTS/CTS mechanism.Theoretical and numerical results demonstrate the\nperformance difference between two RTS/CTS mechanisms.The cross-link RTS/CTS\nmechanism ensures higher link quality at the cost of reduced network\nthroughput.In contrast, the directional RTS/CTS sacrifices the link quality for\nhigher throughput.Our study reveals a fundamental trade-off between link\nreliability and network throughput, providing critical insights into the\nselection and optimization of RTS/CTS mechanisms in next-generation WLAN\nstandards."}
{"id": "2511.05053", "categories": ["cs.DC", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.05053", "abs": "https://arxiv.org/abs/2511.05053", "authors": ["Wakuto Matsumi", "Riaz-Ul-Haque Mian"], "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs", "comment": null, "summary": "Machine learning based on neural networks has advanced rapidly, but the high\nenergy consumption required for training and inference remains a major\nchallenge. Hyperdimensional Computing (HDC) offers a lightweight,\nbrain-inspired alternative that enables high parallelism but often suffers from\nlower accuracy on complex visual tasks. To overcome this, hybrid accelerators\ncombining HDC and Convolutional Neural Networks (CNNs) have been proposed,\nthough their adoption is limited by poor generalizability and programmability.\nThe rise of open-source RISC-V architectures has created new opportunities for\ndomain-specific GPU design. Unlike traditional proprietary GPUs, emerging\nRISC-V-based GPUs provide flexible, programmable platforms suitable for custom\ncomputation models such as HDC. In this study, we design and implement custom\nGPU instructions optimized for HDC operations, enabling efficient processing\nfor hybrid HDC-CNN workloads. Experimental results using four types of custom\nHDC instructions show a performance improvement of up to 56.2 times in\nmicrobenchmark tests, demonstrating the potential of RISC-V GPUs for\nenergy-efficient, high-performance computing."}
{"id": "2511.05321", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.05321", "abs": "https://arxiv.org/abs/2511.05321", "authors": ["Maximilian Kirschner", "Konstantin Dudzik", "Ben Krusekamp", "Jürgen Becker"], "title": "MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference", "comment": null, "summary": "Real-time systems, particularly those used in domains like automated driving,\nare increasingly adopting neural networks. From this trend arises the need for\nhigh-performance hardware exhibiting predictable timing behavior. While\nstate-of-the-art real-time hardware often suffers from limited memory and\ncompute resources, modern AI accelerators typically lack the crucial\npredictability due to memory interference.\n  We present a new hardware architecture to bridge this gap between performance\nand predictability. The architecture features a multi-core vector processor\nwith predictable cores, each equipped with local scratchpad memories. A central\nmanagement core orchestrates access to shared external memory following a\nstatically determined schedule.\n  To evaluate the proposed hardware architecture, we analyze different variants\nof our parameterized design. We compare these variants to a baseline\narchitecture consisting of a single-core vector processor with large vector\nregisters. We find that configurations with a larger number of smaller cores\nachieve better performance due to increased effective memory bandwidth and\nhigher clock frequencies. Crucially for real-time systems, execution time\nfluctuation remains very low, demonstrating the platform's time predictability."}
{"id": "2511.05149", "categories": ["cs.NI", "cs.AR", "C.2; C.4"], "pdf": "https://arxiv.org/pdf/2511.05149", "abs": "https://arxiv.org/abs/2511.05149", "authors": ["Cristina Olmedilla", "Jesus Escudero-Sahuquillo", "Pedro J. Garcia", "Francisco J. Quiles", "Jose Duato"], "title": "Improving Injection-Throttling Mechanisms for Congestion Control for Data-center and Supercomputer Interconnects", "comment": "4 pages, 3 figures", "summary": "Over the past decade, Supercomputers and Data centers have evolved\ndramatically to cope with the increasing performance requirements of\napplications and services, such as scientific computing, generative AI, social\nnetworks or cloud services. This evolution have led these systems to\nincorporate high-speed networks using faster links, end nodes using multiple\nand dedicated accelerators, or a advancements in memory technologies to bridge\nthe memory bottleneck. The interconnection network is a key element in these\nsystems and it must be thoroughly designed so it is not the bottleneck of the\nentire system, bearing in mind the countless communication operations that\ngenerate current applications and services. Congestion is serious threat that\nspoils the interconnection network performance, and its effects are even more\ndramatic when looking at the traffic dynamics and bottlenecks generated by the\ncommunication operations mentioned above. In this vein, numerous congestion\ncontrol (CC) techniques have been developed to address congestion negative\neffects. One popular example is Data Center Quantized Congestion Notification\n(DCQCN), which allows congestion detection at network switch buffers, then\nmarking congesting packets and notifying about congestion to the sources, which\nfinally apply injection throttling of those packets contributing to congestion.\nWhile DCQCN has been widely studied and improved, its main principles for\ncongestion detection, notification and reaction remain largely unchanged, which\nis an important shortcoming considering congestion dynamics in current\nhigh-performance interconnection networks. In this paper, we revisit the DCQCN\nclosed-loop mechanism and refine its design to leverage a more accurate\ncongestion detection, signaling, and injection throttling, reducing control\ntraffic overhead and avoiding unnecessary throttling of non-congesting flows."}
{"id": "2511.05067", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05067", "abs": "https://arxiv.org/abs/2511.05067", "authors": ["Giuseppe Esposito", "Juan-David Guerrero-Balaguera", "Josie Esteban Rodriguez Condia", "Matteo Sonza Reorda", "Marco Barbiero", "Rossella Fortuna"], "title": "GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters", "comment": null, "summary": "Graphics Processing Units (GPUs) are specialized accelerators in data centers\nand high-performance computing (HPC) systems, enabling the fast execution of\ncompute-intensive applications, such as Convolutional Neural Networks (CNNs).\nHowever, sustained workloads can impose significant stress on GPU components,\nraising reliability concerns due to potential faults that corrupt the\nintermediate application computations, leading to incorrect results. Estimating\nthe stress induced by an application is thus crucial to predict reliability\n(with\\,special\\,emphasis\\,on\\,aging\\,effects). In this work, we combine online\ntelemetry parameters and hardware performance counters to assess GPU stress\ninduced by different applications. The experimental results indicate the stress\ninduced by a parallel workload can be estimated by combining telemetry data and\nPerformance Counters that reveal the efficiency in the resource usage of the\ntarget workload. For this purpose the selected performance counters focus on\nmeasuring the i) throughput, ii) amount of issued instructions and iii) stall\nevents."}
{"id": "2511.05149", "categories": ["cs.NI", "cs.AR", "C.2; C.4"], "pdf": "https://arxiv.org/pdf/2511.05149", "abs": "https://arxiv.org/abs/2511.05149", "authors": ["Cristina Olmedilla", "Jesus Escudero-Sahuquillo", "Pedro J. Garcia", "Francisco J. Quiles", "Jose Duato"], "title": "Improving Injection-Throttling Mechanisms for Congestion Control for Data-center and Supercomputer Interconnects", "comment": "4 pages, 3 figures", "summary": "Over the past decade, Supercomputers and Data centers have evolved\ndramatically to cope with the increasing performance requirements of\napplications and services, such as scientific computing, generative AI, social\nnetworks or cloud services. This evolution have led these systems to\nincorporate high-speed networks using faster links, end nodes using multiple\nand dedicated accelerators, or a advancements in memory technologies to bridge\nthe memory bottleneck. The interconnection network is a key element in these\nsystems and it must be thoroughly designed so it is not the bottleneck of the\nentire system, bearing in mind the countless communication operations that\ngenerate current applications and services. Congestion is serious threat that\nspoils the interconnection network performance, and its effects are even more\ndramatic when looking at the traffic dynamics and bottlenecks generated by the\ncommunication operations mentioned above. In this vein, numerous congestion\ncontrol (CC) techniques have been developed to address congestion negative\neffects. One popular example is Data Center Quantized Congestion Notification\n(DCQCN), which allows congestion detection at network switch buffers, then\nmarking congesting packets and notifying about congestion to the sources, which\nfinally apply injection throttling of those packets contributing to congestion.\nWhile DCQCN has been widely studied and improved, its main principles for\ncongestion detection, notification and reaction remain largely unchanged, which\nis an important shortcoming considering congestion dynamics in current\nhigh-performance interconnection networks. In this paper, we revisit the DCQCN\nclosed-loop mechanism and refine its design to leverage a more accurate\ncongestion detection, signaling, and injection throttling, reducing control\ntraffic overhead and avoiding unnecessary throttling of non-congesting flows."}
{"id": "2511.05238", "categories": ["cs.NI", "68T05, 90C26, 68M10", "I.2.11; C.2.1; C.4; G.3"], "pdf": "https://arxiv.org/pdf/2511.05238", "abs": "https://arxiv.org/abs/2511.05238", "authors": ["Peide Li", "Liu Cao", "Lyutianyang Zhang", "Dongyu Wei", "Ye Hu", "Qipeng Xie"], "title": "EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environme", "comment": "Approx. 12 pages, 3 figures, 3 tables; focuses on 6G heterogeneous\n  radio environment digital twin construction via personalized federated\n  learning", "summary": "Radio Environment Map (REM) is transitioning from 5G homogeneous environments\nto B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL),\na natural fit for this distributed task, struggles with performance degradation\nin accuracy and communication efficiency under the non-independent and\nidentically distributed (Non-IID) data conditions inherent to these new\nenvironments. This paper proposes EPFL-REMNet, an efficient personalized\nfederated framework for constructing a high-fidelity digital twin of the 6G\nheterogeneous radio environment. The proposed EPFL-REMNet employs a\"shared\nbackbone + lightweight personalized head\" model, where only the compressed\nshared backbone is transmitted between the server and clients, while each\nclient's personalized head is maintained locally. We tested EPFL-REMNet by\nconstructing three distinct Non-IID scenarios (light, medium, and heavy) based\non radio environment complexity, with data geographically partitioned across 90\nclients. Experimental results demonstrate that EPFL-REMNet simultaneously\nachieves higher digital twin fidelity (accuracy) and lower uplink overhead\nacross all Non-IID settings compared to standard FedAvg and recent\nstate-of-the-art methods. Particularly, it significantly reduces performance\ndisparities across datasets and improves local map accuracy for long-tail\nclients, enhancing the overall integrity of digital twin."}
{"id": "2511.05334", "categories": ["cs.NI", "F.2.2; G.2.2"], "pdf": "https://arxiv.org/pdf/2511.05334", "abs": "https://arxiv.org/abs/2511.05334", "authors": ["Giovanni Fiaschi", "Carlo Vitucci", "Thomas Westerbäck", "Daniel Sundmark", "Thomas Nolte"], "title": "A Formal Model for Path Set Attribute Calculation in Network Systems", "comment": "8 pages, 3 figures, to be published in the proceedings of the IEEE\n  International Symposium on Networks, Computers and Communications (ISNCC'25),\n  27-28 Oct. 2025", "summary": "In graph theory and its practical networking applications, e.g.,\ntelecommunications and transportation, the problem of finding paths has\nparticular importance. Selecting paths requires giving scores to the\nalternative solutions to drive a choice. While previous studies have provided\ncomprehensive evaluation of single-path solutions, the same level of detail is\nlacking when considering sets of paths. This paper emphasizes that the path\ncharacterization strongly depends on the properties under consideration. While\nproperty-based characterization is also valid for single paths, it becomes\ncrucial to analyse multiple path sets. From the above consideration, this paper\nproposes a mathematical approach, defining a functional model that lends itself\nwell to characterizing the path set in its general formulation. The paper shows\nhow the functional model contextualizes specific attributes."}
{"id": "2511.05362", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.05362", "abs": "https://arxiv.org/abs/2511.05362", "authors": ["Lucian Trestioreanu", "Flaviene Scheidt", "Wazen Shbair", "Jerome Francois", "Damien Magoni", "Radu State"], "title": "To Squelch or not to Squelch: Enabling Improved Message Dissemination on the XRP Ledger", "comment": "7 pages", "summary": "With the large increase in the adoption of blockchain technologies, their\nunderlying peer-to-peer networks must also scale with the demand. In this\ncontext, previous works highlighted the importance of ensuring efficient and\nresilient communication for the underlying consensus and replication\nmechanisms. However, they were mainly focused on mainstream,\nProof-of-Work-based Distributed Ledger Technologies like Bitcoin or Ethereum.\n  In this paper, the problem is investigated in the context of\nconsensus-validation based blockchains, like the XRP Ledger. The latter relies\non a Federated Byzantine Agreement (FBA) consensus mechanism which is proven to\nhave a good scalability in regards to transaction throughput. However, it is\nknown that significant increases in the size of the XRP Ledger network would be\nchallenging to achieve. The main reason is the flooding mechanism used to\ndisseminate the messages related to the consensus protocol, which creates many\nduplicates in the network. Squelching is a recent solution proposed for\nlimiting this duplication, however, it was never evaluated quantitatively in\nreal-life scenarios involving the XRPL production network. In this paper, our\naim is to assess this mechanism using a real-life controllable testbed and the\nXRPL production network, to assess its benefit and compare it to alternative\nsolutions relying on Named Data Networking and on a gossip-based approach."}
{"id": "2511.05423", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.05423", "abs": "https://arxiv.org/abs/2511.05423", "authors": ["Maynard Koch", "Raphael Hiesgen", "Marcin Nawrocki", "Thomas C. Schmidt", "Matthias Wählisch"], "title": "Scanning the IPv6 Internet Using Subnet-Router Anycast Probing", "comment": null, "summary": "Identifying active IPv6 addresses is challenging. Various methods emerged to\nmaster the measurement challenge in this huge address space, including\nhitlists, new probing techniques, and AI-generated target lists. In this paper,\nwe apply active Subnet-Router anycast (SRA) probing, a commonly unused method\nto explore the IPv6 address space. We compare our results with lists of active\nIPv6 nodes obtained from prior methods and with random probing. Our findings\nindicate that probing an SRA address reveals on average 10% more router IP\naddresses than random probing and is far less affected by ICMP rate limiting.\nCompared to targeting router addresses directly, SRA probing discovers 80% more\naddresses. We conclude that SRA probing is an important addition to the IPv6\nmeasurement toolbox and may improve the stability of results significantly. We\nalso find evidence that some active scans can cause harmful conditions in\ncurrent IPv6 deployments, which we started to fix in collaboration with network\noperators."}
