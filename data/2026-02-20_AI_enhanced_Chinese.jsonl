{"id": "2602.15995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15995", "abs": "https://arxiv.org/abs/2602.15995", "authors": ["Xiang Fu", "Shiman Meng", "Weiping Zhang", "Luanzheng Guo", "Kento Sato", "Dong H. Ahn", "Ignacio Laguna", "Gregory L. Lee", "Martin Schulz"], "title": "Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs", "comment": "IEEE Cluster 2024", "summary": "After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \\toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16010", "abs": "https://arxiv.org/abs/2602.16010", "authors": ["Xin Huang", "Weiping Zhang", "Shiman Meng", "Wubiao Xu", "Xiang Fu", "Luanzheng Guo", "Kento Sato"], "title": "Scrutinizing Variables for Checkpoint Using Automatic Differentiation", "comment": "The Second Workshop on Enabling Predictive Science with Optimization and Uncertainty Quantification in HPC (EPSOUQ-HPC) in conjunction with SC24", "summary": "Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16100", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16100", "abs": "https://arxiv.org/abs/2602.16100", "authors": ["Zijie Su", "Muhammed Tawfiqul Islam", "Mohammad Goudarzi", "Adel N. Toosi"], "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16222", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16222", "abs": "https://arxiv.org/abs/2602.16222", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Robin Vacus"], "title": "Near-optimal population protocols on bounded-degree trees", "comment": "37 pages, 7 figures", "summary": "We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.\n  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.\n  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \\log n)$ steps on directed trees.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16024", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16024", "abs": "https://arxiv.org/abs/2602.16024", "authors": ["R. Kanda", "H. L. Blevec", "N. Onizawa", "M. Leonardon", "V. Gripon", "T. Hanyu"], "title": "Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware", "comment": null, "summary": "In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16130", "categories": ["cs.NI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16130", "abs": "https://arxiv.org/abs/2602.16130", "authors": ["Zibin Lin", "Taotao Wang", "Shengli Zhang", "Long Shi", "Shui Yu"], "title": "Managing Credible Anonymous Identities in Web 3.0 Services: A Scalable On-Chain Admission Framework with Recursive Proof Aggregation", "comment": "15 pages, 9 figures", "summary": "Open Web 3.0 platforms increasingly operate as \\emph{service ecosystems} (e.g., DeFi, DAOs, and decentralized social applications) where \\emph{admission control} and \\emph{account provisioning} must be delivered as an always-on service under bursty demand. Service operators face a fundamental tension: enforcing Sybil resistance (one-person-one-account) while preserving user privacy, yet keeping on-chain verification cost and admission latency predictable at scale. Existing credential-based ZK admission approaches typically require per-request on-chain verification, making the provisioning cost grow with the number of concurrent joiners. We present \\textbf{ZK-AMS}, a scalable admission and provisioning layer that bridges real-world \\emph{Personhood Credentials} to anonymous on-chain service accounts. ZK-AMS combines (i) zero-knowledge credential validation, (ii) a \\emph{permissionless} batch submitter model, and (iii) a decentralized, privacy-preserving folding pipeline that uses Nova-style recursive aggregation together with multi-key homomorphic encryption, enabling batch settlement with \\emph{constant} on-chain verification per batch. We implement ZK-AMS end-to-end on an Ethereum testbed and evaluate admission throughput, end-to-end latency, and gas consumption. Results show stable verification cost across batch sizes and substantially improved admission efficiency over non-recursive baselines, providing a practical and cost-predictable admission service for large-scale Web 3.0 communities.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16233", "categories": ["cs.DC", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16233", "abs": "https://arxiv.org/abs/2602.16233", "authors": ["Prabhjot Singh", "Adel N. Toosi", "Rajkumar Buyya"], "title": "DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting", "comment": null, "summary": "Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16075", "categories": ["cs.AR", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.16075", "abs": "https://arxiv.org/abs/2602.16075", "authors": ["Ryan Wong", "Ben Feinberg", "Saugata Ghose"], "title": "DARTH-PUM: A Hybrid Processing-Using-Memory Architecture", "comment": "To appear in the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) 2026", "summary": "Analog processing-using-memory (PUM; a.k.a. in-memory computing) makes use of electrical interactions inside memory arrays to perform bulk matrix-vector multiplication (MVM) operations. However, many popular matrix-based kernels need to execute non-MVM operations, which analog PUM cannot directly perform. To retain its energy efficiency, analog PUM architectures augment memory arrays with CMOS-based domain-specific fixed-function hardware to provide complete kernel functionality, but the difficulty of integrating such specialized CMOS logic with memory arrays has largely limited analog PUM to being an accelerator for machine learning inference, or for closely related kernels. An opportunity exists to harness analog PUM for general-purpose computation: recent works have shown that memory arrays can also perform Boolean PUM operations, albeit with very different supporting hardware and electrical signals than analog PUM.\n  We propose DARTH-PUM, a general-purpose hybrid PUM architecture that tackles key hardware and software challenges to integrating analog PUM and digital PUM. We propose optimized peripheral circuitry, coordinating hardware to manage and interface between both types of PUM, an easy-to-use programming interface, and low-cost support for flexible data widths. These design elements allow us to build a practical PUM architecture that can execute kernels fully in memory, and can scale easily to cater to domains ranging from embedded applications to large-scale data-driven computing. We show how three popular applications (AES encryption, convolutional neural networks, large-language models) can map to and benefit from DARTH-PUM, with speedups of 59.4x, 14.8x, and 40.8x over an analog+CPU baseline.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16163", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16163", "abs": "https://arxiv.org/abs/2602.16163", "authors": ["Md Sharif Hossen", "Cole Dickerson", "Ozgur Ozdemir", "Anil Gurses", "Mohamed Rabeek Sarbudeen", "Thomas Zajkowski", "Ahmed Manavi Alam", "Everett Tucker", "William Bjorndahl", "Fred Solis", "Sadaf Javed", "Anirudh Kamath", "Xiangyao Tang", "Joarder Jafor Sadique", "Kevin Liu Hermstein", "Kaies Al Mahmud", "Jose Angel Sanchez Viloria", "Skyler Hawkins", "Yuqing Cui", "Annoy Dey", "Yuchen Liu", "Ali Gurbuz", "Joseph Camp", "Rizwan Ahmad", "Jacobus van der Merwe", "Ahmed Ibrahim Mohamed", "Gil Zussman", "Mehmet Kurum", "Namuduri Kamesh", "Zhangyu Guan", "Dimitris Pados", "George Skilvanitis", "Ismail Guvenc", "Mihail Sichitiu", "Magreth Mushi", "Rudra Dutta"], "title": "Collection: UAV-Based Wireless Multi-modal Measurements from AERPAW Autonomous Data Mule (AADM) Challenge in Digital Twin and Real-World Environments", "comment": "10 pages, 12 figures", "summary": "In this work, we present an unmanned aerial vehicle (UAV) wireless dataset collected as part of the AERPAW Autonomous Aerial Data Mule (AADM) challenge, organized by the NSF Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) project. The AADM challenge was the second competition in which an autonomous UAV acted as a data mule, where the UAV downloaded data from multiple base stations (BSs) in a dynamic wireless environment. Participating teams designed flight control and decision-making algorithms for choosing which BSs to communicate with and how to plan flight trajectories to maximize data download within a mission completion time. The competition was conducted in two stages: Stage 1 involved development and experimentation using a digital twin (DT) environment, and in Stage 2, the final test run was conducted on the outdoor testbed. The total score for each team was compiled from both stages. The resulting dataset includes link quality and data download measurements, both in DT and physical environments. Along with the USRP measurements used in the contest, the dataset also includes UAV telemetry, Keysight RF sensors position estimates, link quality measurements from LoRa receivers, and Fortem radar measurements. It supports reproducible research on autonomous UAV networking, multi-cell association and scheduling, air-to-ground propagation modeling, DT-to-real-world transfer learning, and integrated sensing and communication, which serves as a benchmark for future autonomous wireless experimentation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16338", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16338", "abs": "https://arxiv.org/abs/2602.16338", "authors": ["Mohsen Ahmadvand", "Rok Pajni\u010d", "Ching-Lun Chiu"], "title": "push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation", "comment": null, "summary": "Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16143", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.16143", "abs": "https://arxiv.org/abs/2602.16143", "authors": ["Naoya Onizawa", "Taiga Kubuta", "Duckgyu Shin", "Takahiro Hanyu"], "title": "Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture", "comment": null, "summary": "Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16174", "categories": ["cs.NI", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.16174", "abs": "https://arxiv.org/abs/2602.16174", "authors": ["Fatih Temiz", "Shavbo Salehi", "Melike Erol-Kantarci"], "title": "Edge Learning via Federated Split Decision Transformers for Metaverse Resource Allocation", "comment": "6 pages, 4 figures, Accepted paper at IEEE International Conference on Communications (ICC) 2026", "summary": "Mobile edge computing (MEC) based wireless metaverse services offer an untethered, immersive experience to users, where the superior quality of experience (QoE) needs to be achieved under stringent latency constraints and visual quality demands. To achieve this, MEC-based intelligent resource allocation for virtual reality users needs to be supported by coordination across MEC servers to harness distributed data. Federated learning (FL) is a promising solution, and can be combined with reinforcement learning (RL) to develop generalized policies across MEC-servers. However, conventional FL incurs transmitting the full model parameters across the MEC-servers and the cloud, and suffer performance degradation due to naive global aggregation, especially in heterogeneous multi-radio access technology environments. To address these challenges, this paper proposes Federated Split Decision Transformer (FSDT), an offline RL framework where the transformer model is partitioned between MEC servers and the cloud. Agent-specific components (e.g., MEC-based embedding and prediction layers) enable local adaptability, while shared global layers in the cloud facilitate cooperative training across MEC servers. Experimental results demonstrate that FSDT enhances QoE for up to 10% in heterogeneous environments compared to baselines, while offloadingnearly 98% of the transformer model parameters to the cloud, thereby reducing the computational burden on MEC servers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16347", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16347", "abs": "https://arxiv.org/abs/2602.16347", "authors": ["Jon Vehovar", "Miha Rot", "Matja\u017e Depolli", "Gregor Kosec"], "title": "Load Balanced Parallel Node Generation for Meshless Numerical Methods", "comment": null, "summary": "Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16345", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16345", "abs": "https://arxiv.org/abs/2602.16345", "authors": ["Leonardo Spampinato", "Lorenzo Mario Amorosa", "Enrico Testi", "Chiara Buratti", "Riccardo Marini"], "title": "Multi-Agent Meta-Advisor for UAV Fleet Trajectory Design in Vehicular Networks", "comment": null, "summary": "Future vehicular networks require continuous connectivity to serve highly mobile users in urban environments. To mitigate the coverage limitations of fixed terrestrial macro base stations (MBS) under non line-of-sight (NLoS) conditions, fleets of unmanned aerial base stations (UABSs) can be deployed as aerial base stations, dynamically repositioning to track vehicular users and traffic hotspots in coordination with the terrestrial network. This paper addresses cooperative multi-agent trajectory design under different service areas and takeoff configurations, where rapid and safe adaptation across scenarios is essential. We formulate the problem as a multi-task decentralized partially observable Markov decision process and solve it using centralized training and decentralized execution with double dueling deep Q-network (3DQN), enabling online training for real-world deployments. However, efficient exploration remains a bottleneck, with conventional strategies like $\u03b5$-greedy requiring careful tuning. To overcome this, we propose the multi-agent meta-advisor with advisor override (MAMO). This framework guides agent exploration through a meta-policy learned jointly across tasks. It uses a dynamic override mechanism that allows agents to reject misaligned guidance when the advisor fails to generalize to a specific scenario. Simulation results across three realistic urban scenarios and multiple takeoff configurations show that MAMO achieves faster convergence and higher returns than tuned $\u03b5$-greedy baselines, outperforming both an advisor-only ablation and a single generalized policy. Finally, we demonstrate that the learned UABS fleet significantly improves network performance compared to deployments without aerial support.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16362", "categories": ["cs.DC", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16362", "abs": "https://arxiv.org/abs/2602.16362", "authors": ["MHD Saria Allahham", "Hossam S. Hassanein"], "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability", "comment": null, "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16367", "categories": ["cs.NI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.16367", "abs": "https://arxiv.org/abs/2602.16367", "authors": ["Zahid Ali", "Saritha Unnikrishnan", "Eoghan Furey", "Ian McLoughlin", "Saim Ghafoor"], "title": "A Multihop Rendezvous Protocol for Cognitive Radio-based Emergency Response Network", "comment": "5 pages. Submitted to IEEE Communication Letters", "summary": "This letter proposes a novel Multihop Dual Modular Clock Algorithm (M-DMCA) for efficient node discovery in cognitive radio-based emergency response networks. M-DMCA supports dual-channel selection per timeslot and incorporates a three-way handshake mechanism to significantly reduce rendezvous time. Performance evaluation under a worst-case scenario with 20 nodes, asymmetric channel sets of size 20, channel similarity index (m) as 2, and high primary radio activity shows that M-DMCA achieves a 24% reduction in rendezvous time compared to the multihop Extended Modular Clock Algorithm (EMCA), outperforming existing rendezvous protocols.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16603", "abs": "https://arxiv.org/abs/2602.16603", "authors": ["Chia-chi Hsieh", "Zan Zong", "Xinyang Chen", "Jianjiang Li", "Jidong Zhai", "Lijie Wen"], "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving", "comment": "13 pages", "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16386", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16386", "abs": "https://arxiv.org/abs/2602.16386", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Nikolaos Tsironis", "Vasileios Theodorou", "Christos Verikoukis"], "title": "Towards Secure and Interoperable Data Spaces for 6G: The 6G-DALI Approach", "comment": null, "summary": "The next generation of mobile networks, 6G, is expected to enable data-driven services at unprecedented scale and complexity, with stringent requirements for trust, interoperability, and automation. Central to this vision is the ability to create, manage, and share high-quality datasets across distributed and heterogeneous environments. This paper presents the data architecture of the 6G-DALI project, which implements a federated dataspace and DataOps infrastructure to support secure, compliant, and scalable data sharing for AI-driven experimentation and service orchestration. Drawing from principles defined by GAIA-X and the International Data Spaces Association (IDSA), the architecture incorporates components such as federated identity management, policy-based data contracts, and automated data pipelines. We detail how the 6G-DALI architecture aligns with and extends GAIA-X and IDSA reference models to meet the unique demands of 6G networks, including low-latency edge processing, dynamic trust management, and cross-domain federation. A comparative analysis highlights both convergence points and necessary innovations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
{"id": "2602.16686", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.16686", "abs": "https://arxiv.org/abs/2602.16686", "authors": ["Shakthivelu Janardhanan", "Yaxuan Chen", "Wolfgang Kellerer", "Carmen Mas-Machuca"], "title": "Fast-MCS: A Scalable Open-Source Tool to Find Minimal Cut Sets", "comment": null, "summary": "A network is represented as a graph consisting of nodes and edges. A cut set for a source-destination pair in a network is a set of elements that, when failed, cause the source-destination pair to lose connectivity. A Minimal Cut Set (MCS) is a cut set that cannot be further reduced while maintaining its status as a cut set. MCSs are crucial in identifying the critical elements in the network that have the most significant impact on failure. This work introduces Fast-MCS, an open-source, scalable tool for evaluating MCSs in large, complex networks. Additionally, we compare the computation time of Fast-MCS with the state-of-the-art.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "challenges": "Challenges extraction failed", "contributions": "Contributions extraction failed", "results": "Results analysis unavailable", "conclusion": "Conclusion extraction failed", "related_work": "Related work extraction failed"}}
