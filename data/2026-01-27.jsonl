{"id": "2601.16935", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.16935", "abs": "https://arxiv.org/abs/2601.16935", "authors": ["Wei Wei", "Jingye Xu", "Sahidul Islam", "Dakai Zhu", "Chen Pan", "Mimi Xie"], "title": "AERO: Adaptive and Efficient Runtime-Aware OTA Updates for Energy-Harvesting IoT", "comment": "Accepted at DATE 2026", "summary": "Energy-harvesting (EH) Internet of Things (IoT) devices operate under intermittent energy availability, which disrupts task execution and makes energy-intensive over-the-air (OTA) updates particularly challenging. Conventional OTA update mechanisms rely on reboots and incur significant overhead, rendering them unsuitable for intermittently powered systems. Recent live OTA update techniques reduce reboot overhead but still lack mechanisms to ensure consistency when updates interact with runtime execution. This paper presents AERO, an Adaptive and Efficient Runtime-Aware OTA update mechanism that integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. By identifying update-affected execution regions and dynamically adjusting dependencies, AERO ensures consistent up date integration while adapting to intermittent energy availability. Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches."}
{"id": "2601.16294", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16294", "abs": "https://arxiv.org/abs/2601.16294", "authors": ["Evangelos Georganas", "Alexander Heinecke", "Pradeep Dubey"], "title": "Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple", "comment": null, "summary": "General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance \"glass jaws\". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes."}
{"id": "2601.16323", "categories": ["cs.NI", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.16323", "abs": "https://arxiv.org/abs/2601.16323", "authors": ["Belal Korany", "Peerapol Tinnakornsrisuphap", "Saadallah Kassir", "Prashanth Hande", "Hyun Yong Lee", "Thomas Stockhammer", "Hemanth Sampath"], "title": "Multi-User Content Diversity in Wireless Networks", "comment": "arXiv admin note: text overlap with arXiv:2505.04114", "summary": "Immersive applications such as eXtended Reality (XR), cloud gaming, and real-time video streaming are central to the vision of 6G networks. These applications require not only low latency and high data rates, but also consistent and high-quality User Experience (UX). Traditional rate allocation and congestion control mechanisms in wireless networks treat users uniformly based on channel conditions, rely only on network-centric Key Performance Indicators (KPIs), and ignore the content diversity, which can lead to inefficient resource utilization and degraded UX. In this paper, we introduce the concept of Multi-User Content Diversity, which recognizes that different users concurrently consume media with varying complexity, and therefore have different bitrate requirements to achieve satisfactory UX. We propose multiple different frameworks that exploit multi-user content diversity and lead to overall network-wide gains in terms of UX. For each framework, we demonstrate the required information exchange between Application Servers (ASs), Application Clients (ACs), and the network, and the algorithms that run in each of these components to optimize a network-wide UXbased objective. Simulation results demonstrate that exploiting multi-user content diversity leads to significant gains in UX capacity, UX fairness, and network utilization, when compared to conventional rate control methods. These findings highlight the potential of content-aware networking as a key enabler for emerging wireless systems."}
{"id": "2601.16460", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16460", "abs": "https://arxiv.org/abs/2601.16460", "authors": ["Ivan Klianev"], "title": "Consensus In Asynchrony", "comment": null, "summary": "We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support."}
{"id": "2601.16533", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.16533", "abs": "https://arxiv.org/abs/2601.16533", "authors": ["Wen Zhang", "Aimin Wang", "Geng Sun", "Jiahui Li", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "title": "UAV-Assisted Joint Data Collection and Wireless Power Transfer for Batteryless Sensor Networks", "comment": "Accepted to IEEE WCNC 2026. 6 pages, 6 figures. arXiv admin note: text overlap with arXiv:2507.07481", "summary": "The development of wireless power transfer (WPT) and Internet of Things (IoT) offers significant potential but faces challenges such as limited energy supply, dynamic environmental changes, and unstable transmission links. This paper presents an unmanned aerial vehicle (UAV)-assisted data collection and WPT scheme to support batteryless sensor (BLS) networks in remote areas. In this system, BLSs harvest energy from the UAV and utilize the harvested energy to transmit the collected data back to the UAV. The goal is to maximize the collected data volume and fairness index while minimizing the UAV energy consumption. To achieve these objectives, an optimization problem is formulated to jointly optimize the transmit power and UAV trajectory. Due to the non-convexity and dynamic nature of the problem, a deep reinforcement learning (DRL)-based algorithm is proposed to solve the problem. Specifically, this algorithm integrates prioritized experience replay and the performer module to enhance system stability and accelerate convergence. Simulation results demonstrate that the proposed approach consistently outperforms benchmark schemes in terms of collected data volume, fairness, and UAV energy consumption."}
{"id": "2601.16536", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16536", "abs": "https://arxiv.org/abs/2601.16536", "authors": ["Yuanhong He", "Peiyu Niu", "Jun Chen", "Chenchen Zhang", "Chao Yang"], "title": "W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs", "comment": null, "summary": "As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators."}
{"id": "2601.16559", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2601.16559", "abs": "https://arxiv.org/abs/2601.16559", "authors": ["Roberto Pegurri", "Habu Shintaro", "Francesco Linsalata", "Wang Kui", "Tao Yu", "Eugenio Moro", "Maiya Igarashi", "Antonio Capone", "Kei Sakaguchi"], "title": "Predicting Networks Before They Happen: Experimentation on a Real-Time V2X Digital Twin", "comment": null, "summary": "Emerging safety-critical Vehicle-to-Everything (V2X) applications require networks to proactively adapt to rapid environmental changes rather than merely reacting to them. While Network Digital Twins (NDTs) offer a pathway to such predictive capabilities, existing solutions typically struggle to reconcile high-fidelity physical modeling with strict real-time constraints. This paper presents a novel, end-to-end real-time V2X Digital Twin framework that integrates live mobility tracking with deterministic channel simulation. By coupling the Tokyo Mobility Digital Twin-which provides live sensing and trajectory forecasting-with VaN3Twin-a full-stack simulator with ray tracing-we enable the prediction of network performance before physical events occur. We validate this approach through an experimental proof-of-concept deployed in Tokyo, Japan, featuring connected vehicles operating on 60 GHz links. Our results demonstrate the system's ability to predict Received Signal Strength (RSSI) with a maximum average error of 1.01 dB and reliably forecast Line-of-Sight (LoS) transitions within a maximum average end-to-end system latency of 250 ms, depending on the ray tracing level of detail. Furthermore, we quantify the fundamental trade-offs between digital model fidelity, computational latency, and trajectory prediction horizons, proving that high-fidelity and predictive digital twins are feasible in real-world urban environments."}
{"id": "2601.16635", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16635", "abs": "https://arxiv.org/abs/2601.16635", "authors": ["Julian Legler"], "title": "Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices", "comment": "Accepted Artifact Paper at ICSOC2025", "summary": "Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We present GOXN (Green Observability eXperiment eNginE), an energy experimentation engine for Kubernetes-based microservices that quantifies compute, network, and storage energy at the service level. Using GOXN, we evaluated the OpenTelemetry Demo under varying configurations (monitoring, tracing, service mesh) and steady synthetic load, collecting metrics from Kepler and cAdvisor. Our additive energy model derives service-level energy from container-level data. Results show that excluding network and storage can underestimate auxiliary-service energy by up to 63%, and that high tracing loads shift energy dominance toward network and storage."}
{"id": "2601.16848", "categories": ["cs.NI", "math.NA", "stat.AP"], "pdf": "https://arxiv.org/pdf/2601.16848", "abs": "https://arxiv.org/abs/2601.16848", "authors": ["Jaume Anguera Peris", "Joakim Jald√©n"], "title": "Stochastic Modeling and Resource Dimensioning of Multi-Cellular Edge Intelligent Systems", "comment": null, "summary": "Edge intelligence enables AI inference at the network edge, co-located with or near the radio access network, rather than in centralized clouds or on mobile devices. It targets low-latency, resource-constrained applications with large data volumes, requiring tight integration of wireless access and on-site computing. Yet system performance and cost-efficiency hinge on joint pre-deployment dimensioning of radio and computational resources, especially under spatial and temporal uncertainty. Prior work largely emphasizes run-time allocation or relies on simplified models that decouple radio and computing, missing end-to-end correlations in large-scale deployments. This paper introduces a unified stochastic framework to dimension multi-cell edge-intelligent systems. We model network topology with Poisson point processes, capturing random user and base-station locations, inter-cell interference, distance-based fractional power control, and peak-power constraints. By combining this with queueing theory and empirical AI inference workload profiling, we derive tractable expressions for end-to-end offloading delay. These enable a non-convex joint optimization that minimizes deployment cost under statistical QoS guarantees, expressed through strict tail-latency and inference-accuracy constraints. We prove the problem decomposes into convex subproblems, yielding global optimality. Numerical results in noise- and interference-limited regimes identify cost-efficient design regions and configurations that cause under-utilization or user unfairness. Smaller cells reduce transmission delay but raise per-request computing cost due to weaker server multiplexing, whereas larger cells show the opposite trend. Densification reduces computational costs only when frequency reuse scales with base-station density; otherwise, sparser deployments improve fairness and efficiency in interference-limited settings."}
{"id": "2601.16637", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16637", "abs": "https://arxiv.org/abs/2601.16637", "authors": ["Jun Doi", "Tomonori Shirakawa", "Yukio Kawashima", "Seiji Yunoki", "Hiroshi Horii"], "title": "GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms", "comment": null, "summary": "Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\\sim$40$\\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows."}
{"id": "2601.16950", "categories": ["cs.NI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.16950", "abs": "https://arxiv.org/abs/2601.16950", "authors": ["Ferran Maura", "Francesc Wilhelmi", "Boris Bellalta"], "title": "Evaluating Wi-Fi Performance for VR Streaming: A Study on Realistic HEVC Video Traffic", "comment": null, "summary": "Cloud-based Virtual Reality (VR) streaming presents significant challenges for 802.11 networks due to its high throughput and low latency requirements. When multiple VR users share a Wi-Fi network, the resulting uplink and downlink traffic can quickly saturate the channel. This paper investigates the capacity of 802.11 networks for supporting realistic VR streaming workloads across varying frame rates, bitrates, codec settings, and numbers of users. We develop an emulation framework that reproduces Air Light VR (ALVR) operation, where real HEVC video traffic is fed into an 802.11 simulation model. Our findings explore Wi-Fi's performance anomaly and demonstrate that Intra-refresh (IR) coding effectively reduces latency variability and improves QoS, supporting up to 4 concurrent VR users with Constant Bitrate (CBR) 100 Mbps before the channel is saturated."}
{"id": "2601.16956", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.16956", "abs": "https://arxiv.org/abs/2601.16956", "authors": ["Avinash Maurya", "M. Mustafa Rafique", "Franck Cappello", "Bogdan Nicolae"], "title": "DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers", "comment": null, "summary": "The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training."}
