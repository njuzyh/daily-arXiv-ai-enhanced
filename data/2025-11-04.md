<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Analytical Model of NR-V2X Mode 2 with Re-Evaluation Mechanism](https://arxiv.org/abs/2510.27108)
*Shuo Zhu,Siyu Lin*

Main category: cs.NI

TL;DR: 本文建立了NR-V2X Mode 2的分析模型，利用离散时间马尔可夫链（DTMC）构建消息生成器，模拟3GPP推荐的高级V2X服务交通模式，研究了重评估机制对传输可靠性的影响，并指出仍需局部改进以降低延迟。


<details>
  <summary>Details</summary>
Motivation: 由于大规模消息传输、不可预测的非周期性消息和高速移动车辆导致V2X无线环境复杂，资源冲突频发，现有研究缺乏对可变交通场景的考虑，难以进行有效分析与比较，因此需要建立更贴近实际的分析模型。

Challenges: 如何在高动态、高密度的V2X通信环境中准确建模可变交通模式；如何评估重评估机制在真实交通条件下的性能表现；如何在提升可靠性的同时降低传输延迟。

Contributions: 提出了基于DTMC的NR-V2X Mode 2分析模型；构建了符合3GPP高级V2X服务推荐的交通模式的消息生成器；通过模型分析验证了重评估机制对传输可靠性的提升作用，并识别出延迟优化的改进空间。

Results: 研究表明，重评估机制显著提高了NR-V2X传输的可靠性，但在高负载或高动态场景下仍存在延迟较高的问题，需进一步优化资源选择和重评估时机。

Conclusion: 重评估机制有助于提升NR-V2X的传输可靠性，但为满足低延迟需求，仍需对机制进行局部优化，未来工作应结合实际交通动态进一步改进MAC层设计。

Related Work: 已有研究多集中于静态或简化交通场景下的NR-V2X性能分析，较少考虑3GPP推荐的实际交通模式，且缺乏对重评估机制在动态环境中的建模与评估。

Abstract: Massive message transmissions, unpredictable aperiodic messages, and
high-speed moving vehicles contribute to the complex wireless environment,
resulting in inefficient resource collisions in Vehicle to Everything (V2X). In
order to achieve better medium access control (MAC) layer performance, 3GPP
introduced several new features in NR-V2X. One of the most important is the
re-evaluation mechanism. It allows the vehicle to continuously sense resources
before message transmission to avoid resource collisions. So far, only a few
articles have studied the re-evaluation mechanism of NR-V2X, and they mainly
focus on network simulator that do not consider variable traffic, which makes
analysis and comparison difficult. In this paper, an analytical model of NR-V2X
Mode 2 is established, and a message generator is constructed by using discrete
time Markov chain (DTMC) to simulate the traffic pattern recommended by 3GPP
advanced V2X services. Our study shows that the re-evaluation mechanism
improves the reliability of NR-V2X transmission, but there are still local
improvements needed to reduce latency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh是一个多租户服务架构，用于执行和优化AI工作流（如RLHF/RLAIF训练和智能体工作流），通过将工作流分解为细粒度算子并实现跨用户任务去重与批处理，显著降低成本和能耗，同时保持良好的延迟表现和系统弹性。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署逐渐从单一的大模型任务转变为包含数据转换、微调和智能体交互的复杂流水线，传统孤立的管道式架构难以高效应对资源利用、成本和可扩展性挑战，因此需要一种共享、统一的服务架构来优化整体工作流执行。

Challenges: 如何在多用户、多任务环境下实现高效资源共享，避免重复计算；如何在异构GPU集群中平衡吞吐量、成本和数据局部性；如何在动态和易发生故障的环境中保证系统的可扩展性和容错能力。

Contributions: 提出了FlowMesh，一种将AI工作流分解为细粒度算子并记录血缘关系的多租户服务架构；设计了基于全局控制平面和单一效用函数的调度机制，统一决策批处理和工作节点选择；构建了基于无状态工作节点和内容寻址存储的数据平面，支持弹性扩展、安全重试和跨集群可移植性。

Results: 与基线方案相比，FlowMesh最高可实现3.8倍的成本降低和2.0倍的能耗下降，提供相似或更优的延迟表现，并在动态负载和故障条件下保持高效运行。

Conclusion: FlowMesh通过统一的共享服务架构有效支持现代AI工作流的复杂性和多样性，在成本、能效和系统鲁棒性方面显著优于传统隔离式管道，适用于多租户、异构和分布式GPU环境。

Related Work: 相关工作包括RLHF/RLAIF训练流程、智能体工作流系统、多租户机器学习平台、分布式数据流系统（如Flink、Spark）、以及基于Kubernetes或去中心化GPU市场（如Vast.ai）的资源管理与调度研究。

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [3] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: 本文提出了一种新的协同张量与流水线并行调度方法（STP），通过将前向和反向传播解耦为细粒度计算单元并进行编织组合，有效减少了张量并行和流水线并行中的通信开销与气泡，显著提升了大模型训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的混合并行方法在处理张量并行的通信开销和流水线并行的同步效率问题时多采用孤立视角，未能同时优化两类开销，因此需要一种协同调度机制来统一减少两种并行模式下的性能瓶颈。

Challenges: 主要挑战包括：1）张量并行带来的高集体通信开销；2）流水线并行中存在的气泡导致设备利用率下降；3）如何在不增加复杂性的前提下协同优化两种并行方式的调度。

Contributions: 1）提出一种新的协同张量与流水线并行调度框架（STP）；2）将前向和反向传播解耦为细粒度单元并通过编织形成复合计算序列；3）实现了TP相关气泡的近完全消除，并优化PP调度以最小化流水线气泡；4）开源代码供后续研究使用。

Results: 实验结果表明，与现有调度方法相比，该方法在LLM上最高提升12%的训练吞吐量，在MLLM上提升达16%。

Conclusion: 所提出的STP调度方法能有效协同优化张量并行和流水线并行的性能瓶颈，显著提升大规模语言模型及其多模态扩展的训练效率，具备实际应用价值。

Related Work: 相关工作主要包括张量并行（Tensor Parallelism）中的通信重叠技术，以及流水线并行（Pipeline Parallelism）中的灵活调度策略（如PipeDream、GPipe等），但这些方法通常单独优化其中一种并行模式，缺乏对两者协同效应的综合考虑。

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [4] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 本文提出了一种专用于由能量收集驱动的多接入边缘计算系统的在线资源分配策略，通过动态调度带依赖关系的计算任务和实时调整服务器频率及服务模块迁移，有效平衡了能量供应的间歇性与用户需求的动态性，在保证低服务延迟的同时提高了能量利用效率。


<details>
  <summary>Details</summary>
Motivation: 为了实现可持续发展，多接入边缘计算系统越来越多地与可再生能源采集技术结合，但如何在能量供应不稳定的情况下满足动态变化的用户需求，成为一个关键挑战。因此，需要一种能够在线适应能量和负载波动的资源管理策略。

Challenges: 主要挑战包括：1）采集能量的间歇性和不可预测性；2）用户服务请求的动态性和时空变化；3）计算任务之间存在依赖关系，增加了调度复杂性；4）需在能量自给的前提下同时优化服务延迟和能量利用效率。

Contributions: 本文的主要贡献包括：1）提出了一种完全依赖能量采集的MEC系统在线资源分配框架；2）设计了一个联合优化任务调度、服务器频率调节和服务模块迁移的动态策略；3）在真实数据集上的实验验证了该策略在降低延迟和提升能量效率方面的有效性。

Results: 实验结果表明，所提出的算法相比基准方法能更高效地利用采集到的能量，显著降低服务延迟，同时保持系统能量自给自足，在不同负载和能量输入条件下均表现出良好的鲁棒性和适应性。

Conclusion: 本文验证了在纯能量采集驱动的MEC系统中，通过联合优化任务调度与资源管理策略，可以在满足低延迟服务的同时实现可持续运行，为绿色边缘计算提供了可行的解决方案。

Related Work: 相关工作主要集中在基于电网供电的MEC资源管理、独立的能量采集系统优化以及离线或半在线的任务调度算法，但缺乏对完全依赖能量采集、任务有依赖且需动态迁移的综合在线策略研究。

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [5] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的启发式方法，用于确定CUDA并行划分算法中子系统的最优大小，并通过k近邻分类方法建立预测模型，实验结果表明该模型具有良好的预测性能，随后该方法被扩展到递归并行划分算法中。


<details>
  <summary>Details</summary>
Motivation: 为了提高CUDA实现的并行划分算法在求解线性代数方程组时的性能，需要找到最优的子系统大小，而传统方法难以高效确定这一参数。

Challenges: 不同规模的线性方程组需要不同的最优子系统大小，且在递归并行划分中还需动态决定每一步的最优划分参数，增加了建模难度。

Contributions: 1. 提出基于kNN的机器学习模型来预测最优子系统大小；2. 将启发式方法扩展至递归并行划分算法；3. 建立预测最优递归步数的kNN模型。

Results: 实验表明kNN模型对最优子系统大小和递归步数的预测结果与实际数据吻合良好，算法表现令人满意。

Conclusion: 所提出的机器学习启发式方法能有效预测并行划分算法中的关键参数，提升了CUDA实现的性能和适应性。

Related Work: 相关工作包括并行求解线性方程组的划分策略、CUDA并行优化技术以及机器学习在系统参数调优中的应用。

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: 本文深入分析了FPGA和ASIC中的时序收敛挑战与时序约束，比较了Xilinx Kintex UltraScale+ FPGA与7nm ASIC的时序性能，实验结果表明ASIC在时序上更具优势，但现代FPGA仍具备高性能设计的竞争力。


<details>
  <summary>Details</summary>
Motivation: 为了理解FPGA和ASIC在高精度时序设计中的差异与权衡，推动高性能集成电路设计的发展。

Challenges: FPGA和ASIC在架构和设计流程上的差异导致时序闭合难度不同，特别是在高频率和低延迟应用中。

Contributions: 系统性地比较了FPGA与ASIC的时序特性，提供了基于实际器件的案例研究，量化了两类技术的时序性能差异。

Results: 7nm ASIC实现45ps建立时间和35ps保持时间，Xilinx Kintex UltraScale+ FPGA实现180ps建立时间和120ps保持时间，显示ASIC时序性能更优但FPGA仍具竞争力。

Conclusion: 尽管ASIC在时序性能上优于FPGA，但现代FPGA在性能与灵活性之间提供了良好的平衡，适用于多种高性能应用场景。

Related Work: 已有研究多集中于单独优化FPGA或ASIC的时序，缺乏对两者在相同设计目标下的直接对比分析。

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [7] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: 本文综述了基于描述符的、面向对象的内存系统，该系统通过将描述符提升为一级架构抽象，弥合了硬件/软件接口之间的语义鸿沟，能够动态获取并强制执行软件定义对象的丰富语义。文章建立了内存对象和描述符的基础概念，提出了描述符寻址模式的新分类法，并以CentroID模型为案例，展示了其实用高效的面向对象设计路径。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统的安全性和效率因缺乏在硬件/软件接口间传播高级程序语义（如对象标识、边界和生命周期）的原生架构机制而受到根本性削弱。

Challenges: 如何在硬件层面有效传播和执行软件定义对象的语义，解决内存保护、管理和处理之间的相互关联的挑战。

Contributions: 提出了描述符寻址模式的新分类法；系统地梳理了基于描述符的内存系统的发展与现状；通过统一分析揭示了该范式如何整体应对内存保护、管理和处理的挑战；重新审视了CentroID模型，展示了其混合标记指针编码和描述符处理机制。

Results: 该范式能够使硬件动态获取并强制执行软件定义对象的丰富语义；为下一代缓存层次结构、统一虚拟内存乃至128位架构提供了基础研究方向。

Conclusion: 基于描述符的、面向对象的内存系统为弥合硬件与软件之间的语义鸿沟提供了可行路径，是提升系统安全性与效率的关键方向。

Related Work: 与传统内存系统相比，该工作聚焦于将对象语义显式地跨层传递，相关研究包括能力架构、标记指针、内存安全机制等，但本文提供了更系统的分类与统一分析框架。

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>
