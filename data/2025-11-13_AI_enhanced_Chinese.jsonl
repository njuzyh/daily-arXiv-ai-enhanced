{"id": "2511.07770", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.07770", "abs": "https://arxiv.org/abs/2511.07770", "authors": ["Zewei Guo", "Zhen Jia", "JinXiao Zhu", "Wenhao Huang", "Yin Chen"], "title": "A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices", "comment": null, "summary": "Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u3001\u540c\u578b\u53f7\u8bbe\u5907\u7684\u5c04\u9891\u6307\u7eb9\u6570\u636e\u96c6\u548c\u5f00\u6e90\u5b9e\u9a8c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u8bbe\u5907\u89c4\u6a21\u5c0f\u3001\u578b\u53f7\u5f02\u6784\u7684\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b123\u4e2a\u76f8\u540c\u7684IEEE 802.11g\u8bbe\u5907\uff0c\u91c7\u96c6\u4e863542\u4e07\u6761\u539f\u59cbI/Q\u6837\u672c\u548c185\u4e07\u6761\u5c04\u9891\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u5b9e\u73b0\u4e8689.06%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u540c\u578b\u53f7\u8bbe\u5907\u786c\u4ef6\u5dee\u5f02\u6781\u5c0f\uff0c\u533a\u5206\u5b83\u4eec\u7684\u5c04\u9891\u6307\u7eb9\u4ecd\u5177\u6311\u6218\u6027\uff1b\u540c\u65f6\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u8bbe\u5907\u578b\u53f7\u591a\u6837\uff0c\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u8bad\u7ec3\u4e0e\u516c\u5e73\u8bc4\u4f30\u3002", "challenges": "\u540c\u578b\u53f7\u8bbe\u5907\u95f4\u786c\u4ef6\u5dee\u5f02\u5fae\u5f31\uff0c\u96be\u4ee5\u63d0\u53d6\u6709\u6548\u533a\u5206\u7279\u5f81\uff1b\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u540c\u578b\u53f7\u8bbe\u5907\u6570\u636e\u96c6\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "contributions": "1\uff09\u6784\u5efa\u4e86\u5305\u542b123\u4e2a\u76f8\u540c\u578b\u53f7\u8bbe\u5907\u7684\u5927\u89c4\u6a21\u5c04\u9891\u6307\u7eb9\u6570\u636e\u96c6\uff0c\u542b3542\u4e07I/Q\u6837\u672c\u548c185\u4e07\u7279\u5f81\uff1b2\uff09\u63d0\u51fa\u5b8c\u5168\u53ef\u590d\u73b0\u7684\u5f00\u6e90\u5b9e\u9a8c\u6846\u67b6\uff0c\u8986\u76d6\u6570\u636e\u91c7\u96c6\u5230\u8bc4\u4f30\u5168\u6d41\u7a0b\uff1b3\uff09\u8bbe\u8ba1\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u8bc6\u522b\u7b97\u6cd5\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.06%\u51c6\u786e\u7387\u3002", "results": "\u5728\u5927\u89c4\u6a21\u540c\u578b\u53f7\u8bbe\u5907\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u7b97\u6cd5\u5b9e\u73b0\u4e8689.06%\u7684\u8bbe\u5907\u8bc6\u522b\u51c6\u786e\u7387\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u53d6\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u8054\u6027\uff0c\u8bc1\u660e\u4e86\u7279\u5f81\u7684\u6709\u6548\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u548c\u5f00\u6e90\u6846\u67b6\u4e3a\u5c04\u9891\u6307\u7eb9\u6280\u672f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u540c\u578b\u53f7\u8bbe\u5907\u8bc6\u522b\u65b9\u9762\uff0c\u652f\u6301\u66f4\u516c\u5e73\u3001\u53ef\u590d\u73b0\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u591a\u57fa\u4e8e\u5c0f\u89c4\u6a21\u6216\u5f02\u6784\u8bbe\u5907\u6570\u636e\u96c6\u8fdb\u884c\u5c04\u9891\u6307\u7eb9\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u540c\u578b\u53f7\u8bbe\u5907\u95f4\u7ec6\u5fae\u5dee\u5f02\u7684\u6709\u6548\u5efa\u6a21\u4e0e\u8bc4\u4f30\u73af\u5883\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002"}}
{"id": "2511.07421", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07421", "abs": "https://arxiv.org/abs/2511.07421", "authors": ["Tong Qiao", "Ao Zhou", "Yingjie Qi", "Yiou Wang", "Han Wan", "Jianlei Yang", "Chunming Hu"], "title": "Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms", "comment": "Accepted by The 43rd IEEE International Conference on Computer Design, ICCD'25", "summary": "Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.", "AI": {"tldr": "A3GNN\u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784CPU-GPU\u5e73\u53f0\u7684\u7ecf\u6d4e\u3001\u81ea\u9002\u5e94\u4e14\u81ea\u52a8\u7684GNN\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u6027\u611f\u77e5\u91c7\u6837\u3001\u7ec6\u7c92\u5ea6\u5e76\u884c\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "GNN\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u6602\u8d35\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4f4e\u6210\u672c\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684GNN\u8bad\u7ec3\u3002", "challenges": "\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u9ad8\u6548\u5229\u7528CPU\u548cGPU\u8d44\u6e90\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u6570\u636e\u5c40\u90e8\u6027\u7ba1\u7406\u3001\u4efb\u52a1\u5e76\u884c\u8c03\u5ea6\u4ee5\u53ca\u5728\u541e\u5410\u91cf\u3001\u5185\u5b58\u5360\u7528\u548c\u6a21\u578b\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "contributions": "\u63d0\u51fa\u4e86A3GNN\u6846\u67b6\uff0c\u5305\u542b\u5c40\u90e8\u6027\u611f\u77e5\u91c7\u6837\u3001\u7ec6\u7c92\u5ea6\u5e76\u884c\u8c03\u5ea6\u673a\u5236\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u591a\u76ee\u6807\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e03\u5757Nvidia 2080Ti GPU\u7684A3GNN\u5728\u541e\u5410\u91cf\u4e0a\u6700\u9ad8\u53ef\u8fbe\u4e24\u5757A100 GPU\u76841.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "A3GNN\u6709\u6548\u7f29\u5c0f\u4e86\u4f4e\u6210\u672c\u4e0e\u9ad8\u6027\u80fd\u5e73\u53f0\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684GNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecGNN\u8bad\u7ec3\u4f18\u5316\u3001\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u8c03\u5ea6\u3001\u91c7\u6837\u7b97\u6cd5\u8bbe\u8ba1\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u5728\u7cfb\u7edf\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2511.07422", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07422", "abs": "https://arxiv.org/abs/2511.07422", "authors": ["Madabattula Rajesh Kumar", "Srinivasa Rao Aravilli", "Mustafa Saify", "Shashank Srivastava"], "title": "From Attention to Disaggregation: Tracing the Evolution of LLM Inference", "comment": null, "summary": "The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5b9e\u73b0\u89e3\u805a\u5f0f\u63a8\u7406\u67b6\u6784\uff0c\u4ee5\u4f18\u5316\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u7684\u589e\u957f\uff0c\u63a8\u7406\u9636\u6bb5\u7684\u5ef6\u8fdf\u3001\u541e\u5410\u548c\u6210\u672c\u6210\u4e3a\u90e8\u7f72\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4f20\u7edf\u5355\u4f53GPU\u96c6\u7fa4\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u3002", "challenges": "LLM\u63a8\u7406\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u3001\u8ba1\u7b97\u541e\u5410\u548c\u5ef6\u8fdf\u7684\u591a\u91cd\u7ea6\u675f\uff0c\u540c\u65f6\u9700\u8981\u5728\u591a\u4e2a\u76ee\u6807\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u4f18\u5316\u3002", "contributions": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u670d\u52a1\u89e3\u8026\u3001\u8d44\u6e90\u89e3\u805a\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5212\u5206\u7684\u89e3\u805a\u5f0f\u63a8\u7406\u67b6\u6784\uff0c\u5c06\u9884\u586b\u5145\u4e0e\u89e3\u7801\u9636\u6bb5\u5206\u79bb\u5e76\u72ec\u7acb\u6269\u5c55\u3002", "results": "\u8be5\u67b6\u6784\u6709\u6548\u7f13\u89e3\u4e86\u8d44\u6e90\u4e89\u7528\uff0c\u53ef\u72ec\u7acb\u4f18\u5316\u9996\u4ee4\u724c\u65f6\u95f4\u548c\u4ee4\u724c\u95f4\u5ef6\u8fdf\u7b49\u5173\u952e\u6307\u6807\u3002", "conclusion": "\u89e3\u805a\u5f0f\u63a8\u7406\u662f\u5e94\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6311\u6218\u7684\u6709\u6548\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684LLM\u670d\u52a1\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u5206\u5e03\u5f0f\u63a8\u7406\u7cfb\u7edf\u3001GPU\u8d44\u6e90\u8c03\u5ea6\u3001\u4ee5\u53ca\u9488\u5bf9LLM\u63a8\u7406\u9636\u6bb5\u7279\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2511.08282", "categories": ["cs.NI", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.08282", "abs": "https://arxiv.org/abs/2511.08282", "authors": ["Eranga Bandara", "Safdar H. Bouk", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Peter Foytik", "Ross Gore", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa"], "title": "SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services", "comment": null, "summary": "Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0fAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u533a\u5757\u94fe\u548cNFT\u7684\u65b0\u578bSRE\u5e73\u53f0SRE-Llama\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u548c\u7ba1\u7406\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684SLI/SLO\u53ca\u544a\u8b66\u673a\u5236\uff0c\u5e76\u901a\u8fc75G\u6838\u5fc3\u7f51\u7528\u4f8b\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5f00\u53d1\u4eba\u5458\u666e\u904d\u7f3a\u4e4f\u5bf9Prometheus\u3001Grafana\u7b49\u76d1\u63a7\u5de5\u5177\u4ee5\u53caSLI/SLO\u5b9a\u4e49\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4SRE\u5b9e\u8df5\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u667a\u80fd\u5316\u3001\u6613\u7528\u7684\u81ea\u52a8\u5316\u5e73\u53f0\u6765\u964d\u4f4e\u95e8\u69db\u5e76\u63d0\u5347\u6548\u7387\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5f00\u53d1\u8005\u5bf9SRE\u5de5\u5177\u548c\u6307\u6807\u5b9a\u4e49\u7684\u7406\u89e3\u4e0d\u8db3\uff1b\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u6709\u6548\u8bc6\u522b\u5173\u952e\u6027\u80fd\u6307\u6807\uff1b\u5b9e\u73b0SLI/SLO\u7684\u667a\u80fd\u751f\u6210\u4e0e\u52a8\u6001\u7ba1\u7406\uff1b\u786e\u4fddSRE\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u5ba1\u8ba1\u6027\u4e0e\u4e0d\u53ef\u7be1\u6539\u6027\u3002", "contributions": "1) \u63d0\u51faSRE-Llama\u5e73\u53f0\uff0c\u96c6\u6210\u751f\u6210\u5f0fAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u533a\u5757\u94fe\u4e0eNFT\u6280\u672f\uff1b2) \u5229\u7528\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8bc6\u522b\u5173\u952eSLI\uff1b3) \u4f7f\u7528\u5fae\u8c03\u540e\u7684Llama-3\u5927\u6a21\u578b\u81ea\u52a8\u751f\u6210SLI/SLO\u3001\u9519\u8bef\u9884\u7b97\u548c\u544a\u8b66\u89c4\u5219\uff1b4) \u5c06SLI/SLO\u7f16\u7801\u4e3aNFT\u5b58\u50a8\u4e8e\u533a\u5757\u94fe\uff0c\u5b9e\u73b0\u4e0d\u53ef\u7be1\u6539\u7684\u5ba1\u8ba1\u8ffd\u8e2a\uff1b5) \u901a\u8fc7\u667a\u80fd\u5408\u7ea6\u9a71\u52a8\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5e76\u5728Open5GS 5G\u6838\u5fc3\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u539f\u578b\u9a8c\u8bc1\u3002", "results": "\u5e73\u53f0\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u4e91\u539f\u751f\u670d\u52a1\u91c7\u96c6\u6307\u6807\u3001\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u5206\u6790\u5173\u952eSLI\u3001\u5229\u7528Llama-3\u751f\u6210SLO\u4e0e\u544a\u8b66\u7b56\u7565\u3001\u5c06\u7ed3\u679c\u7f16\u7801\u4e3aNFT\u4e0a\u94fe\u5b58\u50a8\u7684\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\uff0c\u5e76\u5728\u4e00\u4e2a\u5b9a\u5236\u5316\u7684Open5GS 5G\u6838\u5fc3\u7f51\u7528\u4f8b\u4e2d\u5b8c\u6210\u4e86\u539f\u578b\u90e8\u7f72\u4e0e\u9a8c\u8bc1\u3002", "conclusion": "SRE-Llama\u901a\u8fc7\u878d\u5408\u524d\u6cbf\u6280\u672f\u6709\u6548\u964d\u4f4e\u4e86SRE\u5b9e\u8df5\u95e8\u69db\uff0c\u63d0\u5347\u4e86SLI/SLO\u5b9a\u4e49\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u900f\u660e\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u4e91\u539f\u751f\u73af\u5883\u4e2d\u5e94\u7528AI\u4e0e\u533a\u5757\u94fe\u8fdb\u884c\u81ea\u52a8\u5316\u8fd0\u7ef4\u7684\u6f5c\u529b\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u5305\u62ec\u4f20\u7edf\u7684SRE\u5b9e\u8df5\uff08\u5982Google SRE\u624b\u518c\uff09\u3001\u57fa\u4e8ePrometheus/Grafana\u7684\u76d1\u63a7\u7cfb\u7edf\u3001\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u7684\u7814\u7a76\u3001\u533a\u5757\u94fe\u5728\u65e5\u5fd7\u5ba1\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53caNFT\u5728\u6570\u5b57\u8d44\u4ea7\u786e\u6743\u65b9\u9762\u7684\u63a2\u7d22\u3002\u672c\u6587\u5728\u8fd9\u4e9b\u57fa\u7840\u4e0a\u9996\u6b21\u5c06\u751f\u6210\u5f0fAI\u3001\u8054\u90a6\u5b66\u4e60\u4e0eNFT\u7ed3\u5408\u5e94\u7528\u4e8eSRE\u6307\u6807\u81ea\u52a8\u5316\u751f\u6210\u4e0e\u7ba1\u7406\u3002"}}
{"id": "2511.08054", "categories": ["cs.AR", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08054", "abs": "https://arxiv.org/abs/2511.08054", "authors": ["Yunqi Shi", "Xi Lin", "Zhiang Wang", "Siyuan Xu", "Shixiong Kai", "Yao Lai", "Chengrui Gao", "Ke Xue", "Mingxuan Yuan", "Chao Qian", "Zhi-Hua Zhou"], "title": "Re$^{\\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating", "comment": "IEEE Transactions on Comupter-Aided Design under review", "summary": "This work introduces the Re$^{\\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.", "AI": {"tldr": "Re$^{2}$MaP\u662f\u4e00\u79cd\u901a\u8fc7\u9012\u5f52\u539f\u578b\u6784\u5efa\u548c\u57fa\u4e8e\u6253\u5305\u6811\u91cd\u5b9a\u4f4d\u751f\u6210\u4e13\u5bb6\u7ea7\u5b8f\u5e03\u5c40\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408PPA\u611f\u77e5\u805a\u7c7b\u3001\u692d\u5706\u4e0a\u89d2\u5ea6\u4f18\u5316\u4e0e\u8fdb\u5316\u641c\u7d22\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b8f\u5e03\u5c40\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u8bbe\u8ba1\u7ea6\u675f\u548c\u4f18\u5316\u65f6\u5e8f\u3001\u529f\u8017\u3001\u5e03\u7ebf\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u517c\u987e\u5168\u5c40\u5e03\u5c40\u8d28\u91cf\u4e0e\u5c40\u90e8\u4f18\u5316\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u6a21\u62df\u4e13\u5bb6\u7ecf\u9a8c\u5e76\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u5e03\u5c40\u7684\u65b0\u65b9\u6cd5\u3002", "challenges": "\u5982\u4f55\u5728\u591a\u5c42\u7ea7\u5b8f\u5206\u7ec4\u4e0e\u5355\u5143\u805a\u7c7b\u4e2d\u7edf\u4e00\u5efa\u6a21\u8fde\u63a5\u6027\u4e0e\u6570\u636e\u6d41\uff1b\u5982\u4f55\u5728\u4fdd\u8bc1\u5e03\u7ebf\u957f\u5ea6\u548c\u6570\u636e\u6d41\u4f18\u5316\u7684\u540c\u65f6\u5c06\u5b8f\u5747\u5300\u5206\u5e03\u4e8e\u82af\u7247\u8fb9\u7f18\uff1b\u5982\u4f55\u5728\u6ee1\u8db3\u591a\u79cd\u8bbe\u8ba1\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5b8f\u7ec4\u53ca\u5176\u5185\u90e8\u5b8f\u7684\u8054\u5408\u4f18\u5316\uff1b\u5982\u4f55\u901a\u8fc7\u8fed\u4ee3\u63d0\u5347\u539f\u578b\u7cbe\u5ea6\u4ee5\u652f\u6301\u66f4\u4f18\u7684\u6700\u7ec8\u5e03\u5c40\u3002", "contributions": "\u63d0\u51fa\u4e86Re$^{2}$MaP\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1\uff09\u591a\u7ea7\u5b8f\u5206\u7ec4\u4e0ePPA\u611f\u77e5\u5355\u5143\u805a\u7c7b\u6784\u5efa\u7edf\u4e00\u8fde\u63a5\u77e9\u9635\uff1b2\uff09\u5229\u7528DREAMPlace\u751f\u6210\u6df7\u5408\u5c3a\u5bf8\u5e03\u5c40\u539f\u578b\uff1b3\uff09\u63d0\u51faABPlace\u89d2\u5ea6\u4f18\u5316\u65b9\u6cd5\u5728\u692d\u5706\u4e0a\u5206\u5e03\u5b8f\u4f4d\u7f6e\uff1b4\uff09\u8bbe\u8ba1\u57fa\u4e8e\u6253\u5305\u6811\u7684\u91cd\u5b9a\u4f4d\u7b56\u7565\uff0c\u7ed3\u5408\u4e13\u5bb6\u542f\u53d1\u5f0f\u4ee3\u4ef7\u51fd\u6570\u4e0e\u8fdb\u5316\u641c\u7d22\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff1b5\uff09\u91c7\u7528\u9012\u5f52\u8fed\u4ee3\u7b56\u7565\u9010\u6b65\u4f18\u5316\u5b50\u96c6\u5b8f\u7ec4\u4ee5\u63d0\u5347\u6574\u4f53\u5e03\u5c40\u8d28\u91cf\u3002", "results": "\u5728\u4e03\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5b66\u672f\u5de5\u5177Hier-RTLMP\uff0cRe$^{2}$MaP\u5728\u6700\u5dee\u8d1f\u65f6\u5e8f\u88d5\u91cf\uff08WNS\uff09\u4e0a\u6700\u9ad8\u63d0\u534722.22%\uff08\u5e73\u574710.26%\uff09\uff0c\u603b\u8d1f\u65f6\u5e8f\u88d5\u91cf\uff08TNS\uff09\u6700\u9ad8\u6539\u558497.91%\uff08\u5e73\u574733.97%\uff09\uff1b\u540c\u65f6\u5728WNS\u3001TNS\u3001\u529f\u8017\u3001DRC\u8fdd\u89c4\u6570\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4f1a\u8bae\u7248\u672cReMaP\u3002", "conclusion": "Re$^{2}$MaP\u901a\u8fc7\u9012\u5f52\u539f\u578b\u6784\u5efa\u4e0e\u57fa\u4e8e\u4e13\u5bb6\u7ecf\u9a8c\u7684\u6253\u5305\u6811\u91cd\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b8f\u5e03\u5c40\u81ea\u52a8\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u9879\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7efc\u5408\u6027\u80fd\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "related_work": "\u4e0eHier-RTLMP\u7b49\u5206\u5c42\u5b8f\u5e03\u5c40\u65b9\u6cd5\u76f8\u6bd4\uff0cRe$^{2}$MaP\u5f15\u5165\u4e86\u66f4\u7cbe\u7ec6\u7684PPA\u611f\u77e5\u805a\u7c7b\u4e0e\u692d\u5706\u89d2\u5ea6\u4f18\u5316\uff1b\u76f8\u8f83\u4e8e\u4f20\u7edf\u89e3\u6790\u5e03\u5c40\u65b9\u6cd5\uff08\u5982DREAMPlace\uff09\uff0c\u5176\u7ed3\u5408\u4e86ABPlace\u4e0e\u8fdb\u5316\u641c\u7d22\u4ee5\u589e\u5f3a\u7ea6\u675f\u6ee1\u8db3\u80fd\u529b\uff1b\u4e0e\u65e9\u671fReMaP\u76f8\u6bd4\uff0c\u901a\u8fc7\u9012\u5f52\u673a\u5236\u63d0\u5347\u4e86\u5e03\u5c40\u7cbe\u5ea6\u4e0e\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2511.07423", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07423", "abs": "https://arxiv.org/abs/2511.07423", "authors": ["Genglin Wang", "Liekang Zeng", "Bufang Yang", "Kaiwei Liu", "Guoliang Xing", "Chumin Sun", "Li Zhou", "Jie Sun", "Zhenyu Yan"], "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale", "comment": null, "summary": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.", "AI": {"tldr": "Synera\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u5907-\u4e91\u534f\u540c\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684SLM-LLM\u534f\u540c\u673a\u5236\uff0c\u4f18\u5316\u4e86\u79fb\u52a8\u573a\u666f\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u4e0e\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u964d\u4f4e\u4e86\u4e91\u670d\u52a1\u6210\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u7aef\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u9762\u4e34\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u548c\u5ef6\u8fdf\u589e\u52a0\u7684\u95ee\u9898\u3002\u4f20\u7edf\u7684\u4e91\u5378\u8f7d\u6216\u672c\u5730\u5c0f\u6a21\u578b\u65b9\u6848\u5b58\u5728\u901a\u4fe1\u74f6\u9888\u6216\u8d28\u91cf\u727a\u7272\u7684\u5c40\u9650\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u534f\u540c\u63a8\u7406\u65b9\u6848\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5982\u4f55\u505a\u51fa\u9ad8\u6548\u7684\u5378\u8f7d\u51b3\u7b56\uff0c\u907f\u514d\u6d41\u6c34\u7ebf\u505c\u987f\uff0c\u89e3\u51b3\u6279\u5904\u7406\u74f6\u9888\uff0c\u4ee5\u53ca\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u548c\u9ad8\u901a\u4fe1\u5f00\u9500\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "contributions": "\u672c\u6587\u63d0\u51fa\u4e86Synera\u7cfb\u7edf\uff0c\u8d21\u732e\u5305\u62ec\uff1a1\uff09\u8bc6\u522b\u4e86\u8bbe\u5907-\u4e91\u534f\u540c\u63a8\u7406\u4e2d\u7684\u4f18\u5316\u673a\u4f1a\uff1b2\uff09\u8bbe\u8ba1\u4e86\u901a\u4fe1\u9ad8\u6548\u7684 selective offloading\u3001\u65e0\u963b\u585e\u5e76\u884c\u63a8\u7406\u548c\u53ef\u6269\u5c55\u7684\u4e91\u6279\u5904\u7406\u673a\u5236\uff1b3\uff09\u5b9e\u73b0\u4e86\u8d28\u91cf\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u7684\u7efc\u5408\u4f18\u5316\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0cSynera\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u76f8\u540c\u5ef6\u8fdf\u4e0b\u63d0\u5347\u4e861.20-5.47\u500d\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u6bd4\u73b0\u6709\u4e91\u670d\u52a1\u964d\u4f4e\u4e868.2%-16.5%\u7684\u4e91\u670d\u52a1\u6210\u672c\u3002", "conclusion": "Synera\u901a\u8fc7\u8bbe\u5907-\u4e91\u534f\u540c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u7aefLLM\u670d\u52a1\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u7aef\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684\u90e8\u7f72\u65b9\u6848\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u5305\u62ec\u57fa\u4e8e\u4e91\u7684\u6a21\u578b\u5378\u8f7d\u65b9\u6cd5\u548c\u672c\u5730\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u90e8\u7f72\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5206\u522b\u53d7\u9650\u4e8e\u901a\u4fe1\u74f6\u9888\u548c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u672a\u80fd\u5145\u5206\u5e73\u8861\u8d28\u91cf\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.08375", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.08375", "abs": "https://arxiv.org/abs/2511.08375", "authors": ["Darius Saif", "Ashraf Matrawy"], "title": "Demystifying QUIC from the Specifications", "comment": null, "summary": "QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u4ee5\u901a\u4fd7\u6613\u61c2\u7684\u65b9\u5f0f\u5168\u9762\u4ecb\u7ecdQUIC\u534f\u8bae\uff0c\u5e2e\u52a9\u8bfb\u8005\u7406\u89e3\u5176\u89c4\u8303\uff0c\u6d88\u9664\u56e0\u534f\u8bae\u5feb\u901f\u6f14\u8fdb\u3001\u6587\u6863\u590d\u6742\u53ca\u8de8\u5c42\u9690\u79c1\u8bbe\u8ba1\u5e26\u6765\u7684\u5b66\u4e60\u969c\u788d\u3002", "motivation": "\u7531\u4e8eQUIC\u534f\u8bae\u53d1\u5c55\u8fc5\u901f\u3001\u6587\u6863\u5206\u6563\u4e14\u5b9e\u73b0\u590d\u6742\uff0c\u521d\u5b66\u8005\u96be\u4ee5\u638c\u63e1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u7bc7\u7cfb\u7edf\u800c\u6613\u61c2\u7684\u7efc\u8ff0\u6027\u4ecb\u7ecd\u3002", "challenges": "\u5b66\u4e60QUIC\u7684\u4e3b\u8981\u56f0\u96be\u5305\u62ec\uff1a\u534f\u8bae\u5feb\u901f\u6f14\u8fdb\uff08\u5982\u6807\u51c6QUIC\u4e0e\u5df2\u5f03\u7528\u7684Google QUIC\u7684\u533a\u522b\uff09\u3001\u591a\u4e2aRFC\u6587\u6863\u7ed3\u6784\u590d\u6742\u3001\u8bed\u8a00\u6666\u6da9\uff0c\u4ee5\u53ca\u5176\u8de8\u5c42\u548c\u4ee5\u9690\u79c1\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u5bfc\u81f4\u65e0\u6cd5\u4ec5\u901a\u8fc7\u6570\u636e\u5305\u5206\u6790\u6765\u7406\u89e3\u6216\u8c03\u8bd5\u3002", "contributions": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u68b3\u7406\u5e76\u7b80\u5316\u4e86QUIC\u534f\u8bae\u7684\u6838\u5fc3\u5185\u5bb9\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u7406\u89e3\u7684\u8bb2\u89e3\u65b9\u5f0f\uff0c\u5e2e\u52a9\u8bfb\u8005\u514b\u670d\u5b66\u4e60\u969c\u788d\uff0c\u6df1\u5165\u7406\u89e3QUIC\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u3002", "results": "\u8bba\u6587\u6210\u529f\u5730\u5c06\u590d\u6742\u7684QUIC\u534f\u8bae\u4ee5\u6e05\u6670\u3001\u8fde\u8d2f\u548c\u53ef\u8bbf\u95ee\u7684\u65b9\u5f0f\u5448\u73b0\uff0c\u6709\u52a9\u4e8e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528QUIC\u3002", "conclusion": "\u901a\u8fc7\u672c\u6587\u7684\u68b3\u7406\uff0cQUIC\u534f\u8bae\u7684\u590d\u6742\u6027\u5f97\u4ee5\u964d\u4f4e\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728TCP\u3001TLS\u4ee5\u53caHTTP/2\u7b49\u4f20\u7edf\u534f\u8bae\u7684\u6f14\u8fdb\uff0c\u800cQUIC\u4f5c\u4e3a\u878d\u5408\u4f20\u8f93\u4e0e\u5b89\u5168\u7684\u65b0\u4e00\u4ee3\u534f\u8bae\uff0c\u4ee3\u8868\u4e86\u4e92\u8054\u7f51\u534f\u8bae\u53d1\u5c55\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.08395", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08395", "abs": "https://arxiv.org/abs/2511.08395", "authors": ["Xingyu Liu", "Jiawei Liang", "Yipu Zhang", "Linfeng Du", "Chaofang Ma", "Hui Yu", "Jiang Xu", "Wei Zhang"], "title": "DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator", "comment": null, "summary": "We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u786c\u4ef6\u9ad8\u6548RBD\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u3001\u5ef6\u8fdf\u9664\u6cd5\u4f18\u5316\u548c\u6a21\u5757\u95f4DSP\u590d\u7528\u4e09\u9879\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u521a\u4f53\u52a8\u529b\u5b66\uff08RBD\uff09\u8ba1\u7b97\u7684\u6548\u7387\uff0c\u89e3\u51b3\u73b0\u6709\u52a0\u901f\u5668\u5728\u8ba1\u7b97\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u786c\u4ef6\u8d44\u6e90\u5229\u7528\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "challenges": "\u5982\u4f55\u5728\u4fdd\u8bc1\u673a\u5668\u4eba\u63a7\u5236\u548c\u8fd0\u52a8\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4e\u5bf9DSP\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u540c\u65f6\u51cf\u5c11\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\u3002", "contributions": "1) \u63d0\u51fa\u4e86\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u91cf\u5316\u5bf9\u673a\u5668\u4eba\u63a7\u5236\u548c\u8fd0\u52a8\u7684\u5f71\u54cd\uff1b2) \u5f15\u5165\u9664\u6cd5\u5ef6\u8fdf\u4f18\u5316\u6280\u672f\uff0c\u89e3\u8026\u5012\u6570\u8fd0\u7b97\u4e0e\u6700\u957f\u5ef6\u8fdf\u8def\u5f84\uff1b3) \u63d0\u51fa\u6a21\u5757\u95f4DSP\u590d\u7528\u65b9\u6cd5\uff0c\u63d0\u9ad8DSP\u5229\u7528\u7387\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RBD\u52a0\u901f\u5668\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad88\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c7.4\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RBD\u52a0\u901f\u5668\u5728\u6027\u80fd\u3001\u5ef6\u8fdf\u548c\u786c\u4ef6\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "related_work": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u7528\u52a0\u901f\u67b6\u6784\u6216\u672a\u5145\u5206\u4f18\u5316\u7684RBD\u5b9e\u73b0\uff0c\u7f3a\u4e4f\u5bf9\u91cf\u5316\u5f71\u54cd\u7684\u7cfb\u7edf\u5206\u6790\u53ca\u5173\u952e\u8def\u5f84\u4f18\u5316\u3002"}}
{"id": "2511.07425", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07425", "abs": "https://arxiv.org/abs/2511.07425", "authors": ["Tung", "Nguyen", "Tuyen Nguyen"], "title": "An Evaluation of LLMs Inference on Popular Single-board Computers", "comment": "9 pages, 3 figures", "summary": "The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf925\u4e2a\u91cf\u5316\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6811\u8393\u6d3e4\u3001\u6811\u8393\u6d3e5\u548cOrange Pi 5 Pro\u4e09\u79cd\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86Ollama\u548cLlamafile\u4e24\u79cd\u63a8\u7406\u8fd0\u884c\u65f6\u7684\u751f\u6210\u541e\u5410\u91cf\u3001\u5185\u5b58\u4f7f\u7528\u548c\u529f\u8017\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5355\u677f\u8ba1\u7b97\u673a\u53ef\u7a33\u5b9a\u652f\u6301\u6700\u9ad815\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u4e14Llamafile\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e0a\u663e\u8457\u4f18\u4e8eOllama\u3002\u7814\u7a76\u63ed\u793a\u4e86\u67b6\u6784\u7ea7\u74f6\u9888\u4e0e\u8fd0\u884c\u65f6\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u90e8\u7f72\u5efa\u8bae\uff0c\u586b\u8865\u4e86\u9ad8\u6027\u80fd\u8bed\u8a00\u6a21\u578b\u4e0e\u4f4e\u6210\u672c\u8fb9\u7f18\u8ba1\u7b97\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u968f\u7740\u5bf9\u8bbe\u5907\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9700\u6c42\u7684\u589e\u957f\uff0c\u4e9f\u9700\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7AI\u89e3\u51b3\u65b9\u6848\u3002\u5355\u677f\u8ba1\u7b97\u673a\uff08\u5982\u6811\u8393\u6d3e\uff09\u5177\u5907\u672c\u5730\u5316\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u5927\u6a21\u578b\u63a8\u7406\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u5c55\u5b9e\u8bc1\u7814\u7a76\u4ee5\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u3002", "challenges": "\u5355\u677f\u8ba1\u7b97\u673a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\uff1b\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\u548c\u63a8\u7406\u8fd0\u884c\u65f6\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff1b\u5728\u4f4e\u529f\u8017\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u5b58\u5728\u74f6\u9888\uff1b\u7f3a\u4e4f\u9488\u5bf9SBC\u5e73\u53f0\u7684\u7cfb\u7edf\u6027LLM\u6027\u80fd\u57fa\u51c6\u6570\u636e\u3002", "contributions": "1\uff09\u9996\u6b21\u5bf925\u4e2a\u91cf\u5316\u5f00\u6e90LLM\u5728\u4e09\u79cd\u4e3b\u6d41\u5355\u677f\u8ba1\u7b97\u673a\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u8fdb\u884c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff1b2\uff09\u5bf9\u6bd4\u5206\u6790Ollama\u4e0eLlamafile\u4e24\u79cd\u8fd0\u884c\u65f6\u5728\u541e\u5410\u91cf\u3001\u5185\u5b58\u548c\u529f\u8017\u65b9\u9762\u7684\u8868\u73b0\u5dee\u5f02\uff1b3\uff09\u8bc6\u522b\u51fa\u786c\u4ef6\u67b6\u6784\u4e0e\u8fd0\u884c\u65f6\u5c42\u9762\u7684\u5173\u952e\u6027\u80fd\u74f6\u9888\uff1b4\uff09\u63d0\u4f9b\u9762\u5411\u5b9e\u9645\u90e8\u7f72\u7684\u4f18\u5316\u5efa\u8bae\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u843d\u5730\u5e94\u7528\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5355\u677f\u8ba1\u7b97\u673a\u53ef\u7a33\u5b9a\u8fd0\u884c\u6700\u9ad81.5B\u53c2\u6570\u7684\u91cf\u5316\u6a21\u578b\uff1b2\uff09Llamafile\u76f8\u6bd4Ollama\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53474\u500d\uff0c\u529f\u8017\u964d\u4f4e30-40%\uff1b3\uff09CPU\u914d\u7f6e\u548c\u6a21\u578b\u5927\u5c0f\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff1b4\uff09\u4e0d\u540c\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u63a8\u7406\u6548\u7387\u5b58\u5728\u5dee\u5f02\uff0c\u53cd\u6620\u51fa\u73b0\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u5355\u677f\u8ba1\u7b97\u673a\u80fd\u591f\u6709\u6548\u652f\u6301\u4e2d\u5c0f\u578b\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672c\u5730\u63a8\u7406\uff0c\u5c24\u5176\u5728\u4f7f\u7528\u9ad8\u6548\u8fd0\u884c\u65f6\uff08\u5982Llamafile\uff09\u65f6\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u3002\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u8fd0\u884cLLM\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u3001\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8df5\u4f9d\u636e\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u670d\u52a1\u5668\u7ea7GPU\u4e0a\u7684\u5927\u6a21\u578b\u63a8\u7406\u4f18\u5316\uff0c\u4ee5\u53ca\u79fb\u52a8\u7aef\u6216\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u90e8\u7f72\u3002\u5df2\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u5355\u677f\u8ba1\u7b97\u673a\u5e73\u53f0\u4e0a\u7684\u5f00\u6e90LLM\u6027\u80fd\u8868\u73b0\uff0c\u7f3a\u4e4f\u8de8\u786c\u4ef6\u3001\u8de8\u8fd0\u884c\u65f6\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002"}}
{"id": "2511.08575", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08575", "abs": "https://arxiv.org/abs/2511.08575", "authors": ["Zhenxiao Fu", "Chen Fan", "Lei Jiang"], "title": "CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices", "comment": null, "summary": "LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CO2-Meter\uff0c\u4e00\u4e2a\u7528\u4e8e\u4f30\u7b97\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fd0\u884c\u65f6\u548c\u5185\u542b\u78b3\u6392\u653e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u5168\u9762\u7684\u80fd\u8017\u5efa\u6a21\u548c\u9a8c\u8bc1\uff0c\u6709\u6548\u8bc6\u522b\u78b3\u6392\u653e\u70ed\u70b9\u5e76\u6307\u5bfc\u7eff\u8272LLM\u8bbe\u8ba1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u5e26\u6765\u4e86\u663e\u8457\u7684\u78b3\u6392\u653e\u6311\u6218\uff0c\u800c\u73b0\u6709\u78b3\u6392\u653e\u4f30\u7b97\u65b9\u6cd5\u4e0d\u5b8c\u6574\uff0c\u5ffd\u7565\u4e86\u5916\u8bbe\u80fd\u8017\u3001\u9884\u586b\u5145/\u89e3\u7801\u9636\u6bb5\u5dee\u5f02\u4ee5\u53caSoC\u8bbe\u8ba1\u590d\u6742\u6027\u3002", "challenges": "1. \u5916\u56f4\u8bbe\u5907\u80fd\u8017\u7684\u51c6\u786e\u5efa\u6a21\uff1b2. \u533a\u5206LLM\u63a8\u7406\u4e2d\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u4e0d\u540c\u80fd\u8017\u884c\u4e3a\uff1b3. \u8003\u8651SoC\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u590d\u6742\u6027\u5bf9\u78b3\u6392\u653e\u7684\u5f71\u54cd\uff1b4. \u6784\u5efa\u5305\u542b\u8fd0\u884c\u65f6\u80fd\u8017\u4e0e\u786c\u4ef6\u5236\u9020\u5185\u542b\u78b3\u7684\u7edf\u4e00\u4f30\u7b97\u6846\u67b6\u3002", "contributions": "1. \u57fa\u4e8e\u65b9\u7a0b\u7684\u5916\u8bbe\u80fd\u8017\u6a21\u578b\u4e0e\u6570\u636e\u96c6\uff1b2. \u7ed3\u5408\u76f8\u4f4d\u7279\u5b9aLLM\u80fd\u8017\u6570\u636e\u7684GNN\u9884\u6d4b\u5668\uff1b3. \u7528\u4e8eSoC\u74f6\u9888\u5206\u6790\u7684\u5355\u5143\u7ea7\u5185\u542b\u78b3\u6a21\u578b\uff1b4. \u9a8c\u8bc1\u8868\u660e\u5176\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "results": "CO2-Meter\u5728\u591a\u79cd\u8fb9\u7f18\u5e73\u53f0\u548cLLM\u914d\u7f6e\u4e0b\u9a8c\u8bc1\uff0c\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u4f30\u7b97\u7cbe\u5ea6\uff1b\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u5176\u80fd\u6709\u6548\u8bc6\u522b\u78b3\u6392\u653e\u70ed\u70b9\uff0c\u652f\u6301\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u53ef\u6301\u7eed\u7684LLM\u90e8\u7f72\u4e0e\u4f18\u5316\u8bbe\u8ba1\u3002", "conclusion": "CO2-Meter\u4e3a\u8fb9\u7f18LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u7cbe\u786e\u7684\u78b3\u8db3\u8ff9\u8bc4\u4f30\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7eff\u8272AI\u7684\u53d1\u5c55\uff0c\u652f\u6301\u4ece\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u534f\u540c\u53ef\u6301\u7eed\u8bbe\u8ba1\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u4e2d\u5fc3\u7ea7LLM\u78b3\u6392\u653e\u4f30\u7b97\u6216\u7b80\u5316\u7684\u80fd\u8017\u6a21\u578b\uff0c\u7f3a\u4e4f\u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5916\u56f4\u7ec4\u4ef6\u3001\u63a8\u7406\u9636\u6bb5\u5dee\u5f02\u53caSoC\u5c42\u9762\u786c\u4ef6\u78b3\u6210\u672c\u7684\u7cfb\u7edf\u6027\u5efa\u6a21\u3002"}}
{"id": "2511.07426", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07426", "abs": "https://arxiv.org/abs/2511.07426", "authors": ["Zihao Ding", "Mufeng Zhu", "Yao Liu"], "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents", "comment": null, "summary": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.", "AI": {"tldr": "\u672c\u6587\u5bf9Model Context Protocol (MCP) \u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u57fa\u4e8e\u6d4b\u91cf\u7684\u5206\u6790\uff0c\u63ed\u793a\u4e86\u529f\u80fd\u3001\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6a21\u578b\u548c\u914d\u7f6e\u5bf9\u4ee4\u724c\u6548\u7387\u3001\u6210\u672c\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6210\u529f\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5e76\u884c\u5de5\u5177\u8c03\u7528\u548c\u4efb\u52a1\u4e2d\u6b62\u673a\u5236\u7b49\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "MCP\u589e\u5f3a\u4e86LLM\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u4f46\u5f15\u5165\u5927\u91cf\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bfc\u81f4\u4ee4\u724c\u4f7f\u7528\u91cf\u6fc0\u589e\uff0c\u5e26\u6765\u9ad8\u6602\u7684\u6210\u672c\u548c\u8ba1\u7b97\u8d1f\u62c5\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u4ee3\u4ef7\u4e0e\u6536\u76ca\u3002", "challenges": "MCP\u5e26\u6765\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u663e\u8457\u589e\u52a0\u4e86token\u6d88\u8017\uff0c\u5bfc\u81f4\u670d\u52a1\u6210\u672c\u4e0a\u5347\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u529f\u80fd\u589e\u5f3a\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u662f\u4e00\u5927\u6311\u6218\u3002", "contributions": "\u672c\u6587\u9996\u6b21\u5bf9MCP\u542f\u7528\u7684LLM\u4ea4\u4e92\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d4b\u91cf\u5206\u6790\uff0c\u91cf\u5316\u4e86\u4e0d\u540c\u6a21\u578b\u548c\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5982\u5e76\u884c\u5de5\u5177\u8c03\u7528\u548c\u4efb\u52a1\u4e2d\u6b62\u673a\u5236\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e0d\u540cLLM\u6a21\u578b\u548cMCP\u914d\u7f6e\u5728token\u6548\u7387\u3001\u6210\u672c\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6210\u529f\u7387\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u542f\u7528\u5e76\u884c\u5de5\u5177\u8c03\u7528\u53ef\u63d0\u5347\u6548\u7387\uff0c\u800c\u6709\u6548\u7684\u4efb\u52a1\u4e2d\u6b62\u673a\u5236\u6709\u52a9\u4e8e\u63a7\u5236\u8d44\u6e90\u6d6a\u8d39\u3002", "conclusion": "\u5c3d\u7ba1MCP\u63d0\u5347\u4e86LLM\u7684\u80fd\u529b\uff0c\u4f46\u5176\u9ad8\u6210\u672c\u548c\u6027\u80fd\u5f00\u9500\u4e0d\u5bb9\u5ffd\u89c6\uff1b\u901a\u8fc7\u5408\u7406\u914d\u7f6e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u529f\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684MCP\u96c6\u6210\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728LLM\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u5de5\u5177\u8c03\u7528\u673a\u5236\u8bbe\u8ba1\u4ee5\u53caAPI\u6210\u672c\u7ba1\u7406\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9MCP\u534f\u8bae\u7684\u7cfb\u7edf\u6027\u5b9e\u8bc1\u5206\u6790\u3002"}}
{"id": "2511.07427", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07427", "abs": "https://arxiv.org/abs/2511.07427", "authors": ["Tuowei Wang", "Minxing Huang", "Fengzu Li", "Ligeng Chen", "Jinrui Zhang", "Ju Ren"], "title": "DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones", "comment": null, "summary": "As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.\n  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\\times$ in accuracy and $1.47\\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDynaKV\uff0c\u4e00\u79cd\u9762\u5411\u667a\u80fd\u624b\u673a\u7684\u81ea\u9002\u5e94KVCache\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc1\u79fb\u65e0\u5173\u7684\u805a\u7c7b\u81ea\u9002\u5e94\u3001\u8fde\u7eed\u6027\u611f\u77e5\u7684\u95ea\u5b58\u7ba1\u7406\u548c\u9ad8\u6548\u7684\u7f13\u5b58\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u89e3\u7801\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8eDRAM\u5bb9\u91cf\u6709\u9650\uff0c\u667a\u80fd\u624b\u673a\u4e0a\u7684\u957f\u5e8f\u5217\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u53d7\u5230KVCache\u5185\u5b58\u5360\u7528\u7684\u9650\u5236\u3002\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u56e0\u805a\u7c7b\u7d22\u5f15\u9759\u6001\u6216\u5c40\u90e8\u66f4\u65b0\uff0c\u65e0\u6cd5\u9002\u5e94KVCache\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u68c0\u7d22\u4e0d\u51c6\u786e\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "challenges": "1) KVCache\u5206\u5e03\u968f\u89e3\u7801\u8fc7\u7a0b\u52a8\u6001\u53d8\u5316\uff0c\u9759\u6001\u805a\u7c7b\u6613\u5931\u6548\uff1b2) \u667a\u80fd\u624b\u673a\u5e26\u5bbd\u3001IOPS\u548c\u5185\u5b58\u53d7\u9650\uff1b3) \u4f20\u7edf\u65b9\u6cd5\u5728\u66f4\u65b0\u805a\u7c7b\u65f6\u5f15\u5165\u989d\u5916\u6570\u636e\u8fc1\u79fb\u5f00\u9500\u3002", "contributions": "1) \u63d0\u51fa\u9996\u4e2a\u9762\u5411\u624b\u673a\u7684\u81ea\u9002\u5e94KVCache\u7ba1\u7406\u6846\u67b6DynaKV\uff1b2) \u8bbe\u8ba1\u65e0\u9700\u8fc1\u79fb\u7684\u52a8\u6001\u805a\u7c7b\u5206\u88c2\u673a\u5236\uff1b3) \u63d0\u51fa\u8fde\u7eed\u6027\u4e2d\u5fc3\u5316\u7684\u95ea\u5b58\u5e03\u5c40\u4e0e\u53cc\u5934\u7ed3\u6784\uff1b4) \u5b9e\u73b0\u8de8DRAM\u4e0e\u95ea\u5b58\u7684\u865a\u62df\u7f13\u5b58\u4e0e\u96c6\u7fa4\u611f\u77e5\u66ff\u6362\u7b56\u7565\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0cDynaKV\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u53471.38\u500d\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b01.47\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "DynaKV\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u624b\u673a\u4e0a\u957f\u5e8f\u5217LLM\u89e3\u7801\u4e2dKVCache\u7ba1\u7406\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u95ee\u9898\uff0c\u5177\u5907\u5411\u5176\u4ed6\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u548c\u591a\u5c42\u5185\u5b58\u7cfb\u7edf\u6269\u5c55\u7684\u6f5c\u529b\u3002", "related_work": "\u57fa\u4e8e\u805a\u7c7b\u7684KVCache\u68c0\u7d22\u65b9\u6cd5\uff0c\u5982vLLM\u3001PagedAttention\u7b49\uff0c\u91c7\u7528\u9759\u6001\u6216\u5c40\u90e8\u66f4\u65b0\u805a\u7c7b\u7b56\u7565\uff0c\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u6027\u80fd\u53d7\u9650\u3002"}}
{"id": "2511.07574", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07574", "abs": "https://arxiv.org/abs/2511.07574", "authors": ["Vasilis Bountris", "Lauritz Thamsen", "Ulf Leser"], "title": "HyProv: Hybrid Provenance Management for Scientific Workflows", "comment": "10 pages, 2 figures", "summary": "Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.\n  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HyProv\uff0c\u4e00\u79cd\u6df7\u5408\u578b\u6eaf\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4e0e\u8054\u90a6\u5f0f\u67b6\u6784\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u4e14\u5177\u5907\u5de5\u4f5c\u6d41\u611f\u77e5\u80fd\u529b\u7684\u6eaf\u6e90\u67e5\u8be2\u3002\u8be5\u7cfb\u7edf\u5728Airflow\u4e0eKubernetes\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6269\u5c55\u6027\u53ca\u4f4e\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u5de5\u4f5c\u6d41\u65e5\u76ca\u590d\u6742\u5e76\u8fd0\u884c\u5728\u5206\u5e03\u5f0f\u96c6\u7fa4\u4e0a\uff0c\u73b0\u6709\u6eaf\u6e90\u7cfb\u7edf\u96be\u4ee5\u517c\u987e\u53ef\u6269\u5c55\u6027\u3001\u5b9e\u65f6\u5904\u7406\u3001\u5728\u7ebf\u5206\u6790\u53ca\u8de8\u7ec4\u4ef6\u96c6\u6210\u3002\u6b64\u5916\uff0c\u591a\u6570\u7cfb\u7edf\u7f3a\u4e4f\u5de5\u4f5c\u6d41\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5229\u7528\u5de5\u4f5c\u6d41\u7ed3\u6784\u8fdb\u884c\u4f18\u5316\u3002", "challenges": "\u5982\u4f55\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u6eaf\u6e90\u6570\u636e\u6536\u96c6\u4e0e\u96c6\u6210\uff1b\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u65e2\u652f\u6301\u590d\u6742\u67e5\u8be2\u53c8\u5177\u5907\u5de5\u4f5c\u6d41\u611f\u77e5\u80fd\u529b\u7684\u6eaf\u6e90\u7cfb\u7edf\uff1b\u5982\u4f55\u5e73\u8861\u96c6\u4e2d\u5f0f\u7ba1\u7406\u7684\u6548\u7387\u4e0e\u8054\u90a6\u5f0f\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u3002", "contributions": "\u63d0\u51fa\u4e86HyProv\uff0c\u4e00\u79cd\u7ed3\u5408\u96c6\u4e2d\u5f0f\u4e0e\u8054\u90a6\u5f0f\u67b6\u6784\u7684\u6df7\u5408\u6eaf\u6e90\u7ba1\u7406\u7cfb\u7edf\uff1b\u8bbe\u8ba1\u4e86\u9488\u5bf9\u5de5\u4f5c\u6d41\u89c4\u8303\u7684\u5c0f\u89c4\u6a21\u7a33\u5b9a\u6eaf\u6e90\u6570\u636e\u7684\u96c6\u4e2d\u7ba1\u7406\u673a\u5236\uff1b\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u6267\u884c\u65e5\u5fd7\u7684\u8054\u90a6\u67e5\u8be2\u652f\u6301\uff1b\u652f\u6301\u590d\u6742\u6eaf\u6e90\u67e5\u8be2\u5e76\u5728Airflow+Kubernetes\u73af\u5883\u4e0b\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0cHyProv\u80fd\u591f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u5de5\u4f5c\u6d41\uff1b\u6eaf\u6e90\u67e5\u8be2\u54cd\u5e94\u65f6\u95f4\u5728\u4e9a\u79d2\u7ea7\uff1b\u5bf9\u96c6\u7fa4\u7684CPU\u548c\u5185\u5b58\u5f00\u9500\u8f83\u5c0f\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u6eaf\u6e90\u5206\u6790\u3002", "conclusion": "HyProv\u901a\u8fc7\u878d\u5408\u96c6\u4e2d\u5f0f\u4e0e\u8054\u90a6\u5f0f\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u6eaf\u6e90\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u5b9e\u65f6\u6027\u96be\u9898\uff0c\u5177\u5907\u5de5\u4f5c\u6d41\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4f4e\u5f00\u9500\u4e0b\u652f\u6301\u590d\u6742\u67e5\u8be2\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u5206\u5e03\u5f0f\u5de5\u4f5c\u6d41\u7cfb\u7edf\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u6eaf\u6e90\u7cfb\u7edf\uff08\u5982ProvSQL\uff09\u3001\u8054\u90a6\u5f0f\u6eaf\u6e90\u6846\u67b6\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u6d41\u7cfb\u7edf\u7684\u76d1\u63a7\u5de5\u5177\uff08\u5982Apache Airflow\u81ea\u5e26\u7684\u5143\u6570\u636e\u8bb0\u5f55\uff09\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u6d41\u7ed3\u6784\u7684\u5229\u7528\u6216\u65e0\u6cd5\u652f\u6301\u5b9e\u65f6\u5728\u7ebf\u5206\u6790\u3002"}}
{"id": "2511.07885", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07885", "abs": "https://arxiv.org/abs/2511.07885", "authors": ["Jon Saad-Falcon", "Avanika Narayan", "Hakki Orhun Akengin", "J. Wes Griffin", "Herumb Shandilya", "Adrian Gamarra Lafuente", "Medhya Goel", "Rebecca Joseph", "Shlok Natarajan", "Etash Kumar Guha", "Shang Zhu", "Ben Athiwaratkun", "John Hennessy", "Azalia Mirhoseini", "Christopher R\u00e9"], "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI", "comment": null, "summary": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u672c\u5730\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08\u226420B\u53c2\u6570\uff09\u5728\u672c\u5730\u52a0\u901f\u5668\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u7f13\u89e3\u96c6\u4e2d\u5f0f\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u538b\u529b\u3002\u901a\u8fc7\u5bf920\u591a\u4e2a\u5148\u8fdb\u672c\u5730\u6a21\u578b\u30018\u79cd\u52a0\u901f\u5668\u548c100\u4e07\u771f\u5b9e\u5355\u8f6e\u5bf9\u8bdd\u4e0e\u63a8\u7406\u67e5\u8be2\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u201c\u6bcf\u74e6\u667a\u80fd\u201d\uff08IPW\uff09\u4f5c\u4e3a\u8861\u91cf\u672c\u5730\u63a8\u7406\u80fd\u529b\u4e0e\u6548\u7387\u7684\u5173\u952e\u6307\u6807\u3002\u7ed3\u679c\u663e\u793a\uff1a\u672c\u5730\u6a21\u578b\u53ef\u51c6\u786e\u56de\u7b5488.7%\u7684\u67e5\u8be2\uff1b2023\u20132025\u5e74IPW\u63d0\u53475.3\u500d\uff0c\u67e5\u8be2\u8986\u76d6\u7387\u8fbe71.3%\uff1b\u672c\u5730\u52a0\u901f\u5668IPW\u4ecd\u4f4e\u4e8e\u4e91\u7aef\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002\u7814\u7a76\u8868\u660e\u672c\u5730\u63a8\u7406\u5177\u5907\u663e\u8457\u5206\u6d41\u6f5c\u529b\uff0c\u5e76\u5f00\u6e90IPW\u8bc4\u6d4b\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u9700\u6c42\u6fc0\u589e\uff0c\u96c6\u4e2d\u5f0f\u4e91\u57fa\u7840\u8bbe\u65bd\u9762\u4e34\u6269\u5c55\u74f6\u9888\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u5347\u548c\u672c\u5730\u786c\u4ef6\u7b97\u529b\u589e\u5f3a\u4e3a\u5c06\u90e8\u5206\u63a8\u7406\u4efb\u52a1\u4e0b\u653e\u5230\u7ec8\u7aef\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u672c\u5730\u63a8\u7406\u662f\u5426\u80fd\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u5206\u62c5\u4e91\u7aef\u8d1f\u8f7d\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u672c\u5730\u8bbe\u5907\uff08\u5982\u7b14\u8bb0\u672c\u7535\u8111\uff09\u4e0a\u9ad8\u6548\u8fd0\u884c\u8bed\u8a00\u6a21\u578b\uff1b\u5982\u4f55\u5e73\u8861\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u80fd\u8017\uff1b\u7f3a\u4e4f\u7edf\u4e00\u6307\u6807\u6765\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b-\u52a0\u901f\u5668\u7ec4\u5408\u5728\u672c\u5730\u73af\u5883\u4e0b\u7684\u7efc\u5408\u8868\u73b0\u3002", "contributions": "1) \u63d0\u51fa\u201c\u6bcf\u74e6\u667a\u80fd\u201d\uff08Intelligence Per Watt, IPW\uff09\u4f5c\u4e3a\u8861\u91cf\u672c\u5730\u63a8\u7406\u6548\u80fd\u7684\u6838\u5fc3\u6307\u6807\uff1b2) \u6784\u5efa\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u6846\u67b6\uff0c\u6db5\u76d620\u591a\u4e2a\u672c\u5730\u6a21\u578b\u30018\u79cd\u52a0\u901f\u5668\u548c100\u4e07\u771f\u5b9e\u67e5\u8be2\uff1b3) \u53d1\u5e03IPW\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u7cfb\u7edf\u5316\u7684\u80fd\u6548\u57fa\u51c6\u6d4b\u8bd5\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) \u672c\u5730\u8bed\u8a00\u6a21\u578b\u53ef\u51c6\u786e\u56de\u7b5488.7%\u7684\u771f\u5b9e\u5355\u8f6e\u804a\u5929\u4e0e\u63a8\u7406\u67e5\u8be2\uff0c\u8868\u73b0\u56e0\u4efb\u52a1\u9886\u57df\u800c\u5f02\uff1b2) \u4ece2023\u52302025\u5e74\uff0cIPW\u63d0\u5347\u4e865.3\u500d\uff0c\u672c\u5730\u53ef\u5904\u7406\u7684\u67e5\u8be2\u8986\u76d6\u7387\u4ece23.2%\u4e0a\u5347\u81f371.3%\uff1b3) \u76f8\u540c\u6a21\u578b\u4e0b\uff0c\u672c\u5730\u52a0\u901f\u5668\u7684IPW\u81f3\u5c11\u6bd4\u4e91\u52a0\u901f\u5668\u4f4e1.4\u500d\uff0c\u663e\u793a\u672c\u5730\u8f6f\u786c\u4ef6\u534f\u540c\u4f18\u5316\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u672c\u5730\u63a8\u7406\u5df2\u5177\u5907\u663e\u8457\u5206\u6d41\u4e91\u7aefLLM\u8bf7\u6c42\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u80fd\u6548\u6301\u7eed\u6539\u8fdb\u7684\u8d8b\u52bf\u4e0b\u3002IPW\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53ef\u7528\u4e8e\u6307\u5bfc\u672a\u6765\u672c\u5730\u6a21\u578b\u4e0e\u786c\u4ef6\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002\u7814\u7a76\u8bc1\u5b9e\uff0c\u901a\u8fc7\u63d0\u5347\u672c\u5730\u8bbe\u5907\u7684\u667a\u80fd\u5904\u7406\u80fd\u529b\uff0c\u53ef\u5728\u4e0d\u727a\u7272\u7528\u6237\u4f53\u9a8c\u7684\u524d\u63d0\u4e0b\u51cf\u8f7b\u5bf9\u96c6\u4e2d\u5f0f\u57fa\u7840\u8bbe\u65bd\u7684\u4f9d\u8d56\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u6a21\u578b\u538b\u7f29\u4e0e\u91cf\u5316\u6280\u672f\u3001\u79fb\u52a8\u7aef\u63a8\u7406\u4f18\u5316\u3001\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u8bc4\u4f30\uff0c\u4ee5\u53ca\u80fd\u6548\u76f8\u5173\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7814\u7a76\u3002\u672c\u6587\u533a\u522b\u4e8e\u4ee5\u5f80\u5de5\u4f5c\u7684\u5173\u952e\u5728\u4e8e\u7ed3\u5408\u771f\u5b9e\u67e5\u8be2\u6d41\u91cf\uff0c\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u591a\u7ef4\u5ea6\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u5ef6\u8fdf\u3001\u80fd\u8017\u3001IPW\uff09\u3002"}}
{"id": "2511.08147", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08147", "abs": "https://arxiv.org/abs/2511.08147", "authors": ["Andrija Stanisic", "Stefan Nastic"], "title": "ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum", "comment": null, "summary": "Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProbSelect\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u3001\u4e91\u548c\u7a7a\u95f4\u8bbe\u5907\u6784\u6210\u7684\u4e09\u7ef4\u8fde\u7eed\u4f53\u4e2d\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5206\u6790\u5efa\u6a21\u548c\u6982\u7387\u9884\u6d4b\uff0c\u65e0\u9700\u5386\u53f2\u6570\u636e\u6216\u6301\u7eed\u76d1\u63a7\uff0c\u9002\u7528\u4e8eGPU\u52a0\u901f\u8bad\u7ec3\u73af\u5883\uff0c\u5e76\u5728\u6ee1\u8db3SLO\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u3002", "motivation": "\u5728\u5305\u542b\u536b\u661f\u3001\u79fb\u52a8\u8bbe\u5907\u548c\u4e91\u7aef\u7684\u52a8\u6001\u4e09\u7ef4\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\uff0c\u4f20\u7edf\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u548c\u6301\u7eed\u76d1\u63a7\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8eCPU\u8ba1\u7b97\u6a21\u578b\uff0c\u65e0\u6cd5\u53cd\u6620GPU\u52a0\u901f\u8bad\u7ec3\u7684\u590d\u6742\u7279\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u3001\u9002\u5e94\u52a8\u6001\u73af\u5883\u5e76\u652f\u6301GPU\u67b6\u6784\u7684\u65b0\u578b\u5ba2\u6237\u7aef\u9009\u62e9\u673a\u5236\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a1\uff09\u5728\u8bbe\u5907\u72b6\u6001\u9891\u7e41\u53d8\u5316\u7684\u52a8\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u53ef\u9760\u7684\u8fd0\u884c\u65f6\u4fe1\u606f\uff1b2\uff09\u7f3a\u4e4f\u5bf9GPU\u52a0\u901f\u8bad\u7ec3\u7279\u6027\u7684\u5efa\u6a21\u80fd\u529b\uff1b3\uff09\u9700\u5728\u6ee1\u8db3\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u7684\u524d\u63d0\u4e0b\u9ad8\u6548\u9009\u62e9\u5ba2\u6237\u7aef\uff1b4\uff09\u907f\u514d\u56e0\u9891\u7e41\u76d1\u63a7\u5e26\u6765\u7684\u989d\u5916\u5f00\u9500\u3002", "contributions": "1\uff09\u63d0\u51faProbSelect\uff0c\u4e00\u79cd\u65e0\u9700\u5386\u53f2\u6570\u636e\u548c\u6301\u7eed\u76d1\u63a7\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff1b2\uff09\u6784\u5efa\u9002\u7528\u4e8eGPU\u52a0\u901f\u73af\u5883\u7684\u5206\u6790\u6a21\u578b\u4e0e\u6982\u7387\u9884\u6d4b\u673a\u5236\uff1b3\uff09\u5c06\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49SLO\u7684\u4f18\u5316\u95ee\u9898\uff1b4\uff09\u5728\u591a\u79cdGPU\u67b6\u6784\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cProbSelect\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e8613.77%\u7684SLO\u5408\u89c4\u7387\uff0c\u5e76\u5b9e\u73b0\u4e8672.5%\u7684\u8ba1\u7b97\u6d6a\u8d39\u51cf\u5c11\uff0c\u8868\u73b0\u51fa\u5728\u5f02\u6784\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ProbSelect\u901a\u8fc7\u7ed3\u5408\u5206\u6790\u5efa\u6a21\u4e0e\u6982\u7387\u9884\u6d4b\uff0c\u5728\u4e0d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u4e09\u7ef4\u8fde\u7eed\u4f53\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\uff0c\u5c24\u5176\u9002\u7528\u4e8eGPU\u52a0\u901f\u3001\u9ad8\u52a8\u6001\u6027\u7684\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\u4e0eSLO\u6ee1\u8db3\u5ea6\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u5305\u62ec\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u3001\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u7684\u8d44\u6e90\u9884\u6d4b\u65b9\u6cd5\u4ee5\u53ca\u9762\u5411\u8fb9\u7f18\u8ba1\u7b97\u7684SLO\u7ba1\u7406\u673a\u5236\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6301\u7eed\u76d1\u63a7\u548cCPU\u8ba1\u7b97\u5047\u8bbe\uff0c\u96be\u4ee5\u9002\u5e94\u672c\u6587\u6240\u8003\u8651\u7684\u4e09\u7ef4\u52a8\u6001GPU\u73af\u5883\u3002"}}
{"id": "2511.08222", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08222", "abs": "https://arxiv.org/abs/2511.08222", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin", "comment": "25 pages, 9 fugures, 2 tables", "summary": "In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.\n  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728OBLOT\u6a21\u578b\u4e0b\uff0c\u673a\u5668\u4eba\u5728\u9876\u70b9\u548c\u8fb9\u5bf9\u79f0\u56fe\u4e0a\u8fdb\u884c\u805a\u96c6\u7684\u95ee\u9898\uff0c\u8003\u8651\u4e86\u521d\u59cb\u914d\u7f6e\u5b58\u5728\u591a\u91cd\u6027\u3001\u673a\u5668\u4eba\u65e0\u6cd5\u68c0\u6d4b\u591a\u91cd\u6027\u7b49\u6311\u6218\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u4e24\u79cd\u62d3\u6251\u7ed3\u6784\u7684\u65f6\u95f4\u6700\u4f18\u805a\u96c6\u7b97\u6cd5\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u805a\u96c6\u95ee\u9898\u662f\u6838\u5fc3\u6311\u6218\u4e4b\u4e00\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u9650\u5236\u6027\u8f83\u5f3a\uff08\u5982\u65e0\u6cd5\u68c0\u6d4b\u591a\u91cd\u6027\u3001\u5bf9\u79f0\u56fe\u7ed3\u6784\uff09\u7684\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u5b9e\u73b0\u673a\u5668\u4eba\u805a\u96c6\u7684\u95ee\u9898\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u521d\u59cb\u914d\u7f6e\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4f4d\u4e8e\u540c\u4e00\u9876\u70b9\uff08\u591a\u91cd\u6027\uff09\uff0c\u673a\u5668\u4eba\u65e0\u6cd5\u611f\u77e5\u8fd9\u79cd\u591a\u91cd\u6027\uff0c\u4e14\u53ea\u80fd\u5728\u9876\u70b9\u548c\u8fb9\u5bf9\u79f0\u7684\u56fe\u4e0a\u79fb\u52a8\uff1b\u540c\u65f6\u9700\u5728\u8f6e\u8be2\u8c03\u5ea6\u673a\u5236\u4e0b\u8bbe\u8ba1\u6709\u6548\u7b97\u6cd5\u3002", "contributions": "\u63d0\u51fa\u4e86\u5728\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u56fe\u4e0a\u7684\u4e24\u79cd\u65f6\u95f4\u6700\u4f18\u805a\u96c6\u7b97\u6cd5\uff1b\u7ed9\u51fa\u4e86\u82e5\u5e72\u57fa\u672c\u7684\u4e0d\u53ef\u89e3\u6027\u7ed3\u679c\uff1b\u5e76\u63a8\u6d4b\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u51b5\u7684\u901a\u7528\u7b97\u6cd5\u3002", "results": "\u8bbe\u8ba1\u7684\u7b97\u6cd5\u5728\u7279\u5b9a\u62d3\u6251\uff08\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\uff09\u4e0a\u5b9e\u73b0\u4e86\u65f6\u95f4\u6700\u4f18\u7684\u805a\u96c6\uff0c\u5145\u5206\u5229\u7528\u4e86\u56fe\u7684\u5bf9\u79f0\u6027\u7ed3\u6784\uff1b\u540c\u65f6\u8bc1\u660e\u4e86\u4e00\u4e9b\u60c5\u51b5\u4e0b\u805a\u96c6\u95ee\u9898\u65e0\u6cd5\u89e3\u51b3\u3002", "conclusion": "\u7531\u4e8e\u95ee\u9898\u73af\u5883\u7684\u9650\u5236\u6027\u548c\u62d3\u6251\u4f9d\u8d56\u6027\uff0c\u805a\u96c6\u95ee\u9898\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u53ef\u80fd\u4e0d\u5b58\u5728\u901a\u7528\u89e3\uff0c\u9700\u9488\u5bf9\u7279\u5b9a\u56fe\u7ed3\u6784\u8bbe\u8ba1\u4e13\u7528\u7b97\u6cd5\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728OBLOT\u6a21\u578b\u4e0b\u7684\u5206\u5e03\u5f0f\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u79bb\u6563\u56fe\u4e0a\u7684\u7814\u7a76\uff0c\u4ee5\u53ca\u4e0d\u540c\u611f\u77e5\u80fd\u529b\u548c\u79fb\u52a8\u80fd\u529b\u4e0b\u7684\u53ef\u89e3\u6027\u5206\u6790\u3002"}}
{"id": "2511.08373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08373", "abs": "https://arxiv.org/abs/2511.08373", "authors": ["Henrik Daniel Christensen", "Saverio Giallorenzo", "Jacopo Mauro"], "title": "Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing", "comment": null, "summary": "Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.\n  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\\% of scenarios. With a 10-second window, our approach improves placements in over 73\\% and still certifies that the default scheduler's placement is already optimal in over 19\\% of scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u89c4\u5212\u7684Kubernetes\u8c03\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u63d2\u4ef6\uff0c\u5728\u9ed8\u8ba4\u8c03\u5ea6\u5931\u8d25\u65f6\u8fdb\u884c\u56de\u9000\u5904\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u90e8\u5206\u573a\u666f\u4e0b\u80fd\u66f4\u4f18\u5730\u5206\u914d\u9ad8\u4f18\u5148\u7ea7Pod\uff0c\u5e76\u53ef\u9a8c\u8bc1\u9ed8\u8ba4\u8c03\u5ea6\u7684\u6700\u4f18\u6027\u3002", "motivation": "Kubernetes\u9ed8\u8ba4\u8c03\u5ea6\u5668\u4f7f\u7528\u542f\u53d1\u5f0f\u7b97\u6cd5\u53ef\u80fd\u5bfc\u81f4Pod\u5206\u914d\u6b21\u4f18\u548c\u8d44\u6e90\u788e\u7247\u5316\uff0c\u5f71\u54cd\u96c6\u7fa4\u8d44\u6e90\u5229\u7528\u7387\u548c\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u90e8\u7f72\u3002", "challenges": "\u5728\u4fdd\u8bc1\u8c03\u5ea6\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u8d44\u6e90\u5206\u914d\u7684\u6700\u4f18\u5316\uff1b\u5904\u7406\u4e0d\u540c\u4f18\u5148\u7ea7\u548c\u8d44\u6e90\u9700\u6c42\u7684Pod\u7684\u590d\u6742\u7ea6\u675f\uff1b\u5728\u6709\u9650\u65f6\u95f4\u5185\u6c42\u89e3\u5927\u89c4\u6a21\u8c03\u5ea6\u95ee\u9898\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u7f16\u7a0b\u7684Kubernetes Pod\u8c03\u5ea6\u4f18\u5316\u65b9\u6848\uff1b\u5b9e\u73b0\u4e86\u4e0e\u9ed8\u8ba4\u8c03\u5ea6\u5668\u96c6\u6210\u7684\u63d2\u4ef6\u5f0f\u67b6\u6784\uff1b\u901a\u8fc7OR-Tools\u6c42\u89e3\u5668\u57281\u79d2\u548c10\u79d2\u65f6\u95f4\u7a97\u53e3\u5185\u9a8c\u8bc1\u4e86\u8c03\u5ea6\u6027\u80fd\u63d0\u5347\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5c0f\u5230\u4e2d\u7b49\u89c4\u6a21\u96c6\u7fa4\u4e2d\uff0c1\u79d2\u8c03\u5ea6\u7a97\u53e3\u5185\u8be5\u65b9\u6cd5\u572844%\u4ee5\u4e0a\u53ef\u5b9e\u73b0\u7684\u5206\u914d\u573a\u666f\u4e2d\u4f18\u4e8e\u9ed8\u8ba4\u8c03\u5ea6\u5668\uff0c\u5e76\u572819%\u4ee5\u4e0a\u573a\u666f\u4e2d\u9a8c\u8bc1\u9ed8\u8ba4\u8c03\u5ea6\u5df2\u662f\u6700\u4f18\uff1b10\u79d2\u7a97\u53e3\u5185\u4f18\u5316\u6bd4\u4f8b\u63d0\u5347\u81f373%\u4ee5\u4e0a\u3002", "conclusion": "\u57fa\u4e8e\u7ea6\u675f\u7f16\u7a0b\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347Kubernetes\u8c03\u5ea6\u8d28\u91cf\uff0c\u5728\u5408\u7406\u65f6\u95f4\u5185\u627e\u5230\u66f4\u4f18\u6216\u9a8c\u8bc1\u73b0\u6709\u8c03\u5ea6\u7684\u6700\u4f18\u6027\uff0c\u9002\u5408\u4f5c\u4e3a\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u8865\u5145\u673a\u5236\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecKubernetes\u9ed8\u8ba4\u8c03\u5ea6\u5668\u673a\u5236\u3001\u8d44\u6e90\u5206\u914d\u542f\u53d1\u5f0f\u7b97\u6cd5\u3001\u7ea6\u675f\u7f16\u7a0b\u5728\u8c03\u5ea6\u4e2d\u7684\u5e94\u7528\u4ee5\u53ca\u96c6\u7fa4\u8d44\u6e90\u4f18\u5316\u6280\u672f\u3002"}}
