{"id": "2511.05843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05843", "abs": "https://arxiv.org/abs/2511.05843", "authors": ["Hanzheng Lyu", "Shaokang Xie", "Jianyu Niu", "Mohammad Sadoghi", "Yinqian Zhang", "Cong Wang", "Ivan Beschastnikh", "Chen Feng"], "title": "HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus", "comment": null, "summary": "Multi-Byzantine Fault Tolerant (Multi-BFT) consensus, which runs multiple BFT instances in parallel, has recently emerged as a promising approach to overcome the leader bottleneck in classical BFT protocols. However, existing designs rely on a global ordering layer to serialize blocks across instances, an intuitive yet costly mechanism that constrains scalability, amplifies failure propagation, and complicates deployment. In this paper, we challenge this conventional wisdom. We present HYDRA, the first Multi-BFT consensus framework that eliminates global ordering altogether. HYDRA introduces an object-centric execution model that partitions transactions by their accessed objects, enabling concurrent yet deterministic execution across instances. To ensure consistency, HYDRA combines lightweight lock-based coordination with a deadlock resolution mechanism, achieving both scalability and correctness. We implement HYDRA and evaluate it on up to 128 replicas in both LAN and WAN environments. Experimental results show HYDRA outperforms several state-of-the-art Multi-BFT protocols in the presence of a straggler. These results demonstrate strong consistency and high performance by removing global ordering, opening a new direction toward scalable Multi-BFT consensus design.", "AI": {"tldr": "HYDRA\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u62dc\u5360\u5ead\u5bb9\u9519\uff08Multi-BFT\uff09\u5171\u8bc6\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664\u5168\u5c40\u6392\u5e8f\u5c42\uff0c\u91c7\u7528\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6267\u884c\u6a21\u578b\uff0c\u5b9e\u73b0\u8de8\u5b9e\u4f8b\u7684\u5e76\u53d1\u786e\u5b9a\u6027\u6267\u884c\uff0c\u4ece\u800c\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Multi-BFT\u534f\u8bae\u4f9d\u8d56\u5168\u5c40\u6392\u5e8f\u5c42\u6765\u5e8f\u5217\u5316\u591a\u4e2a\u5b9e\u4f8b\u95f4\u7684\u533a\u5757\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3001\u6545\u969c\u4f20\u64ad\u52a0\u5267\u548c\u90e8\u7f72\u590d\u6742\u3002HYDRA\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u4f20\u7edf\u8bbe\u8ba1\uff0c\u63a2\u7d22\u65e0\u9700\u5168\u5c40\u6392\u5e8f\u7684\u9ad8\u6548\u5171\u8bc6\u673a\u5236\u3002", "challenges": "\u5982\u4f55\u5728\u53bb\u9664\u5168\u5c40\u6392\u5e8f\u7684\u540c\u65f6\uff0c\u4fdd\u8bc1\u8de8\u591a\u4e2aBFT\u5b9e\u4f8b\u7684\u4e8b\u52a1\u6267\u884c\u4e00\u81f4\u6027\u4e0e\u6b63\u786e\u6027\uff1b\u5982\u4f55\u5728\u9ad8\u5e76\u53d1\u4e0b\u907f\u514d\u6b7b\u9501\u5e76\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\u3002", "contributions": "1. \u63d0\u51faHYDRA\uff0c\u9996\u4e2a\u65e0\u9700\u5168\u5c40\u6392\u5e8f\u7684Multi-BFT\u5171\u8bc6\u6846\u67b6\uff1b2. \u8bbe\u8ba1\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6267\u884c\u6a21\u578b\uff0c\u6309\u8bbf\u95ee\u5bf9\u8c61\u5212\u5206\u4e8b\u52a1\uff1b3. \u5f15\u5165\u8f7b\u91cf\u7ea7\u57fa\u4e8e\u9501\u7684\u534f\u8c03\u4e0e\u6b7b\u9501\u89e3\u51b3\u673a\u5236\uff0c\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u5e76\u53d1\u6027\u3002", "results": "\u5728\u6700\u591a128\u4e2a\u526f\u672c\u7684\u5c40\u57df\u7f51\u548c\u5e7f\u57df\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u5e76\u8bc4\u4f30HYDRA\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u5b58\u5728\u5ef6\u8fdf\u8282\u70b9\uff08straggler\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684Multi-BFT\u534f\u8bae\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "HYDRA\u901a\u8fc7\u53bb\u9664\u5168\u5c40\u6392\u5e8f\u5c42\uff0c\u663e\u8457\u63d0\u5347\u4e86Multi-BFT\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u5f3a\u4e00\u81f4\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "related_work": "\u4e0e\u4f9d\u8d56\u5168\u5c40\u6392\u5e8f\u7684Multi-BFT\u7cfb\u7edf\uff08\u5982OmniLedger\u3001RapidChain\uff09\u4e0d\u540c\uff0cHYDRA\u5b8c\u5168\u6d88\u9664\u5168\u5c40\u534f\u8c03\uff0c\u4e0e\u57fa\u4e8e\u5206\u7247\u7684\u5171\u8bc6\u548c\u5e76\u53d1\u63a7\u5236\u673a\u5236\uff08\u5982Calvin\u3001Taurus\uff09\u6709\u90e8\u5206\u76f8\u4f3c\uff0c\u4f46\u66f4\u4e13\u6ce8\u4e8e\u591a\u5b9e\u4f8bBFT\u73af\u5883\u4e0b\u7684\u5bf9\u8c61\u7ea7\u5e76\u53d1\u4e0e\u4e00\u81f4\u6027\u4fdd\u969c\u3002"}}
{"id": "2511.05915", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05915", "abs": "https://arxiv.org/abs/2511.05915", "authors": ["Guihang Hong", "Tao Ouyang", "Kongyange Zhao", "Zhi Zhou", "Xu Chen"], "title": "CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing", "comment": "Accepted by RTSS 2025 (Real-Time Systems Symposium, 2025)", "summary": "Motivated by the imperative for real-time responsiveness and data privacy preservation, large language models (LLMs) are increasingly deployed on resource-constrained edge devices to enable localized inference. To improve output quality, retrieval-augmented generation (RAG) is an efficient technique that seamlessly integrates local data into LLMs. However, existing edge computing paradigms primarily focus on single-node optimization, neglecting opportunities to holistically exploit distributed data and heterogeneous resources through cross-node collaboration. To bridge this gap, we propose CoEdge-RAG, a hierarchical scheduling framework for retrieval-augmented LLMs in collaborative edge computing. In general, privacy constraints preclude accurate a priori acquisition of heterogeneous data distributions across edge nodes, directly impeding RAG performance optimization. Thus, we first design an online query identification mechanism using proximal policy optimization (PPO), which autonomously infers query semantics and establishes cross-domain knowledge associations in an online manner. Second, we devise a dynamic inter-node scheduling strategy that balances workloads across heterogeneous edge nodes by synergizing historical performance analytics with real-time resource thresholds. Third, we develop an intra-node scheduler based on online convex optimization, adaptively allocating query processing ratios and memory resources to optimize the latency-quality trade-off under fluctuating assigned loads. Comprehensive evaluations across diverse QA benchmarks demonstrate that our proposed method significantly boosts the performance of collaborative retrieval-augmented LLMs, achieving performance gains of 4.23\\% to 91.39\\% over baseline methods across all tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoEdge-RAG\u7684\u5206\u5c42\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u534f\u4f5c\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5728\u7ebf\u67e5\u8be2\u8bc6\u522b\u3001\u52a8\u6001\u8de8\u8282\u70b9\u8c03\u5ea6\u548c\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\uff0c\u5728\u9690\u79c1\u53d7\u9650\u548c\u8d44\u6e90\u5f02\u6784\u7684\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u5bf9\u5b9e\u65f6\u54cd\u5e94\u548c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u88ab\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u672c\u5730\u63a8\u7406\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u53ef\u6709\u6548\u6574\u5408\u672c\u5730\u6570\u636e\u4ee5\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u8fb9\u7f18\u8ba1\u7b97\u8303\u5f0f\u591a\u5c40\u9650\u4e8e\u5355\u8282\u70b9\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u8de8\u8282\u70b9\u534f\u4f5c\u5bf9\u5206\u5e03\u5f0f\u6570\u636e\u4e0e\u5f02\u6784\u8d44\u6e90\u7684\u7efc\u5408\u5229\u7528\u6f5c\u529b\u3002", "challenges": "1. \u9690\u79c1\u9650\u5236\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u9884\u77e5\u8fb9\u7f18\u8282\u70b9\u95f4\u5f02\u6784\u7684\u6570\u636e\u5206\u5e03\uff0c\u5f71\u54cdRAG\u6027\u80fd\u4f18\u5316\uff1b2. \u8fb9\u7f18\u8282\u70b9\u8d44\u6e90\u5f02\u6784\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u8282\u70b9\u4efb\u52a1\u8c03\u5ea6\uff1b3. \u5c40\u90e8\u8d1f\u8f7d\u6ce2\u52a8\u5927\uff0c\u9700\u5728\u5ef6\u8fdf\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u52a8\u6001\u6743\u8861\u3002", "contributions": "1. \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u5728\u7ebf\u67e5\u8be2\u8bc6\u522b\u673a\u5236\uff0c\u53ef\u81ea\u4e3b\u63a8\u65ad\u67e5\u8be2\u8bed\u4e49\u5e76\u5efa\u7acb\u8de8\u57df\u77e5\u8bc6\u5173\u8054\uff1b2. \u8bbe\u8ba1\u4e00\u79cd\u7ed3\u5408\u5386\u53f2\u6027\u80fd\u5206\u6790\u4e0e\u5b9e\u65f6\u8d44\u6e90\u9608\u503c\u7684\u52a8\u6001\u8de8\u8282\u70b9\u8c03\u5ea6\u7b56\u7565\uff0c\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff1b3. \u6784\u5efa\u57fa\u4e8e\u5728\u7ebf\u51f8\u4f18\u5316\u7684\u8282\u70b9\u5185\u8c03\u5ea6\u5668\uff0c\u81ea\u9002\u5e94\u5206\u914d\u67e5\u8be2\u5904\u7406\u6bd4\u4f8b\u4e0e\u5185\u5b58\u8d44\u6e90\uff0c\u4f18\u5316\u5ef6\u8fdf-\u8d28\u91cf\u6743\u8861\u3002", "results": "\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoEdge-RAG\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u4e864.23%\u81f391.39%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u589e\u5f3a\u578bLLM\u5728\u534f\u4f5c\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u63a8\u7406\u6548\u7387\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "CoEdge-RAG\u901a\u8fc7\u5c42\u6b21\u5316\u8c03\u5ea6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u73af\u5883\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u9762\u4e34\u7684\u9690\u79c1\u3001\u8d44\u6e90\u5f02\u6784\u4e0e\u52a8\u6001\u8d1f\u8f7d\u7b49\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8de8\u8282\u70b9\u534f\u4f5c\u4e0b\u7684\u9ad8\u6027\u80fd\u672c\u5730\u5316\u63a8\u7406\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5355\u8282\u70b9LLM\u63a8\u7406\u4f18\u5316\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u5728\u4e91\u7aef\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u8c03\u5ea6\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u8003\u8651\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u8de8\u8fb9\u7f18\u8282\u70b9\u534f\u540c\u5229\u7528\u5206\u5e03\u5f0f\u6570\u636e\u4e0e\u5f02\u6784\u8d44\u6e90\u4ee5\u652f\u6301RAG\u7684\u5b8c\u6574\u6846\u67b6\u3002"}}
{"id": "2511.05958", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05958", "abs": "https://arxiv.org/abs/2511.05958", "authors": ["Stepan Vanecek", "Manuel Walter Mussbacher", "Dominik Groessler", "Urvij Saroliya", "Martin Schulz"], "title": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies", "comment": "14 pages, including Appendix and References, 5 figures, to be published in Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC Workshops '25)", "summary": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.\n  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.\n  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems.", "AI": {"tldr": "MT4G\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u4e0e\u5382\u5546\u65e0\u5173\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0GPU\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u62d3\u6251\u7ed3\u6784\u53ca\u914d\u7f6e\uff0c\u901a\u8fc7\u7ed3\u5408API\u548c50\u591a\u4e2a\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e94\u7528\u7edf\u8ba1\u65b9\u6cd5\u53ef\u9760\u8bc6\u522b\u62d3\u6251\u5c5e\u6027\uff0c\u5e76\u5728\u591a\u79cdGPU\u4e0a\u9a8c\u8bc1\u5176\u901a\u7528\u6027\uff0c\u5e94\u7528\u4e8e\u6027\u80fd\u5efa\u6a21\u3001\u74f6\u9888\u5206\u6790\u548c\u8d44\u6e90\u52a8\u6001\u5206\u533a\u3002", "motivation": "GPU\u62d3\u6251\u4fe1\u606f\u5bf9\u4e8eHPC\u548cAI\u6027\u80fd\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u50cfCPU\u9886\u57dfhwloc\u8fd9\u6837\u7684\u901a\u7528\u5de5\u5177\uff0c\u4e14\u73b0\u6709\u4fe1\u606f\u4e0d\u5b8c\u6574\u3001\u4f9d\u8d56\u5382\u5546\u3002", "challenges": "GPU\u62d3\u6251\u4fe1\u606f\u96be\u4ee5\u83b7\u53d6\u3001\u4e0d\u5b8c\u6574\u4e14\u5382\u5546\u7279\u5b9a\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u3001\u8de8\u5382\u5546\u7684\u62d3\u6251\u53d1\u73b0\u5de5\u5177\u3002", "contributions": "\u63d0\u51fa\u4e86MT4G\uff0c\u4e00\u4e2a\u5f00\u6e90\u3001\u8de8\u5382\u5546\u7684GPU\u62d3\u6251\u81ea\u52a8\u53d1\u73b0\u5de5\u5177\uff0c\u7ed3\u5408API\u4e0e50\u591a\u4e2a\u5fae\u57fa\u51c6\u6d4b\u8bd5\u53ca\u7edf\u8ba1\u65b9\u6cd5\uff08\u5982Kolmogorov-Smirnov\u68c0\u9a8c\uff09\uff0c\u5b9e\u73b0\u5bf9\u7f13\u5b58\u5927\u5c0f\u3001\u5e26\u5bbd\u548c\u7269\u7406\u5e03\u5c40\u7b49\u5c5e\u6027\u7684\u81ea\u52a8\u8bc6\u522b\u3002", "results": "\u5728\u5341\u79cd\u4e0d\u540cGPU\u4e0a\u9a8c\u8bc1\u4e86MT4G\u7684\u901a\u7528\u6027\uff0c\u5e76\u6210\u529f\u96c6\u6210\u5230GPU\u6027\u80fd\u5efa\u6a21\u3001GPUscout\u74f6\u9888\u5206\u6790\u548c\u52a8\u6001\u8d44\u6e90\u5206\u533a\u4e09\u4e2a\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u5728NVIDIA\u548cAMD GPU\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "MT4G\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u8de8\u5382\u5546\u7406\u89e3\u73b0\u4ee3HPC\u548cAI\u7cfb\u7edf\u4e2d\u7684GPU\u6027\u80fd\u7279\u5f81\uff0c\u586b\u8865\u4e86\u5f53\u524d\u5de5\u5177\u94fe\u7684\u7a7a\u767d\u3002", "related_work": "CPU\u9886\u57df\u6709hwloc\u7b49\u6210\u719f\u5de5\u5177\u7528\u4e8e\u62d3\u6251\u53d1\u73b0\uff0c\u4f46\u5728GPU\u9886\u57df\u5c1a\u7f3a\u4e4f\u7c7b\u4f3c\u901a\u7528\u3001\u81ea\u52a8\u5316\u4e14\u4e0d\u4f9d\u8d56\u5382\u5546\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.05502", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05502", "abs": "https://arxiv.org/abs/2511.05502", "authors": ["Varun Rajesh", "Om Jodhpurkar", "Pooja Anbuselvan", "Mantinder Singh", "Ashok Jallepali", "Shantanu Godbole", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS", "comment": null, "summary": "We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.\n  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.\n  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.", "AI": {"tldr": "\u5bf9\u4e94\u79cd\u5728Apple Silicon\u4e0a\u8fd0\u884c\u7684\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6846\u67b6\uff08MLX\u3001MLC-LLM\u3001llama.cpp\u3001Ollama\u3001PyTorch MPS\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u4f7f\u7528Qwen-2.5\u6a21\u578b\u65cf\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u4e0d\u540c\u8d1f\u8f7d\u4e0b\u6d4b\u8bd5\u6027\u80fd\u6307\u6807\u3002\u7ed3\u679c\u663e\u793a\u5404\u6846\u67b6\u5728\u541e\u5410\u91cf\u3001\u9996 token \u65f6\u95f4\u3001\u90e8\u7f72\u4fbf\u6377\u6027\u7b49\u65b9\u9762\u5404\u6709\u4f18\u52a3\uff0c\u5747\u652f\u6301\u5b8c\u5168\u79bb\u7ebf\u8fd0\u884c\uff0c\u5177\u5907\u5f3a\u9690\u79c1\u4fdd\u969c\uff0c\u867d\u6027\u80fd\u4ecd\u843d\u540e\u4e8eNVIDIA GPU\u65b9\u6848\uff0c\u4f46\u5df2\u9010\u6b65\u6210\u4e3a\u53ef\u884c\u7684\u751f\u4ea7\u7ea7\u672c\u5730\u63a8\u7406\u9009\u62e9\u3002", "motivation": "\u968f\u7740\u672c\u5730\u5927\u6a21\u578b\u90e8\u7f72\u9700\u6c42\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u6ce8\u91cd\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u7684\u573a\u666f\u4e0b\uff0cApple Silicon\u8bbe\u5907\u56e0\u5176\u5f3a\u5927\u7684\u7edf\u4e00\u5185\u5b58\u67b6\u6784\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u6210\u4e3a\u7406\u60f3\u5e73\u53f0\u3002\u7136\u800c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e0d\u540cLLM\u8fd0\u884c\u65f6\u5728\u82f9\u679c\u751f\u6001\u4e2d\u6027\u80fd\u8868\u73b0\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u505a\u51fa\u6280\u672f\u9009\u578b\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u8bc1\u7684\u6027\u80fd\u5206\u6790\u4e0e\u63a8\u8350\u6307\u5357\u3002", "challenges": "1) \u4e0d\u540cLLM\u8fd0\u884c\u65f6\u5728Apple Silicon\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u5dee\u5f02\u5927\uff0c\u4f18\u5316\u7b56\u7565\u4e0d\u900f\u660e\uff1b2) \u957f\u4e0a\u4e0b\u6587\uff08\u957f\u8fbe10\u4e07token\uff09\u5904\u7406\u5bf9\u5185\u5b58\u7ba1\u7406\u548cKV\u7f13\u5b58\u6548\u7387\u63d0\u51fa\u6311\u6218\uff1b3) \u91cf\u5316\u652f\u6301\u3001\u6d41\u5f0f\u8f93\u51fa\u3001\u6279\u5904\u7406\u4e0e\u5e76\u53d1\u80fd\u529b\u5728\u5404\u6846\u67b6\u95f4\u652f\u6301\u4e0d\u4e00\uff1b4) \u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\uff08\u5982TTFT\uff09\u5b58\u5728\u6743\u8861\uff1b5) PyTorch\u7b49\u4f20\u7edf\u6846\u67b6\u5728MPS\u540e\u7aef\u5b58\u5728\u5185\u5b58\u74f6\u9888\u3002", "contributions": "1) \u5bf9\u4e94\u79cd\u4e3b\u6d41\u672c\u5730LLM\u8fd0\u884c\u65f6\uff08MLX\u3001MLC-LLM\u3001llama.cpp\u3001Ollama\u3001PyTorch MPS\uff09\u5728\u771f\u5b9e\u786c\u4ef6\uff08M2 Ultra, 192GB\uff09\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u3001\u53ef\u590d\u73b0\u7684\u6027\u80fd\u8bc4\u6d4b\uff1b2) \u8986\u76d6\u4e86\u5305\u62ecTTFT\u3001\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u5206\u5e03\u3001\u957f\u4e0a\u4e0b\u6587\u884c\u4e3a\u3001\u91cf\u5316\u3001\u6d41\u5f0f\u3001\u6279\u5904\u7406\u7b49\u591a\u7ef4\u5ea6\u6307\u6807\uff1b3) \u516c\u5f00\u4e86\u6240\u6709\u5b9e\u9a8c\u811a\u672c\u3001\u65e5\u5fd7\u548c\u56fe\u8868\uff0c\u63d0\u5347\u7814\u7a76\u900f\u660e\u5ea6\u4e0e\u53ef\u590d\u73b0\u6027\uff1b4) \u63d0\u4f9b\u4e86\u9488\u5bf9\u4ea4\u4e92\u5f0f\u5e94\u7528\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u5177\u4f53\u90e8\u7f72\u5efa\u8bae\uff1b5) \u63a8\u52a8\u4e86Apple Silicon\u4f5c\u4e3a\u79c1\u6709\u5316LLM\u63a8\u7406\u5e73\u53f0\u7684\u8ba4\u77e5\u4e0e\u53d1\u5c55\u3002", "results": "1) MLX\u5728\u6301\u7eed\u751f\u6210\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b2) MLC-LLM\u5728\u4e2d\u7b49\u957f\u5ea6\u63d0\u793a\u4e0b\u9996token\u65f6\u95f4\uff08TTFT\uff09\u6700\u4f4e\uff0c\u5e76\u5177\u5907\u8f83\u5f3a\u7684\u5f00\u7bb1\u5373\u7528\u529f\u80fd\uff1b3) llama.cpp\u5728\u5355\u6d41\u8f7b\u91cf\u7ea7\u573a\u666f\u4e2d\u6548\u7387\u9ad8\u3001\u8d44\u6e90\u5360\u7528\u5c11\uff1b4) Ollama\u5f00\u53d1\u4f53\u9a8c\u53cb\u597d\u4f46\u6027\u80fd\uff08\u541e\u5410\u4e0eTTFT\uff09\u76f8\u5bf9\u843d\u540e\uff1b5) PyTorch MPS\u53d7\u9650\u4e8e\u5927\u6a21\u578b\u548c\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u5185\u5b58\u74f6\u9888\uff1b6) \u6240\u6709\u6846\u67b6\u5747\u652f\u6301\u5168\u8bbe\u5907\u5185\u8fd0\u884c\uff0c\u65e0\u6570\u636e\u5916\u6cc4\u98ce\u9669\uff1b7) \u5c3d\u7ba1\u6574\u4f53\u6027\u80fd\u4ecd\u4f4e\u4e8eNVIDIA\u5e73\u53f0\u4e0a\u7684vLLM\u7b49\u7cfb\u7edf\uff0c\u4f46Apple Silicon\u65b9\u6848\u5df2\u63a5\u8fd1\u5b9e\u7528\u5316\u6c34\u5e73\u3002", "conclusion": "\u5728Apple Silicon\u5e73\u53f0\u4e0a\uff0c\u4e0d\u540cLLM\u8fd0\u884c\u65f6\u5404\u6709\u4f18\u52bf\uff1aMLX\u9002\u5408\u9ad8\u541e\u5410\u751f\u6210\u4efb\u52a1\uff0cMLC-LLM\u9002\u5408\u4f4e\u5ef6\u8fdf\u4ea4\u4e92\u573a\u666f\uff0cllama.cpp\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8f7b\u91cf\u5e94\u7528\uff0cOllama\u9002\u5408\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u800cPyTorch MPS\u5c1a\u9700\u6539\u8fdb\u5185\u5b58\u7ba1\u7406\u3002\u5c3d\u7ba1\u5f53\u524d\u6027\u80fd\u5c1a\u672a\u8d85\u8d8a\u9ad8\u7aefGPU\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u6846\u67b6\u5df2\u5c55\u73b0\u51fa\u4f5c\u4e3a\u79c1\u6709\u3001\u5b89\u5168\u3001\u9ad8\u6548\u672c\u5730\u63a8\u7406\u5e73\u53f0\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6b63\u5feb\u901f\u8d70\u5411\u6210\u719f\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u5305\u62ecvLLM\u3001TensorRT-LLM\u7b49GPU\u52a0\u901f\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u53caHugging Face TGI\u3001Llama.cpp\u7b49\u5f00\u6e90\u63a8\u7406\u7cfb\u7edf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u7814\u7a76\u805a\u7126\u4e8eApple Silicon\u5e73\u53f0\u7279\u6709\u7684\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u5f3a\u8c03\u5b8c\u5168\u79bb\u7ebf\u6267\u884c\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5bf9\u6bd4\u7684\u662f\u4e13\u4e3a\u82f9\u679c\u82af\u7247\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u800c\u975e\u4f9d\u8d56CUDA\u7684\u9ad8\u6027\u80fd\u670d\u52a1\u7aef\u65b9\u6848\u3002\u6b64\u5916\uff0cMLC-LLM\u548cMLX\u672c\u8eab\u4e5f\u6e90\u81ea\u5b66\u672f\u9879\u76ee\uff0c\u4f53\u73b0\u4e86\u7f16\u8bd1\u4f18\u5316\u4e0e\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2511.06052", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.06052", "abs": "https://arxiv.org/abs/2511.06052", "authors": ["Philipp Schaad", "Tal Ben-Nun", "Patrick Iff", "Torsten Hoefler"], "title": "Inductive Loop Analysis for Practical HPC Application Optimization", "comment": null, "summary": "Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\\times$ speedup over the state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7b26\u53f7\u5f52\u7eb3\u5faa\u73af\u4f18\u5316\uff08SILO\uff09\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u8bbf\u95ee\u548c\u4f9d\u8d56\u5efa\u6a21\u4e3a\u5faa\u73af\u6b65\u5e45\u7684\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf9\u987a\u5e8f\u4f9d\u8d56\u5faa\u73af\u7684\u81ea\u52a8\u5e76\u884c\u5316\u4ee5\u53ca\u6570\u636e\u79fb\u52a8\u4f18\u5316\uff0c\u5728\u79d1\u5b66\u8ba1\u7b97\u6838\u5fc3\u7b97\u6cd5\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad812\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u7f16\u8bd1\u5668\u96be\u4ee5\u6709\u6548\u5206\u6790\u8de8\u6b65\u6570\u636e\u8bbf\u95ee\u548c\u5faa\u73af\u643a\u5e26\u4f9d\u8d56\u7b49\u5e38\u89c1\u6a21\u5f0f\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u6570\u636e\u79fb\u52a8\u4f18\u5316\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61\u65b9\u6cd5\u6765\u63d0\u5347\u4f18\u5316\u80fd\u529b\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\u5982\u4f55\u5bf9\u5177\u6709\u590d\u6742\u6570\u636e\u8bbf\u95ee\u6a21\u5f0f\uff08\u5982\u8de8\u6b65\u8bbf\u95ee\uff09\u7684\u591a\u5c42\u5faa\u73af\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u5b58\u5728\u5faa\u73af\u95f4\u4f9d\u8d56\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u81ea\u52a8\u5e76\u884c\u5316\u548c\u9ad8\u6548\u7684\u6570\u636e\u9884\u53d6\u4e0e\u5bc4\u5b58\u5668\u4f7f\u7528\u3002", "contributions": "\u63d0\u51fa\u4e86SILO\u6280\u672f\uff0c\u5f15\u5165\u7b26\u53f7\u5316\u3001\u5f52\u7eb3\u5f0f\u7684\u5faa\u73af\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5efa\u6a21\u6570\u636e\u8bbf\u95ee\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u652f\u6301\u81ea\u52a8\u5e76\u884c\u5316\u3001\u8f6f\u4ef6\u9884\u53d6\u548c\u6307\u9488\u9012\u589e\u4ee5\u51cf\u5c11\u5bc4\u5b58\u5668\u6ea2\u51fa\u3002", "results": "\u5728\u5927\u6c14\u6a21\u578b\u548c\u6570\u503c\u6c42\u89e3\u5668\u7b49\u79d1\u5b66\u5e94\u7528\u7684\u6838\u5fc3\u5185\u6838\u4e0a\u9a8c\u8bc1\u4e86SILO\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u5b9e\u73b0\u4e86\u6700\u9ad812\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SILO\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5faa\u73af\u6b65\u5e45\u7684\u51fd\u6570\u5f0f\u62bd\u8c61\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u591a\u7ef4\u6570\u7ec4\u5faa\u73af\u5d4c\u5957\u7684\u5206\u6790\u4e0e\u4f18\u5316\u80fd\u529b\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u8bd1\u65f6\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u4f20\u7edf\u7f16\u8bd1\u5668\u4f18\u5316\u6280\u672f\uff08\u5982\u5faa\u73af\u5c55\u5f00\u3001\u5206\u5757\uff09\u3001\u81ea\u52a8\u5e76\u884c\u5316\u65b9\u6cd5\u4ee5\u53ca\u73b0\u6709HPC\u6846\u67b6\u4e2d\u7684\u6570\u636e\u5c40\u90e8\u6027\u4f18\u5316\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9ad8\u5c42\u6a21\u5f0f\u5206\u6790\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002"}}
{"id": "2511.05506", "categories": ["cs.AR", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.05506", "abs": "https://arxiv.org/abs/2511.05506", "authors": ["Zhichao Chen", "Puneet Gupta"], "title": "YAP+: Pad-Layout-Aware Yield Modeling and Simulation for Hybrid Bonding", "comment": "The paper is currently under review by IEEE TCAD", "summary": "Three-dimensional (3D) integration continues to advance Moore's Law by facilitating dense interconnects and enabling multi-tier system architectures. Among the various integration approaches, Cu-Cu hybrid bonding has emerged as a leading solution for achieving high interconnect density in chiplet integration. In this work, we present YAP+, a yield modeling framework specifically tailored for wafer-to-wafer (W2W) and die-to-wafer (D2W) hybrid bonding processes. YAP+ incorporates a comprehensive set of yield-impacting failure mechanisms, including overlay misalignment, particle defects, Cu recess variations, surface roughness, and Cu pad density. Furthermore, YAP+ supports pad layout-aware yield analysis, considering critical, redundant, and dummy pads across arbitrary 2D physical layout patterns. To support practical evaluation, we developed an open-source yield simulator, demonstrating that our near-analytical model matches simulation accuracy while achieving over 1,000x speedup in runtime. This performance makes YAP+ a valuable tool for co-optimizing packaging technologies, assembly design rules, and system-level design strategies. Beyond W2W-D2W comparisons, we leverage YAP+ to investigate the impact of pad layout patterns, bonding pitch, and pad ratios across different pad types, and explore the benefits of strategically placing redundant pad replicas.", "AI": {"tldr": "YAP+\u662f\u4e00\u4e2a\u4e13\u4e3a\u6676\u5706\u5bf9\u6676\u5706\uff08W2W\uff09\u548c\u82af\u7247\u5bf9\u6676\u5706\uff08D2W\uff09\u6df7\u5408\u952e\u5408\u5de5\u827a\u8bbe\u8ba1\u7684\u826f\u7387\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u7efc\u5408\u8003\u8651\u591a\u79cd\u5f71\u54cd\u826f\u7387\u7684\u5931\u6548\u673a\u5236\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u710a\u76d8\u5e03\u5c40\u7684\u826f\u7387\u5206\u6790\uff0c\u7ed3\u5408\u5f00\u6e90\u4eff\u771f\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0e\u8d85\u5feb\u8fd0\u884c\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5c01\u88c5\u6280\u672f\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u7684\u534f\u540c\u4f18\u5316\u3002", "motivation": "\u968f\u77403D\u96c6\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0cCu-Cu\u6df7\u5408\u952e\u5408\u6210\u4e3a\u5b9e\u73b0\u9ad8\u5bc6\u5ea6\u4e92\u8fde\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u826f\u7387\u53d7\u591a\u79cd\u5de5\u827a\u7f3a\u9677\u5f71\u54cd\uff0c\u4e9f\u9700\u7cbe\u51c6\u9ad8\u6548\u7684\u826f\u7387\u9884\u6d4b\u6a21\u578b\u4ee5\u652f\u6301\u8bbe\u8ba1\u4e0e\u5236\u9020\u7684\u534f\u540c\u4f18\u5316\u3002", "challenges": "\u6df7\u5408\u952e\u5408\u5de5\u827a\u4e2d\u5b58\u5728\u591a\u79cd\u5f71\u54cd\u826f\u7387\u7684\u56e0\u7d20\uff0c\u5982\u5bf9\u51c6\u8bef\u5dee\u3001\u9897\u7c92\u6c61\u67d3\u3001\u94dc\u51f9\u9677\u3001\u8868\u9762\u7c97\u7cd9\u5ea6\u548c\u710a\u76d8\u5bc6\u5ea6\u7b49\uff0c\u4e14\u4e0d\u540c\u5e03\u5c40\u6a21\u5f0f\u4e0b\u7684\u5173\u952e\u3001\u5197\u4f59\u4e0e\u865a\u62df\u710a\u76d8\u76f8\u4e92\u4f5c\u7528\u590d\u6742\uff0c\u4f20\u7edf\u4eff\u771f\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u3002", "contributions": "\u63d0\u51fa\u4e86YAP+\u826f\u7387\u5efa\u6a21\u6846\u67b6\uff0c\u9996\u6b21\u5168\u9762\u6574\u5408\u591a\u79cd\u5b9e\u9645\u5de5\u827a\u5931\u6548\u673a\u5236\uff1b\u652f\u6301\u4efb\u610f\u4e8c\u7ef4\u5e03\u5c40\u7684\u710a\u76d8\u611f\u77e5\u5206\u6790\uff1b\u5f00\u53d1\u4e86\u5f00\u6e90\u826f\u7387\u4eff\u771f\u5668\uff0c\u5e76\u5b9e\u73b0\u6bd4\u4f20\u7edf\u4eff\u771f\u5feb1000\u500d\u4ee5\u4e0a\u7684\u8fd1\u89e3\u6790\u7ea7\u6a21\u578b\u3002", "results": "YAP+\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u4eff\u771f\u5668\u9ad8\u5ea6\u4e00\u81f4\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\u8d85\u8fc71000\u500d\uff1b\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u710a\u76d8\u5e03\u5c40\u3001\u952e\u5408\u95f4\u8ddd\u3001\u4e0d\u540c\u7c7b\u578b\u710a\u76d8\u6bd4\u4f8b\u53ca\u5197\u4f59\u710a\u76d8\u5e03\u7f6e\u7b56\u7565\u5bf9\u6574\u4f53\u826f\u7387\u7684\u5f71\u54cd\u89c4\u5f8b\u3002", "conclusion": "YAP+\u4e3aW2W\u548cD2W\u6df7\u5408\u952e\u5408\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u826f\u7387\u9884\u6d4b\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6307\u5bfc\u5c01\u88c5\u5de5\u827a\u4f18\u5316\u3001\u7ec4\u88c5\u8bbe\u8ba1\u89c4\u5219\u5236\u5b9a\u548c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u51b3\u7b56\uff0c\u63a8\u52a8\u82af\u7c92\u96c6\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002", "related_work": "\u6b64\u524d\u7684\u826f\u7387\u6a21\u578b\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u7f3a\u9677\u6e90\u6216\u7b80\u5316\u5047\u8bbe\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u5e03\u5c40\u4e2d\u591a\u79cd\u710a\u76d8\u7c7b\u578b\uff08\u5173\u952e\u3001\u5197\u4f59\u3001\u865a\u62df\uff09\u7684\u7efc\u5408\u5efa\u6a21\u80fd\u529b\uff0c\u800cYAP+\u5728\u6b64\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u3001\u66f4\u8d34\u8fd1\u5b9e\u9645\u5de5\u827a\u7684\u5efa\u6a21\u4e0e\u5feb\u901f\u5206\u6790\u3002"}}
{"id": "2511.05583", "categories": ["cs.AR", "physics.ins-det", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.05583", "abs": "https://arxiv.org/abs/2511.05583", "authors": ["Sunwoo Park", "Byungkwon Park", "Eunsung Kim", "Jiwon Yune", "Seungho Han", "Seunggo Nam"], "title": "Delay Time Characterization on FPGA: A Low Nonlinearity, Picosecond Resolution Time-to-Digital Converter on 16-nm FPGA using Bin Sequence Calibration", "comment": null, "summary": "We present a Time-to-Digital Converter (TDC) implemented on a 16 nm Xilinx UltraScale Plus FPGA that achieves a resolution of 1.15 ps, RMS precision of 3.38 ps, a differential nonlinearity (DNL) of [-0.43, 0.24] LSB, and an integral nonlinearity (INL) of [-2.67, 0.15] LSB. This work introduces two novel hardware-independent post-processing techniques - Partial Order Reconstruction (POR) and Iterative Time-bin Interleaving (ITI) - that significantly enhance the performance of FPGA-based TDCs. POR addresses the missing code problem by inferring the partial order of each time bin through code density test data and directed acyclic graph (DAG) analysis, enabling near-complete recovery of usable bins. ITI further improves fine time resolution by merging multiple calibrated tapped delay lines (TDLs) into a single unified delay chain, achieving scalable resolution without resorting to averaging. Compared to state-of-the-art FPGA-based TDC architectures, the proposed methods deliver competitive or superior performance with reduced hardware overhead. These techniques are broadly applicable to high-resolution time measurement and precise delay calibration in programmable logic platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u572816 nm Xilinx UltraScale Plus FPGA\u4e0a\u5b9e\u73b0\u7684\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u6570\u5b57\u8f6c\u6362\u5668\uff08TDC\uff09\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u9896\u7684\u786c\u4ef6\u65e0\u5173\u540e\u5904\u7406\u6280\u672f\u2014\u2014\u90e8\u5206\u5e8f\u91cd\u5efa\uff08POR\uff09\u548c\u8fed\u4ee3\u65f6\u95f4\u4ea4\u7ec7\uff08ITI\uff09\u2014\u2014\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5b9e\u73b0\u4e861.15 ps\u7684\u5206\u8fa8\u7387\u548c3.38 ps\u7684\u5747\u65b9\u6839\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728FPGA\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u548c\u5206\u8fa8\u7387\u7684\u65f6\u95f4\u6d4b\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u5bf9\u4e13\u7528\u786c\u4ef6\u7684\u4f9d\u8d56\u548c\u8d44\u6e90\u5f00\u9500\u3002", "challenges": "FPGA\u4e0a\u7684TDC\u9762\u4e34\u65f6\u95f4\u5206\u8fa8\u6709\u9650\u3001\u975e\u7ebf\u6027\u8bef\u5dee\uff08\u5982DNL\u548cINL\uff09\u4ee5\u53ca\u7f3a\u5931\u7801\u95ee\u9898\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff1a\u90e8\u5206\u5e8f\u91cd\u5efa\uff08POR\uff09\u89e3\u51b3\u7f3a\u5931\u7801\u95ee\u9898\uff1b\u8fed\u4ee3\u65f6\u95f4\u4ea4\u7ec7\uff08ITI\uff09\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u6821\u51c6\u5ef6\u8fdf\u94fe\u63d0\u5347\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u4e0d\u4f9d\u8d56\u7279\u5b9a\u786c\u4ef6\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u79fb\u690d\u6027\u548c\u6269\u5c55\u6027\u3002", "results": "\u5b9e\u73b0\u4e861.15 ps\u7684\u5206\u8fa8\u7387\u30013.38 ps\u7684RMS\u7cbe\u5ea6\uff0cDNL\u4e3a[-0.43, 0.24] LSB\uff0cINL\u4e3a[-2.67, 0.15] LSB\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524d\u6700\u5148\u8fdb\u7684FPGA-TDC\u8bbe\u8ba1\uff0c\u4e14\u786c\u4ef6\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "POR\u548cITI\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86FPGA\u4e0aTDC\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ebf\u6027\u5ea6\uff0c\u4e3a\u53ef\u7f16\u7a0b\u903b\u8f91\u5e73\u53f0\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u6d4b\u91cf\u548c\u5ef6\u8fdf\u6821\u51c6\u63d0\u4f9b\u4e86\u901a\u7528\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u73b0\u6709\u5de5\u4f5c\u591a\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u786c\u4ef6\u7ed3\u6784\u6216\u5e73\u5747\u6280\u672f\u6765\u63d0\u5347\u5206\u8fa8\u7387\uff0c\u5982\u62bd\u5934\u5ef6\u8fdf\u7ebf\uff08TDL\uff09\u9635\u5217\u548c\u65f6\u95f4\u4ea4\u7ec7\u67b6\u6784\uff0c\u4f46\u901a\u5e38\u4f34\u968f\u9ad8\u8d44\u6e90\u6d88\u8017\u548c\u975e\u7406\u60f3\u6027\u95ee\u9898\u3002"}}
{"id": "2511.06187", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06187", "abs": "https://arxiv.org/abs/2511.06187", "authors": ["Mathew Joseph", "Tanush Savadi", "Abel Souza"], "title": "LiteCast: A Lightweight Forecaster for Carbon Optimizations", "comment": null, "summary": "Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.", "AI": {"tldr": "LiteCast\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u5efa\u6a21\u533a\u57df\u80fd\u6e90\u7ed3\u6784\u5e76\u4f30\u7b97\u78b3\u5f3a\u5ea6\uff0c\u4ec5\u9700\u51e0\u5929\u7684\u5386\u53f2\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u5feb\u901f\u9002\u5e94\u7535\u7f51\u53d8\u5316\u7684\u9884\u6d4b\uff0c\u572850\u4e2a\u5168\u7403\u533a\u57df\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8282\u7701\u4e8620%\u4ee5\u4e0a\u7684\u6210\u672c\uff0c\u5e76\u8fbe\u5230\u6700\u5927\u53ef\u5b9e\u73b0\u5e73\u5747\u8282\u7701\u768497%\u3002", "motivation": "\u968f\u7740\u7535\u6c14\u5316\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u7535\u529b\u9700\u6c42\u6301\u7eed\u589e\u957f\uff0c\u7535\u7f51\u901a\u8fc7\u6269\u5927\u53d1\u7535\u80fd\u529b\u548c\u5f15\u5165\u53ef\u518d\u751f\u80fd\u6e90\u6765\u5e94\u5bf9\u8d1f\u8377\u6fc0\u589e\uff0c\u4f46\u8fd9\u5f71\u54cd\u4e86\u533a\u57df\u78b3\u5f3a\u5ea6\u3002\u4e3a\u51cf\u5c11\u6d88\u8d39\u5e26\u6765\u7684\u73af\u5883\u5f71\u54cd\uff0c\u73b0\u6709\u7684\u78b3\u611f\u77e5\u4f18\u5316\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u3001\u957f\u65f6\u57df\u7684\u78b3\u5f3a\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u8ba1\u7b97\u590d\u6742\u3001\u6269\u5c55\u6027\u5dee\uff0c\u4e14\u7cbe\u5ea6\u63d0\u5347\u5e76\u4e0d\u76f4\u63a5\u8f6c\u5316\u4e3a\u8282\u80fd\u6548\u679c\u7684\u7ebf\u6027\u589e\u957f\u3002\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u7b56\u7565\u3002", "challenges": "\u5982\u4f55\u5728\u4e0d\u727a\u7272\u8282\u80fd\u6548\u679c\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u78b3\u5f3a\u5ea6\u9884\u6d4b\u6a21\u578b\u7684\u590d\u6742\u5ea6\u548c\u6570\u636e\u9700\u6c42\uff1b\u5982\u4f55\u4f7f\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u7535\u7f51\u4e2d\u7684\u7a81\u53d1\u53d8\u5316\uff1b\u5982\u4f55\u5728\u591a\u79cd\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u548c\u4e0d\u540c\u533a\u57df\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "contributions": "\u63d0\u51faLiteCast\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u78b3\u5f3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u6570\u5929\u5386\u53f2\u6570\u636e\u5373\u53ef\u8fd0\u884c\uff1b\u8bc1\u660e\u9884\u6d4b\u6392\u5e8f\u7684\u4fdd\u771f\u5ea6\u6bd4\u7edd\u5bf9\u7cbe\u5ea6\u66f4\u80fd\u51b3\u5b9a\u8282\u80fd\u6548\u679c\uff1b\u572850\u4e2a\u5168\u7403\u533a\u57df\u7684\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86LiteCast\u5728\u8282\u80fd\u65b9\u9762\u7684\u4f18\u8d8a\u8868\u73b0\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "results": "LiteCast\u5728\u591a\u4e2a\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u5168\u740350\u4e2a\u533a\u57df\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6a21\u578b\u5b9e\u73b0\u4e8620%\u66f4\u9ad8\u7684\u8282\u80fd\u6548\u679c\uff0c\u5e76\u8fbe\u5230\u4e86\u7406\u8bba\u6700\u5927\u5e73\u5747\u8282\u80fd\u768497%\uff0c\u540c\u65f6\u5177\u5907\u5feb\u901f\u63a8\u7406\u548c\u5feb\u901f\u9002\u5e94\u65b0\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "\u78b3\u5f3a\u5ea6\u9884\u6d4b\u4e0d\u5fc5\u4f9d\u8d56\u590d\u6742\u7684\u9ad8\u7cbe\u5ea6\u6a21\u578b\u6765\u5b9e\u73b0\u663e\u8457\u7684\u8282\u80fd\u6548\u679c\uff0cLiteCast\u901a\u8fc7\u4fdd\u6301\u9884\u6d4b\u6392\u540d\u7684\u51c6\u786e\u6027\uff0c\u5728\u6781\u4f4e\u7684\u6570\u636e\u548c\u8ba1\u7b97\u9700\u6c42\u4e0b\u5b9e\u73b0\u4e86\u8fd1\u6700\u4f18\u7684\u8282\u80fd\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u78b3\u611f\u77e5\u8c03\u5ea6\u7684\u53ef\u884c\u6027\u3002", "related_work": "\u672c\u6587\u4e0e\u78b3\u611f\u77e5\u8ba1\u7b97\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u80fd\u6e90\u7cfb\u7edf\u5efa\u6a21\u4ee5\u53ca\u57fa\u4e8e\u9700\u6c42\u54cd\u5e94\u7684\u8282\u80fd\u7b56\u7565\u76f8\u5173\uff0c\u7279\u522b\u662f\u4e0e\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u957f\u671f\u78b3\u5f3a\u5ea6\u9884\u6d4b\u7684\u5de5\u4f5c\u5f62\u6210\u5bf9\u6bd4\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u6548\u7387\u4e0e\u5b9e\u9645\u8282\u80fd\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2511.06174", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06174", "abs": "https://arxiv.org/abs/2511.06174", "authors": ["Zifan He", "Shengyu Ye", "Rui Ma", "Yang Wang", "Jason Cong"], "title": "LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs", "comment": null, "summary": "The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.", "AI": {"tldr": "LUT-LLM\u662f\u9996\u4e2a\u57fa\u4e8eFPGA\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u548c\u5185\u5b58\u67e5\u627e\u5c06LLM\u63a8\u7406\u4ece\u7b97\u672f\u5bc6\u96c6\u578b\u8f6c\u53d8\u4e3a\u5185\u5b58\u5bc6\u96c6\u578b\uff0c\u57281B\u4ee5\u4e0a\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u6548\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5355\u6279\u6b21\u63a8\u7406\u6548\u7387\u5bf9\u4e8e\u7aef\u4fa7\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002FPGA\u867d\u5177\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5728\u7b97\u672f\u8ba1\u7b97\u4e0a\u5df2\u88abGPU\u4f18\u5316\u7f29\u5c0f\u5dee\u8ddd\uff0c\u56e0\u6b64\u9700\u63a2\u7d22\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u4ee5\u53d1\u6325\u5176\u5185\u5b58\u4f18\u52bf\u3002", "challenges": "\u5982\u4f55\u5728FPGA\u4e0a\u9ad8\u6548\u8fd0\u884c1B\u4ee5\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff1b\u5982\u4f55\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff1b\u5982\u4f55\u5728\u91cf\u5316\u540e\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\uff1b\u5982\u4f55\u8bbe\u8ba1\u9ad8\u6548\u7684\u67e5\u627e\u8868\u67b6\u6784\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u5411\u91cf\u68c0\u7d22\u3002", "contributions": "\u63d0\u51fa\u4e86LUT-LLM\uff0c\u9996\u4e2a\u57fa\u4e8e\u67e5\u627e\u8868\u7684FPGA\u5927\u6a21\u578b\u52a0\u901f\u5668\uff1b\u63d0\u51fa\u6fc0\u6d3b\u503c\u4e0e\u6743\u91cd\u8054\u5408\u91cf\u5316\u65b9\u6848\uff1b\u8bbe\u8ba1\u4e86\u5e26\u5bbd\u611f\u77e5\u7684\u5e76\u884c\u8d28\u5fc3\u641c\u7d22\u3001\u9ad8\u6548\u7684\u4e8c\u7ef4\u67e5\u627e\u8868\u548c\u65f6\u7a7a\u6df7\u5408\u67b6\u6784\u3002", "results": "\u5728AMD V80 FPGA\u4e0a\u5b9e\u73b0Qwen 3 1.7B\u6a21\u578b\uff0c\u76f8\u6bd4AMD MI210\u964d\u4f4e1.66\u500d\u5ef6\u8fdf\uff0c\u76f8\u6bd4NVIDIA A100\u63d0\u53471.72\u500d\u80fd\u6548\uff1b\u53ef\u6269\u5c55\u81f332B\u6a21\u578b\uff0c\u80fd\u6548\u6bd4A100\u9ad82.16\u500d\u3002", "conclusion": "LUT-LLM\u901a\u8fc7\u5c06LLM\u63a8\u7406\u8f6c\u4e3a\u5185\u5b58\u67e5\u627e\u64cd\u4f5c\uff0c\u5145\u5206\u53d1\u6325FPGA\u7684\u5185\u5b58\u4f18\u52bf\uff0c\u5728\u5927\u6a21\u578b\u5355\u6279\u6b21\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u63d0\u5347\uff0c\u4e3a\u7aef\u4fa7\u5927\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002", "related_work": "\u73b0\u6709\u5de5\u4f5c\u591a\u805a\u7126\u4e8eGPU\u4e0a\u7684\u91cf\u5316\u4e0e\u7a00\u758f\u5316\u4f18\u5316\uff0c\u6216FPGA\u4e0a\u7684\u7b97\u672f\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff1b\u800c\u672c\u6587\u9996\u6b21\u63a2\u7d22\u5c06FPGA\u7528\u4e8e\u5168\u67e5\u627e\u8868\u5f0f\u7684\u5927\u6a21\u578b\u63a8\u7406\uff0c\u5f00\u8f9f\u4e86\u4e0d\u540c\u4e8e\u4e3b\u6d41\u7684\u5185\u5b58\u9a71\u52a8\u8303\u5f0f\u3002"}}
{"id": "2511.06249", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06249", "abs": "https://arxiv.org/abs/2511.06249", "authors": ["Omin Kwon", "Kyungjun Oh", "Jaeyong Lee", "Myungsuk Kim", "Jihong Kim"], "title": "STAR: Improving Lifetime and Performance of High-Capacity Modern SSDs Using State-Aware Randomizer", "comment": "To appear in the Proceedings of the 2025 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2025)", "summary": "Although NAND flash memory has achieved continuous capacity improvements via advanced 3D stacking and multi-level cell technologies, these innovations introduce new reliability challenges, particularly lateral charge spreading (LCS), absent in low-capacity 2D flash memory. Since LCS significantly increases retention errors over time, addressing this problem is essential to ensure the lifetime of modern SSDs employing high-capacity 3D flash memory. In this paper, we propose a novel data randomizer, STate-Aware Randomizer (STAR), which proactively eliminates the majority of weak data patterns responsible for retention errors caused by LCS. Unlike existing techniques that target only specific worst-case patterns, STAR effectively removes a broad spectrum of weak patterns, significantly enhancing reliability against LCS. By employing several optimization schemes, STAR can be efficiently integrated into the existing I/O datapath of an SSD controller with negligible timing overhead. To evaluate the proposed STAR scheme, we developed a STAR-aware SSD emulator based on characterization results from 160 real 3D NAND flash chips. Experimental results demonstrate that STAR improves SSD lifetime by up to 2.3x and reduces read latency by an average of 50% on real-world traces compared to conventional SSDs", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6570\u636e\u968f\u673a\u5316\u5668STAR\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5bb9\u91cf3D NAND\u95ea\u5b58\u4e2d\u7531\u6a2a\u5411\u7535\u8377\u6269\u6563\uff08LCS\uff09\u5f15\u8d77\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86SSD\u7684\u5bff\u547d\u548c\u8bfb\u53d6\u6027\u80fd\u3002", "motivation": "\u968f\u77403D\u5806\u53e0\u548c\u591a\u5c42\u5355\u5143\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u5bb9\u91cfNAND\u95ea\u5b58\u9762\u4e34\u6a2a\u5411\u7535\u8377\u6269\u6563\uff08LCS\uff09\u5e26\u6765\u7684\u6570\u636e\u4fdd\u6301\u9519\u8bef\u95ee\u9898\uff0c\u5f71\u54cdSSD\u5bff\u547d\uff0c\u9700\u6709\u6548\u5e94\u5bf9\u3002", "challenges": "LCS\u5728\u4f20\u7edf2D\u95ea\u5b58\u4e2d\u4e0d\u5b58\u5728\uff0c\u4f46\u5728\u9ad8\u5bc6\u5ea63D\u95ea\u5b58\u4e2d\u663e\u8457\u589e\u52a0\u6570\u636e\u4fdd\u6301\u9519\u8bef\uff1b\u73b0\u6709\u65b9\u6cd5\u4ec5\u9488\u5bf9\u7279\u5b9a\u6700\u5dee\u6a21\u5f0f\uff0c\u65e0\u6cd5\u5168\u9762\u6d88\u9664\u5404\u7c7b\u5f31\u6570\u636e\u6a21\u5f0f\u3002", "contributions": "\u63d0\u51fa\u4e86STate-Aware Randomizer\uff08STAR\uff09\uff0c\u80fd\u4e3b\u52a8\u6d88\u9664\u5bfc\u81f4LCS\u76f8\u5173\u4fdd\u6301\u9519\u8bef\u7684\u591a\u79cd\u5f31\u6570\u636e\u6a21\u5f0f\uff1b\u8bbe\u8ba1\u4e86\u591a\u9879\u4f18\u5316\u65b9\u6848\uff0c\u4f7fSTAR\u53ef\u4f4e\u5f00\u9500\u96c6\u6210\u5230SSD\u63a7\u5236\u5668I/O\u8def\u5f84\u4e2d\u3002", "results": "\u57fa\u4e8e160\u4e2a\u771f\u5b9e3D NAND\u82af\u7247\u6784\u5efaSTAR\u611f\u77e5SSD\u6a21\u62df\u5668\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSTAR\u53ef\u5c06SSD\u5bff\u547d\u63d0\u5347\u6700\u9ad82.3\u500d\uff0c\u5e76\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5e73\u5747\u51cf\u5c1150%\u7684\u8bfb\u53d6\u5ef6\u8fdf\u3002", "conclusion": "STAR\u80fd\u6709\u6548\u7f13\u89e3\u9ad8\u5bb9\u91cf3D NAND\u95ea\u5b58\u4e2d\u7684LCS\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u53ef\u9760\u6027\u4e0eSSD\u6027\u80fd\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7ea0\u9519\u7801\u3001\u5199\u5165\u5747\u8861\u548c\u6570\u636e\u5237\u65b0\u7b49\u673a\u5236\u6765\u7f13\u89e3\u4fdd\u6301\u9519\u8bef\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u6a21\u5f0f\u5c42\u9762\u5f31\u4fe1\u53f7\u7684\u7cfb\u7edf\u6027\u968f\u673a\u5316\u5904\u7406\uff0cSTAR\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002"}}
{"id": "2511.06345", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06345", "abs": "https://arxiv.org/abs/2511.06345", "authors": ["Kelun Lei", "Hailong Yang", "Huaitao Zhang", "Xin You", "Kaige Zhang", "Zhongzhi Luan", "Yi Liu", "Depei Qian"], "title": "PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization", "comment": null, "summary": "Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\\times$ and 2.30$\\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.", "AI": {"tldr": "PRAGMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6027\u80fd\u5206\u6790\u7684AI\u5185\u6838\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6267\u884c\u53cd\u9988\u548c\u7ec6\u7c92\u5ea6\u786c\u4ef6\u5206\u6790\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u5e76\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347CPU\u548cGPU\u4e0a\u7684\u5185\u6838\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684AI\u5185\u6838\u751f\u6210\u7cfb\u7edf\u5927\u591a\u4ec5\u4f9d\u8d56\u6b63\u786e\u6027\u6216\u6267\u884c\u65f6\u95f4\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u6027\u80fd\u74f6\u9888\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u751f\u6210\u4ee3\u7801\u7684\u6027\u80fd\u4f18\u5316\u6f5c\u529b\u3002", "challenges": "\u5982\u4f55\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6548\u8bc6\u522b\u548c\u7406\u89e3\u786c\u4ef6\u5c42\u9762\u7684\u6027\u80fd\u74f6\u9888\uff1b\u5982\u4f55\u5728\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u5386\u53f2\u6700\u4f18\u7248\u672c\u5e76\u6307\u5bfc\u4ee3\u7801\u6539\u8fdb\uff1b\u5982\u4f55\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\uff08\u5982CPU\u548cGPU\uff09\u4e0a\u5b9e\u73b0\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "contributions": "\u63d0\u51fa\u4e86PRAGMA\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u7ec6\u7c92\u5ea6\u786c\u4ef6\u6027\u80fd\u5206\u6790\u5f15\u5165AI\u5185\u6838\u751f\u6210\u7684\u63a8\u7406\u5faa\u73af\uff1b\u8bbe\u8ba1\u4e86\u4fdd\u7559\u6700\u4f73\u5386\u53f2\u7248\u672c\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u673a\u5236\uff1b\u5728CPU\u548cGPU\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "results": "\u5728KernelBench\u57fa\u51c6\u4e0a\uff0cPRAGMA\u76f8\u6bd4\u65e0\u5206\u6790\u529f\u80fd\u7684\u57fa\u7ebfAIKG\u8868\u73b0\u66f4\u4f18\uff0c\u76f8\u8f83\u4e8eTorch\u5728CPU\u548cGPU\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u57472.81\u500d\u548c2.30\u500d\u7684\u52a0\u901f\u3002", "conclusion": "PRAGMA\u901a\u8fc7\u5f15\u5165\u6027\u80fd\u5206\u6790\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u5185\u6838\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c06\u786c\u4ef6\u5206\u6790\u4e0e\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u9ad8\u6027\u80fd\u5185\u6838\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u81ea\u52a8\u5185\u6838\u4f18\u5316\u5de5\u5177\uff08\u5982TVM\u3001Halide\uff09\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff08\u5982CodeGen\u3001PaLM Code\uff09\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u6216\u641c\u7d22\u7684AI\u5185\u6838\u751f\u6210\uff08AIKG\uff09\u7cfb\u7edf\u3002"}}
{"id": "2511.06599", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06599", "abs": "https://arxiv.org/abs/2511.06599", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads", "comment": "12 pages, 9 figures, 1 table, 2 algorithms", "summary": "FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaarthi\u7684\u65b0\u578b\u7aef\u5230\u7aef\u65e0\u670d\u52a1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u611f\u77e5\u548c\u591a\u76ee\u6807\u4f18\u5316\u6a21\u578b\uff0c\u52a8\u6001\u7ba1\u7406\u51fd\u6570\u8d44\u6e90\u9700\u6c42\uff0c\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3FaaS\u5e73\u53f0\u4e2d\u5b58\u5728\u7684\u542f\u52a8\u5ef6\u8fdf\u3001\u9759\u6001\u8d44\u6e90\u914d\u7f6e\u3001\u8d44\u6e90\u5206\u914d\u4e0e\u8c03\u5ea6\u6b21\u4f18\u7b49\u95ee\u9898\uff0c\u63d0\u5347\u51fd\u6570\u6027\u80fd\u7684\u4e00\u81f4\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "challenges": "\u4e3b\u8981\u5305\u62ec\u51b7\u542f\u52a8\u5ef6\u8fdf\u3001\u9759\u6001\u8d44\u6e90\u5206\u914d\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u3001\u5de5\u4f5c\u8d1f\u8f7d\u65e0\u5173\u7684\u901a\u7528\u8c03\u5ea6\u7b56\u7565\u4ee5\u53ca\u8fd0\u884c\u6210\u672c\u4e0d\u53ef\u9884\u6d4b\u7b49\u6311\u6218\u3002", "contributions": "1. \u63d0\u51faSaarthi\u6846\u67b6\uff0c\u5b9e\u73b0\u8f93\u5165\u611f\u77e5\u7684\u8d44\u6e90\u9884\u6d4b\uff1b2. \u8bbe\u8ba1\u57fa\u4e8eILP\u7684\u591a\u76ee\u6807\u4f18\u5316\u6a21\u578b\u4ee5\u5e73\u8861\u541e\u5410\u91cf\u4e0e\u6210\u672c\uff1b3. \u5f15\u5165\u4e3b\u52a8\u5bb9\u9519\u5197\u4f59\u673a\u5236\uff1b4. \u5b9e\u73b0\u52a8\u6001\u8bf7\u6c42\u7f16\u6392\u4e0e\u51fd\u6570\u914d\u7f6e\u7ba1\u7406\u3002", "results": "\u5728OpenFaaS\u4e0a\u5b9e\u73b0Saarthi\u540e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad81.45\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u6210\u672c\u964d\u4f4e1.84\u500d\uff0c\u5e76\u80fd\u7ef4\u6301\u9ad8\u8fbe98.3%\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff0c\u989d\u5916\u5f00\u9500\u6700\u591a\u4e3a0.2\u79d2\u3002", "conclusion": "Saarthi\u901a\u8fc7\u667a\u80fd\u3001\u52a8\u6001\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u81ea\u9a71\u5f0f\uff08self-driving\uff09\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u53d1\u5c55\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u57fa\u4e8e\u9759\u6001\u914d\u7f6e\u7684FaaS\u5e73\u53f0\u5982OpenFaaS\u3001Knative\uff0c\u4ee5\u53ca\u4e00\u4e9b\u52a8\u6001\u8d44\u6e90\u8c03\u6574\u673a\u5236\u7684\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8f93\u5165\u7279\u5f81\u7684\u611f\u77e5\u548c\u591a\u76ee\u6807\u8054\u5408\u4f18\u5316\u3002"}}
{"id": "2511.06558", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06558", "abs": "https://arxiv.org/abs/2511.06558", "authors": ["Akshay Revankar", "Charan Renganathan", "Sartaj Wariah"], "title": "Offloading Data Center Tax", "comment": null, "summary": "The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6570\u636e\u4e2d\u5fc3\u4e2dMongoDB\u5fae\u670d\u52a1\u7684\u7a0e\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5728DeathStarBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u8ba8\u4e86\u5c06\u591a\u4e2a\u7a0e\u7ec4\u4ef6\u5171\u540c\u5378\u8f7d\u5230\u52a0\u901f\u5668\u4e0a\u7684\u4f18\u5316\u673a\u4f1a\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u7740\u591a\u6837\u5316\u7684\u8d1f\u8f7d\uff0c\u5171\u4eab\u8bb8\u591a\u5e95\u5c42\u529f\u80fd\uff08\u7a0e\u7ec4\u4ef6\uff09\uff0c\u5bf9\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u4f18\u5316\u53ef\u5e7f\u6cdb\u63d0\u5347\u6027\u80fd\u3002\u7136\u800c\uff0c\u5e76\u975e\u6240\u6709\u7a0e\u7ec4\u4ef6\u90fd\u9002\u5408\u5355\u72ec\u5378\u8f7d\u5230\u52a0\u901f\u5668\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8054\u5408\u5378\u8f7d\u7b56\u7565\u3002", "challenges": "\u8bc6\u522bMongoDB\u4e2d\u7684\u5173\u952e\u7a0e\u7ec4\u4ef6\u53ca\u5176\u5fae\u67b6\u6784\u5f71\u54cd\uff1b\u786e\u5b9a\u54ea\u4e9b\u7ec4\u4ef6\u9002\u5408\u8054\u5408\u5378\u8f7d\uff1b\u5728\u4e0d\u589e\u52a0\u786c\u4ef6\u590d\u6742\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6027\u80fd\u589e\u76ca\u3002", "contributions": "1) \u5bf9\u8fd0\u884c\u5728DeathStarBench\u4e0b\u7684MongoDB\u8fdb\u884c\u8be6\u7ec6\u5256\u6790\uff0c\u8bc6\u522b\u5176\u7a0e\u7ec4\u4ef6\uff1b2) \u5206\u6790\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u6027\u80fd\u7279\u5f81\u548c\u5fae\u67b6\u6784\u5f00\u9500\uff1b3) \u63d0\u51fa\u9488\u5bf9\u591a\u4e2a\u7a0e\u7ec4\u4ef6\u8054\u5408\u5378\u8f7d\u7684\u4f18\u5316\u5efa\u8bae\u3002", "results": "\u901a\u8fc7\u5256\u6790\u53d1\u73b0\u4e86MongoDB\u4e2d\u82e5\u5e72\u5173\u952e\u7a0e\u7ec4\u4ef6\u53ca\u5176\u8d44\u6e90\u6d88\u8017\u6a21\u5f0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u8054\u5408\u5378\u8f7d\u65b9\u6848\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7ea7\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "conclusion": "\u8054\u5408\u5378\u8f7d\u591a\u4e2a\u7a0e\u7ec4\u4ef6\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u6570\u636e\u4e2d\u5fc3\u4f18\u5316\u65b9\u5411\uff0c\u672c\u6587\u4ee5MongoDB\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u6570\u636e\u4e2d\u5fc3\u7a0e\u7ec4\u4ef6\u7684\u7814\u7a76\u3001\u901a\u7528\u529f\u80fd\u5378\u8f7d\uff08\u5982\u538b\u7f29\u3001\u52a0\u5bc6\uff09\u3001\u4ee5\u53caDeathStarBench\u5fae\u670d\u52a1\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u7684\u5e94\u7528\u3002"}}
{"id": "2511.06605", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06605", "abs": "https://arxiv.org/abs/2511.06605", "authors": ["Suchita Pati", "Mahzabeen Islam", "Shaizeen Aga", "Mohamed Assem Ibrahim"], "title": "DMA Collectives for Efficient ML Communication Offloads", "comment": null, "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728AMD Instinct MI300X GPU\u4e0a\u5c06\u673a\u5668\u5b66\u4e60\u901a\u4fe1\u89c4\u7ea6\uff08\u5982all-gather\u3001all-to-all\uff09\u5378\u8f7d\u5230DMA\u5f15\u64ce\u7684\u6027\u80fd\u3001\u529f\u8017\u548c\u540c\u6b65\u5f00\u9500\uff0c\u53d1\u73b0DMA\u5728\u5927\u5c3a\u5bf8\u4f20\u8f93\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5c0f\u5c3a\u5bf8\u573a\u666f\u4e0b\u56e0\u8c03\u5ea6\u4e0e\u540c\u6b65\u5f00\u9500\u800c\u6027\u80fd\u8f83\u5dee\uff1b\u901a\u8fc7\u5229\u7528\u73b0\u6709DMA\u67b6\u6784\u521b\u65b0\u4f18\u5316\u540e\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\uff0c\u63a8\u52a8DMA\u89c4\u7ea6\u5411\u4e3b\u6d41\u5e93\u5b9e\u7528\u5316\u8fc8\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u901a\u4fe1\u4e0e\u8ba1\u7b97\u7684\u91cd\u53e0\u6548\u7387\uff0c\u964d\u4f4eGPU\u6838\u5fc3\u548c\u5185\u5b58\u5b50\u7cfb\u7edf\u7684\u8d1f\u62c5\uff0c\u7814\u7a76\u5c06\u901a\u4fe1\u89c4\u7ea6\u5378\u8f7d\u5230DMA\u5f15\u64ce\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4ec5\u9650\u4e8e\u5e26\u5bbd\u5bc6\u96c6\u578b\u5927\u4f20\u8f93\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u529f\u8017\u3001\u80fd\u6548\u548c\u5c0f\u6570\u636e\u91cf\u5ef6\u8fdf\u7684\u5168\u9762\u5206\u6790\u3002", "challenges": "DMA\u96c6\u4f53\u901a\u4fe1\u5728\u5c0f\u6570\u636e\u91cf\uff08latency-bound\uff09\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u843d\u540e\uff0c\u4e3b\u8981\u53d7\u9650\u4e8eDMA\u547d\u4ee4\u8c03\u5ea6\u548c\u540c\u6b65\u5f00\u9500\uff1b\u540c\u65f6\u7f3a\u4e4f\u5bf9\u6027\u80fd\u3001\u529f\u8017\u548c\u80fd\u6548\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "contributions": "1\uff09\u9996\u6b21\u5bf9DMA\u96c6\u4f53\u901a\u4fe1\u5728\u5148\u8fdbGPU\u4e0a\u7684\u6027\u80fd\u3001\u529f\u8017\u548c\u540c\u6b65\u6210\u672c\u8fdb\u884c\u5168\u9762\u5206\u6790\uff1b2\uff09\u63ed\u793a\u4e86DMA\u5728\u5927\u5c0f\u6570\u636e\u91cf\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u74f6\u9888\uff1b3\uff09\u5229\u7528\u672a\u88ab\u5145\u5206\u5229\u7528\u7684DMA\u67b6\u6784\u521b\u65b0\u4f18\u5316\u96c6\u4f53\u901a\u4fe1\u5b9e\u73b0\uff1b4\uff09\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u5316\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6570\u636e\u91cf\u6027\u80fd\u5e76\u8fdb\u4e00\u6b65\u6539\u5584\u5927\u6570\u636e\u91cf\u6548\u7387\u3002", "results": "\u76f8\u6bd4RCCL\uff0cDMA\u96c6\u4f53\u5728\u5927\u5c3a\u5bf8\uff08\u6570\u5341MB\u81f3GB\uff09\u4e0b\u6027\u80fd\u63d0\u534716%\u3001\u529f\u8017\u964d\u4f4e32%\uff1b\u4f46\u5728\u5c0f\u5c3a\u5bf8\u4e0ball-gather\u61624.5\u500d\uff0call-to-all\u61622.5\u500d\u3002\u7ecf\u4f18\u5316\u540e\uff0c\u5c0f\u5c3a\u5bf8\u4e0ball-gather\u4ec5\u616230%\uff0call-to-all\u53cd\u800c\u5feb20%\uff0c\u5927\u5c3a\u5bf8\u6027\u80fd\u518d\u63d0\u53477%\uff0c\u529f\u8017\u8282\u7701\u589e\u52a03-10%\u3002", "conclusion": "DMA\u96c6\u4f53\u901a\u4fe1\u5728\u5927\u5c3a\u5bf8\u4f20\u8f93\u4e2d\u5df2\u5177\u5907\u4f18\u52bf\uff0c\u4f46\u5c0f\u5c3a\u5bf8\u6027\u80fd\u53d7\u9650\u4e8e\u8c03\u5ea6\u4e0e\u540c\u6b65\u5f00\u9500\uff1b\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4f18\u5316\u53ef\u663e\u8457\u7f29\u5c0f\u5dee\u8ddd\uff0c\u4f7fDMA\u96c6\u4f53\u66f4\u63a5\u8fd1\u4e3b\u6d41\u901a\u4fe1\u5e93\u7684\u5b9e\u7528\u8981\u6c42\u3002", "related_work": "\u6b64\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5c06\u901a\u4fe1\u4efb\u52a1\u5378\u8f7d\u5230DMA\u4ee5\u63d0\u5347\u5e76\u53d1\u6027\u80fd\uff0c\u4f46\u4ec5\u8bc4\u4f30\u4e86\u5e26\u5bbd\u53d7\u9650\u7684\u5927\u4f20\u8f93\u573a\u666f\uff0c\u4e14\u591a\u5173\u6ce8\u6027\u80fd\u800c\u5ffd\u89c6\u529f\u8017\u4e0e\u540c\u6b65\u6210\u672c\u3002"}}
{"id": "2511.06565", "categories": ["cs.AR", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.06565", "abs": "https://arxiv.org/abs/2511.06565", "authors": ["Arnab A Purkayastha", "Jay Tharwani", "Shobhit Aggarwal"], "title": "FPGA or GPU? Analyzing comparative research for application-specific guidance", "comment": "7 pages", "summary": "The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86FPGA\u548cGPU\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u7684\u6027\u80fd\u3001\u80fd\u6548\u548c\u53ef\u7f16\u7a0b\u6027\uff0c\u65e8\u5728\u4e3a\u7279\u5b9a\u9886\u57df\u5e94\u7528\u9009\u62e9\u5408\u9002\u7684\u52a0\u901f\u5668\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u4efb\u52a1\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u4e13\u7528\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u5173\u4e8eFPGA\u548cGPU\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u4f46\u5927\u591a\u4ec5\u5173\u6ce8\u6027\u80fd\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u5404\u81ea\u6700\u9002\u5408\u5e94\u7528\u7c7b\u578b\u7684\u6df1\u5165\u5206\u6790\u3002", "challenges": "\u5982\u4f55\u7cfb\u7edf\u5730\u6bd4\u8f83FPGA\u548cGPU\u5728\u4e0d\u540c\u7c7b\u578b\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7efc\u5408\u6027\u80fd\u3001\u80fd\u6548\u548c\u7f16\u7a0b\u96be\u5ea6\u7b49\u591a\u4e2a\u7ef4\u5ea6\u505a\u51fa\u5408\u7406\u63a8\u8350\u3002", "contributions": "\u672c\u6587\u901a\u8fc7\u5206\u7c7b\u548c\u5206\u6790\u73b0\u6709\u7814\u7a76\uff0c\u603b\u7ed3\u4e86FPGA\u548cGPU\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u53ca\u6700\u4f73\u4f7f\u7528\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u9009\u62e9\u52a0\u901f\u5668\u7684\u5b9e\u9645\u5efa\u8bae\u3002", "results": "\u7814\u7a76\u53d1\u73b0FPGA\u5728\u4f4e\u529f\u8017\u3001\u9ad8\u5e76\u884c\u3001\u5b9a\u5236\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cGPU\u5728\u5927\u89c4\u6a21\u6570\u636e\u5e76\u884c\u3001\u6df1\u5ea6\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\u6027\u80fd\u66f4\u5f3a\u3002\u540c\u65f6\u6307\u51fa\u4e86\u4e24\u8005\u5728\u80fd\u6548\u548c\u7f16\u7a0b\u7075\u6d3b\u6027\u4e0a\u7684\u6743\u8861\u3002", "conclusion": "FPGA\u548cGPU\u5404\u6709\u4f18\u52bf\uff0c\u9009\u62e9\u5e94\u57fa\u4e8e\u5177\u4f53\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\uff1b\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u6bd4\u8f83\u6846\u67b6\u548c\u5b9e\u7528\u7684\u9009\u578b\u6307\u5357\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u6027\u80fd\u6307\u6807\u5bf9\u6bd4\uff0c\u7f3a\u4e4f\u5bf9\u5e94\u7528\u7c7b\u578b\u3001\u80fd\u6548\u548c\u7f16\u7a0b\u6027\u7684\u7efc\u5408\u5206\u6790\u3002\u672c\u6587\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\u4e0e\u5206\u7c7b\u603b\u7ed3\u3002"}}
{"id": "2511.06679", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06679", "abs": "https://arxiv.org/abs/2511.06679", "authors": ["Sangun Choi", "Yunho Oh"], "title": "EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations", "comment": null, "summary": "Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\\% and an average on-chip memory access count error of 2.2\\%.", "AI": {"tldr": "EONSim\u662f\u4e00\u4e2a\u65b0\u578bNPU\u6a21\u62df\u5668\uff0c\u80fd\u591f\u5168\u9762\u5efa\u6a21\u77e9\u9635\u548c\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\uff0c\u652f\u6301\u4e0b\u4e00\u4ee3NPU\u4e2d\u590d\u6742\u5d4c\u5165\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7075\u6d3b\u5185\u5b58\u67b6\u6784\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709NPU\u6a21\u62df\u5668\u4e3b\u8981\u9488\u5bf9\u5177\u6709\u786e\u5b9a\u6027\u8bbf\u5b58\u6a21\u5f0f\u7684\u77e9\u9635\u8ba1\u7b97\uff0c\u7f3a\u4e4f\u5bf9\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u4e2d\u975e\u786e\u5b9a\u6027\u3001\u6570\u636e\u4f9d\u8d56\u578b\u5185\u5b58\u8bbf\u95ee\u7684\u771f\u5b9e\u5efa\u6a21\u80fd\u529b\uff0c\u96be\u4ee5\u652f\u6301\u4e0b\u4e00\u4ee3NPU\u5bf9\u7075\u6d3b\u7247\u4e0a\u5185\u5b58\u67b6\u6784\u7684\u9700\u6c42\u3002", "challenges": "\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u5177\u6709\u8f93\u5165\u6570\u636e\u4f9d\u8d56\u548c\u975e\u786e\u5b9a\u6027\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u4f20\u7edf\u53cc\u7f13\u51b2\u7247\u4e0a\u5185\u5b58\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5efa\u6a21\uff1b\u540c\u65f6\u9700\u517c\u987e\u77e9\u9635\u8ba1\u7b97\u7684\u9ad8\u6548\u6a21\u62df\u4e0e\u5d4c\u5165\u8bbf\u95ee\u7684\u7cbe\u7ec6\u5185\u5b58\u4eff\u771f\u3002", "contributions": "\u63d0\u51faEONSim\uff0c\u9996\u4e2a\u540c\u65f6\u652f\u6301\u77e9\u9635\u548c\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u7684NPU\u6a21\u62df\u5668\uff1b\u96c6\u6210\u4e86\u7ecf\u9a8c\u8bc1\u7684\u77e9\u9635\u6027\u80fd\u6a21\u578b\u4e0e\u8be6\u7ec6\u7684\u5d4c\u5165\u5185\u5b58\u8bbf\u95ee\u6a21\u62df\uff1b\u652f\u6301\u591a\u79cd\u7247\u4e0a\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff1b\u5e76\u5728TPUv6e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u7cbe\u5ea6\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u8bef\u5dee1.4%\uff0c\u5185\u5b58\u8bbf\u95ee\u8ba1\u6570\u8bef\u5dee2.2%\u3002", "results": "EONSim\u5728TPUv6e\u4e0a\u7684\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5176\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u8bef\u5dee\u4e3a1.4%\uff0c\u5e73\u5747\u7247\u4e0a\u5185\u5b58\u8bbf\u95ee\u6b21\u6570\u8bef\u5dee\u4e3a2.2%\uff0c\u8868\u73b0\u51fa\u9ad8\u4eff\u771f\u7cbe\u5ea6\u3002", "conclusion": "EONSim\u4e3a\u4e0b\u4e00\u4ee3NPU\u67b6\u6784\u7684\u63a2\u7d22\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u7075\u6d3b\u7684\u4eff\u771f\u5e73\u53f0\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u590d\u6742\u5d4c\u5165\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u9762\u5411\u77e9\u9635\u8ba1\u7b97\u7684NPU\u6a21\u62df\u5668\uff08\u5982Gem5\u3001McSim\u3001MAESTRO\u7b49\uff09\uff0c\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u5047\u8bbe\u7b80\u5355\u7684\u53cc\u7f13\u51b2\u5185\u5b58\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5bf9\u5d4c\u5165\u64cd\u4f5c\u975e\u89c4\u5219\u8bbf\u5b58\u884c\u4e3a\u7684\u652f\u6301\u3002"}}
{"id": "2511.06824", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.06824", "abs": "https://arxiv.org/abs/2511.06824", "authors": ["Xin Yao", "Yang Liu", "Jin Jiang", "Yesen Chen", "Zhilong Chen", "Hongkang Dong", "Xiaofeng Wei", "Teng Zhang", "Dongyun Wang"], "title": "A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump", "comment": null, "summary": "Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u9ad8\u6027\u80fd\u591a\u5de5\u51b5\u8054\u5408\u5206\u6790\u6846\u67b6\uff08GMAF\uff09\uff0c\u7528\u4e8e\u5feb\u901f\u6c42\u89e3\u8f74\u5411\u67f1\u585e\u6cf5\uff08APP\uff09\u5728\u591a\u5468\u671f\u5185\u7684\u52a8\u529b\u5b66\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u529b\u573a\u548c\u6cb9\u6db2\u529b\u77e9\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63ed\u793a\u4e86\u8868\u9762\u7ec7\u6784\u5bf9\u538b\u529b\u627f\u8f7d\u80fd\u529b\u548c\u6297\u626d\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "motivation": "\u4f20\u7edfCPU\u548c\u8fed\u4ee3\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u7cbe\u7ec6\u7f51\u683c\u7684\u590d\u6742\u8868\u9762\u7ec7\u6784\u95ee\u9898\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5b9e\u73b0\u591a\u5468\u671f\u52a8\u529b\u5b66\u4eff\u771f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "challenges": "\u590d\u6742\u8868\u9762\u7ec7\u6784\u9700\u8981\u7cbe\u7ec6\u7f51\u683c\uff0c\u5bfc\u81f4\u8ba1\u7b97\u89c4\u6a21\u5927\uff1b\u591a\u5468\u671f\u4eff\u771f\u5bf9\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff1b\u4f20\u7edf\u6c42\u89e3\u5668\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u3002", "contributions": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eGPU\u7684GMAF\u6846\u67b6\uff0c\u91c7\u7528PCG\u4e0eASSOR\u9884\u6761\u4ef6\u5b50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5e76\u884c\u5ea6\u548c\u9ad8\u8ba1\u7b97\u5f3a\u5ea6\uff1b\u63d0\u51fa\u540c\u6b65\u6536\u655b\u7b56\u7565\u786e\u4fdd\u5168\u5c40\u6536\u655b\uff1b\u9996\u6b21\u9ad8\u6548\u5b9e\u73b0\u4e86\u5149\u6ed1\u4e0e\u7ec7\u6784\u8868\u9762APP\u5728\u591a\u5468\u671f\u5185\u7684\u8054\u5408\u52a8\u529b\u5b66\u5206\u6790\u3002", "results": "GMAF\u663e\u8457\u52a0\u901f\u4e86\u538b\u529b\u573a\u7684\u5efa\u7acb\u4e0e\u6c42\u89e3\u4ee5\u53ca\u6cb9\u6db2\u529b\u548c\u529b\u77e9\u7684\u6570\u503c\u79ef\u5206\uff1b\u7ed3\u679c\u663e\u793a\u8f74\u5411\u6cb9\u538b\u529b\u548c\u5468\u5411\u529b\u77e9\u76f4\u63a5\u54cd\u5e94\u8f93\u5165\u538b\u529b\uff0c\u5176\u4ed6\u5206\u91cf\u5448\u6b63\u5f26\u53d8\u5316\uff1b\u6cd5\u5411\u538b\u529b\u76f8\u5173\u7684\u529b\u548c\u529b\u77e9\u8fc5\u901f\u8fbe\u5230\u7a33\u6001\uff0c\u800c\u7c98\u6027\u526a\u5e94\u529b\u76f8\u5173\u7684\u5219\u968f\u5468\u671f\u6f14\u5316\uff1b\u7ec7\u6784\u8868\u9762\u5728\u538b\u529b\u573a\u4e2d\u5f62\u6210\u2018\u53f0\u9636\u2019\uff0c\u63d0\u5347\u4e86\u538b\u529b\u627f\u8f7d\u80fd\u529b\u548c\u6297\u626d\u6027\u80fd\u3002", "conclusion": "GMAF\u6846\u67b6\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u6a21\u62df\u8f74\u5411\u67f1\u585e\u6cf5\u5728\u591a\u5468\u671f\u5185\u7684\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5305\u542b\u8868\u9762\u7ec7\u6784\u7684\u590d\u6742\u5de5\u51b5\uff0c\u4e3aAPP\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u4eff\u771f\u5de5\u5177\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u591a\u91c7\u7528\u4f20\u7edfCPU\u8fed\u4ee3\u65b9\u6cd5\u8fdb\u884cAPP\u52a8\u529b\u5b66\u4eff\u771f\uff0c\u53d7\u9650\u4e8e\u8ba1\u7b97\u6548\u7387\uff0c\u96be\u4ee5\u5904\u7406\u7cbe\u7ec6\u7f51\u683c\u548c\u591a\u5468\u671f\u95ee\u9898\uff1b\u90e8\u5206\u5de5\u4f5c\u4f7f\u7528GPU\u52a0\u901f\u5355\u7269\u7406\u573a\u6c42\u89e3\uff0c\u4f46\u7f3a\u4e4f\u591a\u5de5\u51b5\u8054\u5408\u5206\u6790\u6846\u67b6\u4e0e\u5168\u5c40\u6536\u655b\u7b56\u7565\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2511.06736", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2511.06736", "abs": "https://arxiv.org/abs/2511.06736", "authors": ["Arsalan Ali Malik", "John Buchanan", "Aydin Aysu"], "title": "Preemption-Enhanced Benchmark Suite for FPGAs", "comment": "13 Pages, 4 Figures, 4 Tables", "summary": "Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.\n  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u7684\u3001\u652f\u6301\u62a2\u5360\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30FPGA\u62a2\u5360\u7b56\u7565\u548c\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5305\u542b27\u4e2a\u591a\u6837\u5316\u5e94\u7528\uff0c\u5e76\u96c6\u6210\u4e0a\u4e0b\u6587\u4fdd\u5b58\u4e0e\u6062\u590d\u673a\u5236\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u548c\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709FPGA\u4efb\u52a1\u8c03\u5ea6\u4e0e\u62a2\u5360\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u3001\u516c\u5f00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u591a\u4f9d\u8d56\u4e13\u6709\u6216\u5408\u6210\u8d1f\u8f7d\uff0c\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u590d\u73b0\u548c\u6bd4\u8f83\uff0c\u963b\u788d\u4e86\u591a\u79df\u6237FPGA\u73af\u5883\u4e0b\u7684\u6709\u6548\u8bc4\u4f30\u3002", "challenges": "\u8bbe\u8ba1\u4e00\u4e2a\u652f\u6301\u62a2\u5360\u3001\u6db5\u76d6\u591a\u79cd\u5e94\u7528\u573a\u666f\u3001\u5177\u5907\u6807\u51c6\u5316\u4e0a\u4e0b\u6587\u7ba1\u7406\u673a\u5236\u4e14\u6613\u4e8e\u6269\u5c55\u7684\u5f00\u6e90\u57fa\u51c6\u5957\u4ef6\u5177\u6709\u6311\u6218\u6027\uff1b\u540c\u65f6\u9700\u786e\u4fdd\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4ee3\u8868\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u3002", "contributions": "1\uff09\u9996\u4e2a\u5f00\u6e90\u7684\u3001\u652f\u6301\u62a2\u5360\u7684FPGA\u57fa\u51c6\u5957\u4ef6\uff0c\u542b27\u4e2a\u591a\u6837\u5316\u5e94\u7528\uff1b2\uff09\u96c6\u6210\u4e0a\u4e0b\u6587\u4fdd\u5b58\u4e0e\u6062\u590d\u673a\u5236\uff1b3\uff09\u63d0\u4f9b\u6dfb\u52a0\u65b0\u57fa\u51c6\u7684\u6307\u5357\uff1b4\uff09\u4fc3\u8fdbFPGA\u8c03\u5ea6\u7b56\u7565\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u3002", "results": "\u8be5\u5957\u4ef6\u80fd\u591f\u6709\u6548\u652f\u6301\u591a\u79cdFPGA\u5e94\u7528\u573a\u666f\u4e0b\u7684\u62a2\u5360\u4e0e\u8c03\u5ea6\u6d4b\u8bd5\uff0c\u7b80\u5316\u4e86\u65b0\u8c03\u5ea6\u7b97\u6cd5\u7684\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u5e76\u4e3a\u591a\u79df\u6237\u73af\u5883\u4e0b\u8c03\u5ea6\u516c\u5e73\u6027\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u6027\u80fd\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002", "conclusion": "\u672c\u57fa\u51c6\u5957\u4ef6\u586b\u8865\u4e86FPGA\u62a2\u5360\u4e0e\u8c03\u5ea6\u7814\u7a76\u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u4e3a\u672a\u6765FPGA\u64cd\u4f5c\u7cfb\u7edf\u548c\u8c03\u5ea6\u7b56\u7565\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u3002", "related_work": "\u5df2\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8eFPGA\u8c03\u5ea6\u6216\u62a2\u5360\u673a\u5236\u8bbe\u8ba1\uff0c\u4f46\u666e\u904d\u4f7f\u7528\u79c1\u6709\u6216\u5408\u6210\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u516c\u5f00\u3001\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5bfc\u81f4\u8de8\u7814\u7a76\u6bd4\u8f83\u56f0\u96be\u3002"}}
{"id": "2511.07229", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07229", "abs": "https://arxiv.org/abs/2511.07229", "authors": ["Jaehong Cho", "Hyunmin Choi", "Jongse Park"], "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure", "comment": "4 pages, 3 figures", "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.", "AI": {"tldr": "LLMServingSim2.0\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f02\u6784\u786c\u4ef6\u63a2\u7d22\u7684\u7cfb\u7edf\u7ea7\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8f68\u8ff9\u7684\u6027\u80fd\u5efa\u6a21\u548c\u7b97\u5b50\u7ea7\u5ef6\u8fdf\u5206\u6790\u5668\uff0c\u663e\u8457\u7b80\u5316\u4e86\u65b0\u52a0\u901f\u5668\u7684\u96c6\u6210\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u73b0\u4ee3LLM\u670d\u52a1\u6280\u672f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728TPU\u6848\u4f8b\u4e2d\u4ee3\u7801\u91cf\u51cf\u5c1118.5\u500d\uff0cGPU\u6a21\u62df\u8bef\u5dee\u4ec5\u4e3a1.9%\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u6a21\u62df\u5668\u5728\u786c\u4ef6\u6a21\u578b\u96c6\u6210\u65b9\u9762\u7f3a\u4e4f\u6e05\u6670\u7684\u62bd\u8c61\uff0c\u4e14\u4ec5\u652f\u6301\u6709\u9650\u7684\u670d\u52a1\u6280\u672f\uff0c\u96be\u4ee5\u8986\u76d6\u73b0\u4ee3LLM\u670d\u52a1\u7684\u591a\u6837\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u7075\u6d3b\u3001\u6613\u6269\u5c55\u7684\u6a21\u62df\u5e73\u53f0\u3002", "challenges": "1) \u786c\u4ef6\u6a21\u578b\u96be\u4ee5\u96c6\u6210\u5230\u7cfb\u7edf\u7ea7\u6a21\u62df\u5668\u4e2d\uff0c\u7f3a\u4e4f\u7edf\u4e00\u62bd\u8c61\uff1b2) \u73b0\u6709\u6a21\u62df\u5668\u652f\u6301\u7684\u670d\u52a1\u6280\u672f\u8303\u56f4\u72ed\u7a84\uff0c\u65e0\u6cd5\u53cd\u6620\u73b0\u4ee3LLM\u670d\u52a1\u7684\u5168\u8c8c\u3002", "contributions": "1) \u63d0\u51faLLMServingSim2.0\uff0c\u91c7\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684\u6027\u80fd\u5efa\u6a21\u4e0e\u7b97\u5b50\u7ea7\u5ef6\u8fdf\u5206\u6790\u5668\uff0c\u5b9e\u73b0\u4e00\u952e\u5f0f\u52a0\u901f\u5668\u96c6\u6210\uff1b2) \u96c6\u6210\u6700\u65b0\u7684LLM\u670d\u52a1\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u8bf7\u6c42\u8def\u7531\u3001\u7f13\u5b58\u7ba1\u7406\u548c\u8c03\u5ea6\u7b56\u7565\u7684\u7075\u6d3b\u63a5\u53e3\uff1b3) \u5728TPU\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u4f4e\u4ee3\u7801\u91cf\u548c\u9ad8\u6027\u80fd\u7684\u786c\u4ef6\u6269\u5c55\u80fd\u529b\u3002", "results": "1) \u5728TPU\u6848\u4f8b\u4e2d\uff0c\u65b0\u5206\u6790\u5668\u76f8\u6bd4\u524d\u4ee3\u51cf\u5c1118.5\u500d\u4ee3\u7801\u91cf\uff0c\u786c\u4ef6\u96c6\u6210\u66f4\u9ad8\u6548\uff1b2) \u5bf9GPU\u4e0aLLM\u670d\u52a1\u7684\u6a21\u62df\u8bef\u5dee\u4ec5\u4e3a1.9%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u7684\u6a21\u62df\u65f6\u95f4\u3002", "conclusion": "LLMServingSim2.0\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7cbe\u786e\u4e14\u6613\u4e8e\u6269\u5c55\u7684LLM\u670d\u52a1\u7cfb\u7edf\u6a21\u62df\u5e73\u53f0\uff0c\u80fd\u591f\u652f\u6301\u5e7f\u6cdb\u7684\u786c\u4ef6\u548c\u7cfb\u7edf\u6280\u672f\u63a2\u7d22\uff0c\u9002\u7528\u4e8e\u786c\u4ef6\u5f00\u53d1\u8005\u4e0eLLM\u670d\u52a1\u63d0\u4f9b\u5546\u3002", "related_work": "\u672c\u6587\u7684\u524d\u8eabLLMServingSim\u662f\u65e9\u671f\u9762\u5411LLM\u670d\u52a1\u7684\u6a21\u62df\u5668\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5f02\u6784\u786c\u4ef6\u7684\u826f\u597d\u652f\u6301\u548c\u7075\u6d3b\u7684\u7cfb\u7edf\u7ec4\u4ef6\u5efa\u6a21\u80fd\u529b\uff0c\u5176\u4ed6\u76f8\u5173\u5de5\u4f5c\u591a\u96c6\u4e2d\u4e8e\u7279\u5b9a\u786c\u4ef6\u6216\u7279\u5b9a\u670d\u52a1\u7b56\u7565\u7684\u6a21\u62df\uff0c\u7f3a\u4e4f\u7efc\u5408\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.06838", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06838", "abs": "https://arxiv.org/abs/2511.06838", "authors": ["Yuzong Chen", "Chao Fang", "Xilai Dai", "Yuheng Wu", "Thierry Tambe", "Marian Verhelst", "Mohamed S. Abdelfattah"], "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats", "comment": "Preprint. Under review", "summary": "The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86P3-LLM\uff0c\u4e00\u79cd\u7ed3\u5408NPU\u4e0ePIM\u7684\u6df7\u5408\u7cbe\u5ea6LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u6df7\u5408\u6570\u503c\u683c\u5f0f\u91cf\u5316\u3001\u8f7b\u91cf\u7ea7PIM\u67b6\u6784\u8bbe\u8ba1\u548c\u7b97\u5b50\u878d\u5408\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u541e\u5410\u91cf\u548c\u80fd\u6548\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9762\u4e34\u5de8\u5927\u7684\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u73b0\u6709\u57fa\u4e8e\u9ad8\u7cbe\u5ea6\uff08\u5982FP16\uff09\u7684PIM\u65b9\u6848\u5728DRAM\u4e2d\u5e26\u6765\u663e\u8457\u7684\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002", "challenges": "\u5982\u4f55\u5728\u4fdd\u8bc1LLM\u63a8\u7406\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4ePIM\u7cfb\u7edf\u7684\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\uff0c\u5e76\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\uff1b\u540c\u65f6\u6709\u6548\u7ba1\u7406\u4f4e\u7cbe\u5ea6\u6570\u636e\u6d41\u548c\u51cf\u5c11\u8fd0\u884c\u65f6\u53cd\u91cf\u5316\u5f00\u9500\u3002", "contributions": "1\uff09\u63d0\u51fa\u4e00\u79cd\u7075\u6d3b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff0c\u4f7f\u7528\u6df7\u5408\u6570\u503c\u683c\u5f0f\u5bf9LLM\u4e0d\u540c\u64cd\u4f5c\u6570\u8fdb\u884c\u9ad8\u6548\u538b\u7f29\uff1b2\uff09\u8bbe\u8ba1\u652f\u6301\u8be5\u683c\u5f0f\u7684\u8f7b\u91cf\u7ea7PIM\u52a0\u901f\u5668\u67b6\u6784\uff0c\u5728\u76f8\u540c\u9762\u79ef\u4e0b\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\uff1b3\uff09\u901a\u8fc7\u7b97\u5b50\u878d\u5408\u4f18\u5316\u4f4e\u7cbe\u5ea6\u6570\u636e\u6d41\uff0c\u51cf\u5c11\u53cd\u91cf\u5316\u5f00\u9500\u3002", "results": "\u5728\u591a\u4e2a\u4ee3\u8868\u6027LLM\u548c\u4efb\u52a1\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cP3-LLM\u5728KV\u7f13\u5b58\u91cf\u5316\u548c\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5e76\u76f8\u6bd4HBM-PIM\u3001Ecco\u548cPimba\u5e73\u5747\u5b9e\u73b04.9\u500d\u30012.0\u500d\u548c3.4\u500d\u7684\u52a0\u901f\u3002", "conclusion": "P3-LLM\u901a\u8fc7\u6df7\u5408\u6570\u503c\u683c\u5f0f\u91cf\u5316\u4e0ePIM\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5e26\u5bbd\u4e0e\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u52a0\u901f\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "related_work": "\u5df2\u6709\u5de5\u4f5c\u63a2\u7d22\u4e86NPU\u4e0eDRAM\u57faPIM\u7684\u5f02\u6784\u7cfb\u7edf\u7528\u4e8eLLM\u52a0\u901f\uff0c\u5982HBM-PIM\u3001Ecco\u548cPimba\uff0c\u4f46\u5176\u9ad8\u7cbe\u5ea6PIM\u5355\u5143\u5b58\u5728\u9762\u79ef\u548c\u529f\u8017\u74f6\u9888\u3002"}}
{"id": "2511.06907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06907", "abs": "https://arxiv.org/abs/2511.06907", "authors": ["Ilias Papalamprou", "Dimosthenis Masouros", "Ioannis Loudaros", "Francky Catthoor", "Dimitrios Soudris"], "title": "Optimizing GEMM for Energy and Performance on Versal ACAP Architectures", "comment": null, "summary": "General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9AMD Versal ACAP\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6307\u5bfc\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u4f18\u5316GEMM\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u7684\u6620\u5c04\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "motivation": "GEMM\u662f\u8bb8\u591a\u79d1\u5b66\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\uff0c\u4f46\u5728\u8d44\u6e90\u548c\u529f\u8017\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5e38\u6210\u4e3a\u6027\u80fd\u548c\u80fd\u6548\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u5728Versal ACAP\u4e0a\u5bf9GEMM\u7684\u6620\u5c04\u7f3a\u4e4f\u5bf9\u80fd\u6548\u4e0e\u6027\u80fd\u6743\u8861\u7684\u5145\u5206\u8003\u8651\u3002", "challenges": "\u5728Versal ACAP\u7684\u5f02\u6784\u7ec4\u4ef6\uff08AIE\u3001PL\u3001PS\uff09\u4e0a\u9ad8\u6548\u6620\u5c04GEMM\u5177\u6709\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5728\u517c\u987e\u6027\u80fd\u4e0e\u80fd\u91cf\u6548\u7387\u65b9\u9762\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u548c\u4f18\u5316\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ea66000\u6b21\u677f\u4e0a\u5b9e\u9a8c\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9GEMM\u6620\u5c04\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u652f\u6301\u4ee5\u6027\u80fd\u6216\u80fd\u6548\u4e3a\u76ee\u6807\u7684\u4f18\u5316\u3002", "results": "\u5728Versal VCK190\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\uff0c\u541e\u5410\u91cf\u51e0\u4f55\u5e73\u5747\u63d0\u53471.23\u500d\uff08\u6700\u9ad8\u8fbe2.5\u500d\uff09\uff0c\u80fd\u6548\u63d0\u53471.25\u500d\uff08\u6700\u9ad8\u8fbe2.7\u500d\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347GEMM\u5728Versal ACAP\u4e0a\u7684\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u5f02\u6784\u7cfb\u7edf\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4e2d\u7684\u4f18\u52bf\u3002", "related_work": "\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u6790\u6027\u5efa\u6a21\u6216\u7279\u5b9a\u786c\u4ef6\u7ec4\u4ef6\u4e0a\u7684GEMM\u4f18\u5316\uff0c\u8f83\u5c11\u8003\u8651Versal ACAP\u5f02\u6784\u67b6\u6784\u4e0b\u7684\u80fd\u91cf-\u6027\u80fd\u6743\u8861\uff0c\u4e14\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u6d4b\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6620\u5c04\u65b9\u6cd5\u3002"}}
