<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ是一种算法-硬件协同设计的推测解码方法，利用完整模型的部分权重位构建量化草稿模型，无需额外训练或存储开销，在15个大语言模型和任务上实现了最高2.07倍的加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但推理延迟高；现有量化方法常导致性能下降，而传统推测解码虽无损但带来额外开销。

Challenges: 如何在不增加训练或存储开销的前提下，实现高效且无损的推理加速。

Contributions: 提出SPEQ，首次实现基于部分权重位的量化草稿模型，结合可重构处理单元阵列，完成算法与硬件的协同优化。

Results: 在15个大语言模型和任务上，SPEQ相比FP16、Olive和Tender分别实现2.07x、1.53x和1.45x的加速。

Conclusion: SPEQ通过算法-硬件协同设计有效提升了大模型推理效率，兼顾速度与精度，具有广泛适用性。

Related Work: 与量化方法、推测解码技术以及硬件加速器相关的研究工作。

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 本文提出了一种将大规模大气和海洋模型从PyTorch迁移至MindSpore并针对国产芯片进行优化的框架，实现了在保持模型精度的同时提升运行效率，降低对GPU的依赖，推动了我国在气象和气候AI模型中的技术自主性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在气候与天气研究中的作用日益增强，对高效模型训练与推理的需求不断上升。然而，现有模型严重依赖GPU，限制了硬件独立性，尤其是在国产硬件和框架上的应用。因此，亟需一种支持国产芯片的迁移与优化方案。

Challenges: 主要挑战包括跨框架模型迁移的兼容性问题、在国产芯片上的高效并行计算实现、内存优化以及在不损失精度的前提下提升训练与推理速度。

Contributions: 提出了一套完整的从PyTorch到MindSpore的模型迁移与优化框架，重点解决软件与硬件适配、内存优化和并行计算问题；系统评估了在国产芯片上的训练速度、推理速度、精度和能效，并验证了其作为GPU替代方案的可行性。

Results: 实验结果表明，迁移后的模型在国产芯片上保持了原始精度，同时显著降低了系统依赖性，在训练与推理效率方面均有提升，且展现出更高的能源效率。

Conclusion: 该框架为利用国产芯片和框架开展大气与海洋AI模型研究提供了可行路径，有助于提升我国在科学计算领域的技术自主能力。

Related Work: 相关工作包括FourCastNet和AI-GOMS等基于GPU的AI气象模型，以及PyTorch等主流深度学习框架在气候建模中的应用。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [3] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出了一种端到端的并行性能分析框架，用于高效处理大规模GPU跟踪数据，通过并发处理和因果图方法显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU跟踪数据在异构高性能计算架构中识别性能瓶颈至关重要，但单个跟踪数据的体积和复杂性使得性能分析计算成本高且耗时。

Challenges: 如何高效处理大规模、复杂的GPU跟踪数据，并在多执行流中揭示性能差异和依赖关系。

Contributions: 设计了一个端到端的并行性能分析框架，支持对多个大规模GPU跟踪数据进行并发分区处理，并引入因果图方法和并行协调图来暴露性能变异性与依赖关系。

Results: 实验结果表明，该框架在可扩展性方面提升了67%，验证了其独立分析多个跟踪数据的有效性。

Conclusion: 所提出的并行分析框架能够有效提升大规模GPU跟踪数据分析的效率和可扩展性，适用于异构HPC环境中的性能诊断。

Related Work: 现有工作主要集中在单一轨迹分析或串行处理方法，缺乏对多大规模GPU轨迹的并行化支持和系统级性能依赖建模。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [4] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: 本文提出了一种面向边缘计算场景的新型调度方案SLICE，能够有效满足大语言模型服务中差异化的服务等级目标（SLO）需求，显著提升SLO达成率并缩短任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的大语言模型应用对延迟极为敏感，具有多样化的SLO需求（如TTFT、TPOT和端到端延迟），但现有调度系统仅以最大化吞吐量为目标，难以满足这些差异化需求，导致SLO违规率高。

Challenges: 如何在资源受限的边缘环境中，针对不同任务的SLO要求动态调整生成速率并合理调度请求，同时兼顾系统效率与服务质量，是一个关键挑战。

Contributions: 提出了SLICE调度框架，结合效用最大化的请求调度算法与生成速率的动态迭代控制机制，首次在边缘LLM服务中实现对多样化SLO的精细化支持。

Results: 实验表明，与Orca和FastServe相比，SLICE的SLO达成率最高提升35倍，任务完成时间缩短3.4倍。

Conclusion: SLICE通过联合优化请求调度与生成速率控制，在边缘LLM服务中显著提升了对多样化SLO的满足能力，为面向实时交互的边缘智能应用提供了高效可靠的推理调度解决方案。

Related Work: 现有工作如Orca和FastServe主要关注提高输出令牌吞吐量，缺乏对差异化SLO的支持，未能充分考虑边缘场景下实时任务的严格延迟约束。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [5] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake是一种面向多智能体应用的KV缓存优化框架，通过动态内存划分和预测性数据卸载机制，显著降低延迟并提升GPU内存利用率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体大语言模型应用中，外部函数调用导致KV缓存面临空间争用和时间利用不足的问题，影响整体性能。

Challenges: 1）多个智能体共享KV缓存时产生空间竞争，关键智能体的缓存可能被错误驱逐；2）智能体在等待长时工具调用期间，其KV缓存仍占用GPU内存，造成资源闲置。

Contributions: 提出Tokencake，一种以KV缓存为中心的服务框架：1）设计空间调度器，采用动态内存分区保护关键智能体；2）设计时间调度器，通过主动卸载和预测加载机制，在工具调用停滞期间重新利用GPU内存。

Results: 在典型多智能体基准测试中，Tokencake相比vLLM可将端到端延迟降低47.06%，有效GPU内存利用率提升达16.9%。

Conclusion: Tokencake通过联合优化调度与内存管理，显著提升了多智能体LLM应用的性能和资源效率，验证了KV缓存中心化设计的重要性。

Related Work: 相关工作包括vLLM等KV缓存管理方案，但它们未针对多智能体场景下的空间争用和时间浪费问题进行专门优化。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [6] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 本文提出了高效的分布式交互式证明协议，用于图的平面性验证，具有较少的通信轮数和较小的证明大小。


<details>
  <summary>Details</summary>
Motivation: 为了减少分布式系统中验证图属性（如平面性）所需的通信开销，设计更高效的分布式交互式证明（DIP）协议。

Challenges: 如何在保证正确性和完整性的前提下，最小化证明过程中的通信轮数和每轮的证明大小，尤其是在大规模图中实现高效验证。

Contributions: 提出了一个O(log*n)轮的DIP协议，用于嵌入式平面性和一般平面性验证，分别具有O(1)和O(⌈logΔ/log*n⌉)的证明大小；并进一步推广到对于任意1≤r≤log*n，存在O(r)轮协议，具有更细粒度的通信效率权衡。

Results: 实现了O(log*n)轮内完成平面性验证，证明大小仅为常数或对数级别，显著优于之前的方案；并通过参数化轮数r提供了灵活的效率折中方案。

Conclusion: 本文展示了通过合理设计交互轮数，可以在分布式环境中以极低的通信成本验证图的平面性，为分布式证明系统提供了新的高效构造方法。

Related Work: 基于Kol、Oshman和Saxena在PODC 2018年提出的分布式交互式证明（DIP）框架，本文在其基础上优化了通信复杂度。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [7] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 本文探讨了在大型软件系统中频繁进行性能基准测试以检测性能退步的重要性，并指出现有基准优化技术缺乏在实际CI/CD流水线中的集成。文章提出了三大挑战：基准优化策略的可组合性、结果的自动评估以及在实践中应用的可用性与复杂性，并提出一个概念性的云基准框架来应对这些挑战。


<details>
  <summary>Details</summary>
Motivation: 性能退步会导致资源浪费，影响服务级别协议（SLA），因此需要在CI/CD流程中频繁执行性能基准测试。然而，传统基准测试耗时且资源消耗大，限制了其在持续集成中的应用。

Challenges: （a）不同基准优化策略之间的可组合性差；（b）缺乏对基准测试结果的自动化评估机制；（c）现有工具在CI/CD系统中使用时的可用性低和操作复杂。

Contributions: 本文提出了在CI/CD中实现高效基准测试的三个核心挑战，并引入了一个概念性的云基准框架，旨在透明地解决这些挑战，推动该领域的进一步研究。

Results: 本文为未来研究提供了明确的方向，但尚未实现具体系统或实验结果，属于愿景论文。

Conclusion: 要实现高效、频繁的性能回归检测，必须解决基准优化策略的可组合性、结果自动化评估和实际可用性问题，云原生基准框架是未来发展的关键方向。

Related Work: 已有研究提出了多种加速基准执行的优化技术，如采样、跳过无关测试、并行执行等，但缺乏将这些技术整合到真实CI/CD环境中的系统性解决方案。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [8] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: 本文提出了PCMS，一种用于在超级计算机上耦合模拟代码的GPU加速通用耦合框架，支持最多五维的分布式控制和场映射方法，并在Frontier的2,080个GPU上实现了85%的弱扩展效率。


<details>
  <summary>Details</summary>
Motivation: 为了在领导级超级计算机上实现高效、灵活的多模型耦合模拟，尤其是在处理复杂物理场和多维数据时，需要一种能够充分利用GPU加速能力的通用耦合框架。

Challenges: 开发一种能够在大规模GPU集群上高效运行的通用耦合框架，同时支持多维场映射和物理约束的处理，是一个巨大的挑战。

Contributions: 提出了PCMS框架，支持分布式控制和多维场映射，能够利用离散化和场信息来满足物理约束，并在实际应用中展示了其高效性和可扩展性。

Results: PCMS在Frontier的2,080个GPU上实现了85%的弱扩展效率，并成功应用于XGC与DEGAS2的耦合以及GNET与GTC的5D分布函数耦合。

Conclusion: PCMS是一种高效、可扩展的GPU加速耦合框架，能够有效支持多模型模拟，特别是在处理复杂物理场和多维数据时表现出色。

Related Work: 现有的耦合框架通常局限于特定领域或维度，缺乏通用性和高效的GPU加速支持，而PCMS通过引入分布式控制和多维场映射方法，填补了这一空白。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>
