<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 11]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS](https://arxiv.org/abs/2511.05502)
*Varun Rajesh,Om Jodhpurkar,Pooja Anbuselvan,Mantinder Singh,Ashok Jallepali,Shantanu Godbole,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AR

TL;DR: 对五种在Apple Silicon上运行的本地大语言模型（LLM）推理框架（MLX、MLC-LLM、llama.cpp、Ollama、PyTorch MPS）进行了系统性实证评估，使用Qwen-2.5模型族在长上下文和不同负载下测试性能指标。结果显示各框架在吞吐量、首 token 时间、部署便捷性等方面各有优劣，均支持完全离线运行，具备强隐私保障，虽性能仍落后于NVIDIA GPU方案，但已逐步成为可行的生产级本地推理选择。


<details>
  <summary>Details</summary>
Motivation: 随着本地大模型部署需求增长，尤其是在注重隐私和数据安全的场景下，Apple Silicon设备因其强大的统一内存架构和隐私保护能力成为理想平台。然而目前缺乏对不同LLM运行时在苹果生态中性能表现的系统性比较，开发者难以做出技术选型决策。因此，本文旨在填补这一空白，提供基于实证的性能分析与推荐指南。

Challenges: 1) 不同LLM运行时在Apple Silicon上的性能表现差异大，优化策略不透明；2) 长上下文（长达10万token）处理对内存管理和KV缓存效率提出挑战；3) 量化支持、流式输出、批处理与并发能力在各框架间支持不一；4) 在保持高性能的同时实现低延迟（如TTFT）存在权衡；5) PyTorch等传统框架在MPS后端存在内存瓶颈。

Contributions: 1) 对五种主流本地LLM运行时（MLX、MLC-LLM、llama.cpp、Ollama、PyTorch MPS）在真实硬件（M2 Ultra, 192GB）上进行了全面、可复现的性能评测；2) 覆盖了包括TTFT、吞吐量、延迟分布、长上下文行为、量化、流式、批处理等多维度指标；3) 公开了所有实验脚本、日志和图表，提升研究透明度与可复现性；4) 提供了针对交互式应用和长上下文任务的具体部署建议；5) 推动了Apple Silicon作为私有化LLM推理平台的认知与发展。

Results: 1) MLX在持续生成吞吐量方面表现最佳；2) MLC-LLM在中等长度提示下首token时间（TTFT）最低，并具备较强的开箱即用功能；3) llama.cpp在单流轻量级场景中效率高、资源占用少；4) Ollama开发体验友好但性能（吞吐与TTFT）相对落后；5) PyTorch MPS受限于大模型和长上下文下的内存瓶颈；6) 所有框架均支持全设备内运行，无数据外泄风险；7) 尽管整体性能仍低于NVIDIA平台上的vLLM等系统，但Apple Silicon方案已接近实用化水平。

Conclusion: 在Apple Silicon平台上，不同LLM运行时各有优势：MLX适合高吞吐生成任务，MLC-LLM适合低延迟交互场景，llama.cpp适合资源受限的轻量应用，Ollama适合快速原型开发，而PyTorch MPS尚需改进内存管理。尽管当前性能尚未超越高端GPU方案，但这些框架已展现出作为私有、安全、高效本地推理平台的巨大潜力，正快速走向成熟。

Related Work: 相关工作主要包括vLLM、TensorRT-LLM等GPU加速推理框架，以及Hugging Face TGI、Llama.cpp等开源推理系统。相比之下，本研究聚焦于Apple Silicon平台特有的运行时环境，强调完全离线执行与隐私保护，并对比的是专为苹果芯片优化的轻量级框架，而非依赖CUDA的高性能服务端方案。此外，MLC-LLM和MLX本身也源自学术项目，体现了编译优化与系统级设计的研究进展。

Abstract: We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.
  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.
  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.

</details>


### [2] [YAP+: Pad-Layout-Aware Yield Modeling and Simulation for Hybrid Bonding](https://arxiv.org/abs/2511.05506)
*Zhichao Chen,Puneet Gupta*

Main category: cs.AR

TL;DR: YAP+是一个专为晶圆对晶圆（W2W）和芯片对晶圆（D2W）混合键合工艺设计的良率建模框架，能够综合考虑多种影响良率的失效机制，并支持基于焊盘布局的良率分析，结合开源仿真器实现高精度与超快运行速度，适用于封装技术与系统设计的协同优化。


<details>
  <summary>Details</summary>
Motivation: 随着3D集成技术的发展，Cu-Cu混合键合成为实现高密度互连的关键技术，但其良率受多种工艺缺陷影响，亟需精准高效的良率预测模型以支持设计与制造的协同优化。

Challenges: 混合键合工艺中存在多种影响良率的因素，如对准误差、颗粒污染、铜凹陷、表面粗糙度和焊盘密度等，且不同布局模式下的关键、冗余与虚拟焊盘相互作用复杂，传统仿真方法计算成本高、效率低。

Contributions: 提出了YAP+良率建模框架，首次全面整合多种实际工艺失效机制；支持任意二维布局的焊盘感知分析；开发了开源良率仿真器，并实现比传统仿真快1000倍以上的近解析级模型。

Results: YAP+模型在精度上与仿真器高度一致，同时运行速度提升超过1000倍；通过案例研究揭示了焊盘布局、键合间距、不同类型焊盘比例及冗余焊盘布置策略对整体良率的影响规律。

Conclusion: YAP+为W2W和D2W混合键合提供了高效准确的良率预测工具，可用于指导封装工艺优化、组装设计规则制定和系统级设计决策，推动芯粒集成技术的发展。

Related Work: 此前的良率模型多集中于单一缺陷源或简化假设，缺乏对真实布局中多种焊盘类型（关键、冗余、虚拟）的综合建模能力，而YAP+在此基础上实现了更精细、更贴近实际工艺的建模与快速分析。

Abstract: Three-dimensional (3D) integration continues to advance Moore's Law by facilitating dense interconnects and enabling multi-tier system architectures. Among the various integration approaches, Cu-Cu hybrid bonding has emerged as a leading solution for achieving high interconnect density in chiplet integration. In this work, we present YAP+, a yield modeling framework specifically tailored for wafer-to-wafer (W2W) and die-to-wafer (D2W) hybrid bonding processes. YAP+ incorporates a comprehensive set of yield-impacting failure mechanisms, including overlay misalignment, particle defects, Cu recess variations, surface roughness, and Cu pad density. Furthermore, YAP+ supports pad layout-aware yield analysis, considering critical, redundant, and dummy pads across arbitrary 2D physical layout patterns. To support practical evaluation, we developed an open-source yield simulator, demonstrating that our near-analytical model matches simulation accuracy while achieving over 1,000x speedup in runtime. This performance makes YAP+ a valuable tool for co-optimizing packaging technologies, assembly design rules, and system-level design strategies. Beyond W2W-D2W comparisons, we leverage YAP+ to investigate the impact of pad layout patterns, bonding pitch, and pad ratios across different pad types, and explore the benefits of strategically placing redundant pad replicas.

</details>


### [3] [Delay Time Characterization on FPGA: A Low Nonlinearity, Picosecond Resolution Time-to-Digital Converter on 16-nm FPGA using Bin Sequence Calibration](https://arxiv.org/abs/2511.05583)
*Sunwoo Park,Byungkwon Park,Eunsung Kim,Jiwon Yune,Seungho Han,Seunggo Nam*

Main category: cs.AR

TL;DR: 本文提出了一种在16 nm Xilinx UltraScale Plus FPGA上实现的高分辨率时间数字转换器（TDC），通过两种新颖的硬件无关后处理技术——部分序重建（POR）和迭代时间交织（ITI）——显著提升了性能，实现了1.15 ps的分辨率和3.38 ps的均方根精度。


<details>
  <summary>Details</summary>
Motivation: 为了在FPGA上实现更高精度和分辨率的时间测量，同时降低对专用硬件的依赖和资源开销。

Challenges: FPGA上的TDC面临时间分辨有限、非线性误差（如DNL和INL）以及缺失码问题等挑战，限制了其在高精度应用中的使用。

Contributions: 提出了两种新的后处理方法：部分序重建（POR）解决缺失码问题；迭代时间交织（ITI）通过合并多个校准延迟链提升时间分辨率。两种方法均不依赖特定硬件，具有良好的可移植性和扩展性。

Results: 实现了1.15 ps的分辨率、3.38 ps的RMS精度，DNL为[-0.43, 0.24] LSB，INL为[-2.67, 0.15] LSB，性能优于或媲美当前最先进的FPGA-TDC设计，且硬件开销更低。

Conclusion: POR和ITI技术有效提升了FPGA上TDC的时间分辨率和线性度，为可编程逻辑平台中的高精度时间测量和延迟校准提供了通用、高效的解决方案。

Related Work: 现有工作多依赖于复杂的硬件结构或平均技术来提升分辨率，如抽头延迟线（TDL）阵列和时间交织架构，但通常伴随高资源消耗和非理想性问题。

Abstract: We present a Time-to-Digital Converter (TDC) implemented on a 16 nm Xilinx UltraScale Plus FPGA that achieves a resolution of 1.15 ps, RMS precision of 3.38 ps, a differential nonlinearity (DNL) of [-0.43, 0.24] LSB, and an integral nonlinearity (INL) of [-2.67, 0.15] LSB. This work introduces two novel hardware-independent post-processing techniques - Partial Order Reconstruction (POR) and Iterative Time-bin Interleaving (ITI) - that significantly enhance the performance of FPGA-based TDCs. POR addresses the missing code problem by inferring the partial order of each time bin through code density test data and directed acyclic graph (DAG) analysis, enabling near-complete recovery of usable bins. ITI further improves fine time resolution by merging multiple calibrated tapped delay lines (TDLs) into a single unified delay chain, achieving scalable resolution without resorting to averaging. Compared to state-of-the-art FPGA-based TDC architectures, the proposed methods deliver competitive or superior performance with reduced hardware overhead. These techniques are broadly applicable to high-resolution time measurement and precise delay calibration in programmable logic platforms.

</details>


### [4] [LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs](https://arxiv.org/abs/2511.06174)
*Zifan He,Shengyu Ye,Rui Ma,Yang Wang,Jason Cong*

Main category: cs.AR

TL;DR: LUT-LLM是首个基于FPGA的大型语言模型加速器，通过向量量化和内存查找将LLM推理从算术密集型转变为内存密集型，在1B以上模型上实现了显著的延迟和能效优势。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型发展迅速，但单批次推理效率对于端侧智能至关重要。FPGA虽具能效优势，但在算术计算上已被GPU优化缩小差距，因此需探索新的计算范式以发挥其内存优势。

Challenges: 如何在FPGA上高效运行1B以上的大语言模型；如何减少内存带宽瓶颈；如何在量化后保持模型精度；如何设计高效的查找表架构以支持大规模向量检索。

Contributions: 提出了LUT-LLM，首个基于查找表的FPGA大模型加速器；提出激活值与权重联合量化方案；设计了带宽感知的并行质心搜索、高效的二维查找表和时空混合架构。

Results: 在AMD V80 FPGA上实现Qwen 3 1.7B模型，相比AMD MI210降低1.66倍延迟，相比NVIDIA A100提升1.72倍能效；可扩展至32B模型，能效比A100高2.16倍。

Conclusion: LUT-LLM通过将LLM推理转为内存查找操作，充分发挥FPGA的内存优势，在大模型单批次推理中实现了显著的性能和能效提升，为端侧大模型部署提供了新路径。

Related Work: 现有工作多聚焦于GPU上的量化与稀疏化优化，或FPGA上的算术流水线设计；而本文首次探索将FPGA用于全查找表式的大模型推理，开辟了不同于主流的内存驱动范式。

Abstract: The rapid progress of large language models (LLMs) has advanced numerous applications, yet efficient single-batch inference remains vital for on-device intelligence. While FPGAs offer fine-grained data control and high energy efficiency, recent GPU optimizations have narrowed their advantage, especially under arithmetic-based computation. To overcome this, we leverage FPGAs' abundant on-chip memory to shift LLM inference from arithmetic- to memory-based computation through table lookups. We present LUT-LLM, the first FPGA accelerator enabling 1B+ LLM inference via vector-quantized memory operations. Our analysis identifies activation-weight co-quantization as the most effective scheme, supported by (1) bandwidth-aware parallel centroid search, (2) efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency gain over A100.

</details>


### [5] [STAR: Improving Lifetime and Performance of High-Capacity Modern SSDs Using State-Aware Randomizer](https://arxiv.org/abs/2511.06249)
*Omin Kwon,Kyungjun Oh,Jaeyong Lee,Myungsuk Kim,Jihong Kim*

Main category: cs.AR

TL;DR: 本文提出了一种新型数据随机化器STAR，用于解决高容量3D NAND闪存中由横向电荷扩散（LCS）引起的可靠性问题，显著提升了SSD的寿命和读取性能。


<details>
  <summary>Details</summary>
Motivation: 随着3D堆叠和多层单元技术的发展，高容量NAND闪存面临横向电荷扩散（LCS）带来的数据保持错误问题，影响SSD寿命，需有效应对。

Challenges: LCS在传统2D闪存中不存在，但在高密度3D闪存中显著增加数据保持错误；现有方法仅针对特定最差模式，无法全面消除各类弱数据模式。

Contributions: 提出了STate-Aware Randomizer（STAR），能主动消除导致LCS相关保持错误的多种弱数据模式；设计了多项优化方案，使STAR可低开销集成到SSD控制器I/O路径中。

Results: 基于160个真实3D NAND芯片构建STAR感知SSD模拟器进行评估，实验结果显示STAR可将SSD寿命提升最高2.3倍，并在真实工作负载下平均减少50%的读取延迟。

Conclusion: STAR能有效缓解高容量3D NAND闪存中的LCS问题，显著提高数据可靠性与SSD性能，具备实际部署的可行性。

Related Work: 已有研究主要集中在纠错码、写入均衡和数据刷新等机制来缓解保持错误，但缺乏对数据模式层面弱信号的系统性随机化处理，STAR填补了这一空白。

Abstract: Although NAND flash memory has achieved continuous capacity improvements via advanced 3D stacking and multi-level cell technologies, these innovations introduce new reliability challenges, particularly lateral charge spreading (LCS), absent in low-capacity 2D flash memory. Since LCS significantly increases retention errors over time, addressing this problem is essential to ensure the lifetime of modern SSDs employing high-capacity 3D flash memory. In this paper, we propose a novel data randomizer, STate-Aware Randomizer (STAR), which proactively eliminates the majority of weak data patterns responsible for retention errors caused by LCS. Unlike existing techniques that target only specific worst-case patterns, STAR effectively removes a broad spectrum of weak patterns, significantly enhancing reliability against LCS. By employing several optimization schemes, STAR can be efficiently integrated into the existing I/O datapath of an SSD controller with negligible timing overhead. To evaluate the proposed STAR scheme, we developed a STAR-aware SSD emulator based on characterization results from 160 real 3D NAND flash chips. Experimental results demonstrate that STAR improves SSD lifetime by up to 2.3x and reduces read latency by an average of 50% on real-world traces compared to conventional SSDs

</details>


### [6] [Offloading Data Center Tax](https://arxiv.org/abs/2511.06558)
*Akshay Revankar,Charan Renganathan,Sartaj Wariah*

Main category: cs.AR

TL;DR: 本文研究了数据中心中MongoDB微服务的税组件，通过分析其在DeathStarBench基准测试中的表现，探讨了将多个税组件共同卸载到加速器上的优化机会。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行着多样化的负载，共享许多底层功能（税组件），对这些组件的优化可广泛提升性能。然而，并非所有税组件都适合单独卸载到加速器上，因此需要探索更高效的联合卸载策略。

Challenges: 识别MongoDB中的关键税组件及其微架构影响；确定哪些组件适合联合卸载；在不增加硬件复杂性的前提下实现性能增益。

Contributions: 1) 对运行在DeathStarBench下的MongoDB进行详细剖析，识别其税组件；2) 分析这些组件的性能特征和微架构开销；3) 提出针对多个税组件联合卸载的优化建议。

Results: 通过剖析发现了MongoDB中若干关键税组件及其资源消耗模式，并基于此提出了可行的联合卸载方案，为数据中心级性能优化提供了新思路。

Conclusion: 联合卸载多个税组件是一种有潜力的数据中心优化方向，本文以MongoDB为例验证了该方法的可行性，并为未来加速器设计提供了指导。

Related Work: 相关工作包括数据中心税组件的研究、通用功能卸载（如压缩、加密）、以及DeathStarBench微服务基准测试套件的应用。

Abstract: The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.

</details>


### [7] [FPGA or GPU? Analyzing comparative research for application-specific guidance](https://arxiv.org/abs/2511.06565)
*Arnab A Purkayastha,Jay Tharwani,Shobhit Aggarwal*

Main category: cs.AR

TL;DR: 本文综述了FPGA和GPU在不同应用领域的性能、能效和可编程性，旨在为特定领域应用选择合适的加速器提供指导。


<details>
  <summary>Details</summary>
Motivation: 由于计算任务日益复杂，需要更高效和专用的硬件加速器。尽管已有大量关于FPGA和GPU的比较研究，但大多仅关注性能指标，缺乏对各自最适合应用类型的深入分析。

Challenges: 如何系统地比较FPGA和GPU在不同类型应用中的表现，并综合性能、能效和编程难度等多个维度做出合理推荐。

Contributions: 本文通过分类和分析现有研究，总结了FPGA和GPU的优势、局限性及最佳使用场景，提供了针对特定领域选择加速器的实际建议。

Results: 研究发现FPGA在低功耗、高并行、定制化任务中表现更优，而GPU在大规模数据并行、深度学习等应用中性能更强。同时指出了两者在能效和编程灵活性上的权衡。

Conclusion: FPGA和GPU各有优势，选择应基于具体应用场景的需求；本文为研究人员和实践者提供了系统的比较框架和实用的选型指南。

Related Work: 已有研究多集中于单一性能指标对比，缺乏对应用类型、能效和编程性的综合分析。本文在此基础上进行了系统性综述与分类总结。

Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.

</details>


### [8] [EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations](https://arxiv.org/abs/2511.06679)
*Sangun Choi,Yunho Oh*

Main category: cs.AR

TL;DR: EONSim是一个新型NPU模拟器，能够全面建模矩阵和嵌入向量操作，支持下一代NPU中复杂嵌入工作负载的灵活内存架构设计与评估。


<details>
  <summary>Details</summary>
Motivation: 现有NPU模拟器主要针对具有确定性访存模式的矩阵计算，缺乏对嵌入向量操作中非确定性、数据依赖型内存访问的真实建模能力，难以支持下一代NPU对灵活片上内存架构的需求。

Challenges: 嵌入向量操作具有输入数据依赖和非确定性的内存访问模式，传统双缓冲片上内存系统难以有效建模；同时需兼顾矩阵计算的高效模拟与嵌入访问的精细内存仿真。

Contributions: 提出EONSim，首个同时支持矩阵和嵌入向量操作的NPU模拟器；集成了经验证的矩阵性能模型与详细的嵌入内存访问模拟；支持多种片上内存管理策略；并在TPUv6e上验证了其高精度，平均推理时间误差1.4%，内存访问计数误差2.2%。

Results: EONSim在TPUv6e上的验证结果显示，其平均推理时间误差为1.4%，平均片上内存访问次数误差为2.2%，表现出高仿真精度。

Conclusion: EONSim为下一代NPU架构的探索提供了高精度、灵活的仿真平台，能够有效支持复杂嵌入工作负载下的内存系统设计与优化。

Related Work: 相关工作主要集中在面向矩阵计算的NPU模拟器（如Gem5、McSim、MAESTRO等），这些工具通常假设简单的双缓冲内存系统，缺乏对嵌入操作非规则访存行为的支持。

Abstract: Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\% and an average on-chip memory access count error of 2.2\%.

</details>


### [9] [Preemption-Enhanced Benchmark Suite for FPGAs](https://arxiv.org/abs/2511.06736)
*Arsalan Ali Malik,John Buchanan,Aydin Aysu*

Main category: cs.AR

TL;DR: 本文提出了首个开源的、支持抢占的基准测试套件，用于评估FPGA抢占策略和调度算法，包含27个多样化应用，并集成上下文保存与恢复机制，支持可重复研究和公平比较。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA任务调度与抢占研究缺乏统一、公开的基准测试框架，多依赖专有或合成负载，导致结果难以复现和比较，阻碍了多租户FPGA环境下的有效评估。

Challenges: 设计一个支持抢占、涵盖多种应用场景、具备标准化上下文管理机制且易于扩展的开源基准套件具有挑战性；同时需确保真实工作负载的代表性与可重复性。

Contributions: 1）首个开源的、支持抢占的FPGA基准套件，含27个多样化应用；2）集成上下文保存与恢复机制；3）提供添加新基准的指南；4）促进FPGA调度策略与操作系统研究的可重复性和公平评估。

Results: 该套件能够有效支持多种FPGA应用场景下的抢占与调度测试，简化了新调度算法的验证过程，并为多租户环境下调度公平性、资源利用率和上下文切换性能的评估提供了统一平台。

Conclusion: 本基准套件填补了FPGA抢占与调度研究中标准化评估工具的空白，推动了该领域的可重复研究，为未来FPGA操作系统和调度策略的发展提供了重要基础设施。

Related Work: 已有研究多聚焦于FPGA调度或抢占机制设计，但普遍使用私有或合成负载进行评估，缺乏公开、统一的基准测试集，导致跨研究比较困难。

Abstract: Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.
  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.

</details>


### [10] [P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats](https://arxiv.org/abs/2511.06838)
*Yuzong Chen,Chao Fang,Xilai Dai,Yuheng Wu,Thierry Tambe,Marian Verhelst,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: 本文提出了P3-LLM，一种结合NPU与PIM的混合精度LLM推理加速器，通过灵活的混合数值格式量化、轻量级PIM架构设计和算子融合优化，在保持高精度的同时显著提升了计算吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在推理过程中面临巨大的内存带宽和计算需求，现有基于高精度（如FP16）的PIM方案在DRAM中带来显著的面积和功耗开销，限制了计算吞吐量，因此需要更高效的加速方案。

Challenges: 如何在保证LLM推理精度的前提下，降低PIM系统的面积和功耗开销，并提升计算吞吐量；同时有效管理低精度数据流和减少运行时反量化开销。

Contributions: 1）提出一种灵活的混合精度量化方案，使用混合数值格式对LLM不同操作数进行高效压缩；2）设计支持该格式的轻量级PIM加速器架构，在相同面积下显著提升计算吞吐量；3）通过算子融合优化低精度数据流，减少反量化开销。

Results: 在多个代表性LLM和任务上评估显示，P3-LLM在KV缓存量化和权重-激活量化方面均达到最先进精度，并相比HBM-PIM、Ecco和Pimba平均实现4.9倍、2.0倍和3.4倍的加速。

Conclusion: P3-LLM通过混合数值格式量化与PIM架构协同设计，有效解决了LLM推理中的带宽与计算瓶颈，在保持高精度的同时大幅提升了加速性能，为高效LLM推理提供了可行方案。

Related Work: 已有工作探索了NPU与DRAM基PIM的异构系统用于LLM加速，如HBM-PIM、Ecco和Pimba，但其高精度PIM单元存在面积和功耗瓶颈。

Abstract: The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\times$, $2.0\times$, and $3.4\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git

</details>


### [11] [Optimizing GEMM for Energy and Performance on Versal ACAP Architectures](https://arxiv.org/abs/2511.06907)
*Ilias Papalamprou,Dimosthenis Masouros,Ioannis Loudaros,Francky Catthoor,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 本文提出了一种针对AMD Versal ACAP的自动化框架，利用机器学习模型指导设计空间探索，优化GEMM在性能和能效方面的映射，实验结果显示在吞吐量和能效上均显著优于现有框架。


<details>
  <summary>Details</summary>
Motivation: GEMM是许多科学计算和深度学习任务中的关键操作，但在资源和功耗受限的边缘环境中常成为性能和能效瓶颈。现有方法在Versal ACAP上对GEMM的映射缺乏对能效与性能权衡的充分考虑。

Challenges: 在Versal ACAP的异构组件（AIE、PL、PS）上高效映射GEMM具有复杂性，尤其是在兼顾性能与能量效率方面，传统分析方法难以准确建模和优化。

Contributions: 提出了一种基于机器学习的自动化框架，通过在约6000次板上实验数据上训练模型，实现对GEMM映射的设计空间探索，支持以性能或能效为目标的优化。

Results: 在Versal VCK190上的评估显示，相比现有最先进框架，吞吐量几何平均提升1.23倍（最高达2.5倍），能效提升1.25倍（最高达2.7倍）。

Conclusion: 该框架能有效提升GEMM在Versal ACAP上的性能与能效，验证了基于机器学习的方法在异构系统设计空间探索中的优势。

Related Work: 先前的研究主要集中在分析性建模或特定硬件组件上的GEMM优化，较少考虑Versal ACAP异构架构下的能量-性能权衡，且缺乏基于实测数据驱动的自动化映射方法。

Abstract: General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus](https://arxiv.org/abs/2511.05843)
*Hanzheng Lyu,Shaokang Xie,Jianyu Niu,Mohammad Sadoghi,Yinqian Zhang,Cong Wang,Ivan Beschastnikh,Chen Feng*

Main category: cs.DC

TL;DR: HYDRA是一种新型的多拜占庭容错（Multi-BFT）共识框架，通过消除全局排序层，采用以对象为中心的执行模型，实现跨实例的并发确定性执行，从而提升可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有Multi-BFT协议依赖全局排序层来序列化多个实例间的区块，导致可扩展性受限、故障传播加剧和部署复杂。HYDRA旨在打破这一传统设计，探索无需全局排序的高效共识机制。

Challenges: 如何在去除全局排序的同时，保证跨多个BFT实例的事务执行一致性与正确性；如何在高并发下避免死锁并实现高效协调。

Contributions: 1. 提出HYDRA，首个无需全局排序的Multi-BFT共识框架；2. 设计以对象为中心的执行模型，按访问对象划分事务；3. 引入轻量级基于锁的协调与死锁解决机制，确保一致性和并发性。

Results: 在最多128个副本的局域网和广域网环境中实现并评估HYDRA，实验表明其在存在延迟节点（straggler）的情况下，性能优于多个最先进的Multi-BFT协议，同时保持强一致性。

Conclusion: HYDRA通过去除全局排序层，显著提升了Multi-BFT系统的可扩展性和容错能力，为构建高性能、强一致的分布式共识系统提供了新方向。

Related Work: 与依赖全局排序的Multi-BFT系统（如OmniLedger、RapidChain）不同，HYDRA完全消除全局协调，与基于分片的共识和并发控制机制（如Calvin、Taurus）有部分相似，但更专注于多实例BFT环境下的对象级并发与一致性保障。

Abstract: Multi-Byzantine Fault Tolerant (Multi-BFT) consensus, which runs multiple BFT instances in parallel, has recently emerged as a promising approach to overcome the leader bottleneck in classical BFT protocols. However, existing designs rely on a global ordering layer to serialize blocks across instances, an intuitive yet costly mechanism that constrains scalability, amplifies failure propagation, and complicates deployment. In this paper, we challenge this conventional wisdom. We present HYDRA, the first Multi-BFT consensus framework that eliminates global ordering altogether. HYDRA introduces an object-centric execution model that partitions transactions by their accessed objects, enabling concurrent yet deterministic execution across instances. To ensure consistency, HYDRA combines lightweight lock-based coordination with a deadlock resolution mechanism, achieving both scalability and correctness. We implement HYDRA and evaluate it on up to 128 replicas in both LAN and WAN environments. Experimental results show HYDRA outperforms several state-of-the-art Multi-BFT protocols in the presence of a straggler. These results demonstrate strong consistency and high performance by removing global ordering, opening a new direction toward scalable Multi-BFT consensus design.

</details>


### [13] [CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing](https://arxiv.org/abs/2511.05915)
*Guihang Hong,Tao Ouyang,Kongyange Zhao,Zhi Zhou,Xu Chen*

Main category: cs.DC

TL;DR: 本文提出了一种名为CoEdge-RAG的分层调度框架，用于在协作式边缘计算环境中优化检索增强型大语言模型（LLM）的性能，通过在线查询识别、动态跨节点调度和自适应资源分配，在隐私受限和资源异构的条件下显著提升了响应质量与效率。


<details>
  <summary>Details</summary>
Motivation: 由于对实时响应和数据隐私保护的需求日益增长，大语言模型正被部署在资源受限的边缘设备上进行本地推理。检索增强生成（RAG）技术可有效整合本地数据以提升输出质量，但现有边缘计算范式多局限于单节点优化，忽视了跨节点协作对分布式数据与异构资源的综合利用潜力。

Challenges: 1. 隐私限制导致无法准确预知边缘节点间异构的数据分布，影响RAG性能优化；2. 边缘节点资源异构且动态变化，难以实现高效的跨节点任务调度；3. 局部负载波动大，需在延迟与生成质量之间进行动态权衡。

Contributions: 1. 提出一种基于近端策略优化（PPO）的在线查询识别机制，可自主推断查询语义并建立跨域知识关联；2. 设计一种结合历史性能分析与实时资源阈值的动态跨节点调度策略，实现负载均衡；3. 构建基于在线凸优化的节点内调度器，自适应分配查询处理比例与内存资源，优化延迟-质量权衡。

Results: 在多个问答基准上的实验表明，CoEdge-RAG相比基线方法在各项任务中性能提升了4.23%至91.39%，显著提高了检索增强型LLM在协作边缘环境中的推理效率与生成质量。

Conclusion: CoEdge-RAG通过层次化调度框架有效解决了边缘环境中检索增强生成面临的隐私、资源异构与动态负载等挑战，实现了跨节点协作下的高性能本地化推理，为边缘智能应用提供了可行的技术路径。

Related Work: 相关工作主要集中在边缘计算中的单节点LLM推理优化、检索增强生成（RAG）技术在云端的应用，以及分布式边缘系统中的资源调度方法。然而，现有研究较少考虑在隐私约束下跨边缘节点协同利用分布式数据与异构资源以支持RAG的完整框架。

Abstract: Motivated by the imperative for real-time responsiveness and data privacy preservation, large language models (LLMs) are increasingly deployed on resource-constrained edge devices to enable localized inference. To improve output quality, retrieval-augmented generation (RAG) is an efficient technique that seamlessly integrates local data into LLMs. However, existing edge computing paradigms primarily focus on single-node optimization, neglecting opportunities to holistically exploit distributed data and heterogeneous resources through cross-node collaboration. To bridge this gap, we propose CoEdge-RAG, a hierarchical scheduling framework for retrieval-augmented LLMs in collaborative edge computing. In general, privacy constraints preclude accurate a priori acquisition of heterogeneous data distributions across edge nodes, directly impeding RAG performance optimization. Thus, we first design an online query identification mechanism using proximal policy optimization (PPO), which autonomously infers query semantics and establishes cross-domain knowledge associations in an online manner. Second, we devise a dynamic inter-node scheduling strategy that balances workloads across heterogeneous edge nodes by synergizing historical performance analytics with real-time resource thresholds. Third, we develop an intra-node scheduler based on online convex optimization, adaptively allocating query processing ratios and memory resources to optimize the latency-quality trade-off under fluctuating assigned loads. Comprehensive evaluations across diverse QA benchmarks demonstrate that our proposed method significantly boosts the performance of collaborative retrieval-augmented LLMs, achieving performance gains of 4.23\% to 91.39\% over baseline methods across all tasks.

</details>


### [14] [MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies](https://arxiv.org/abs/2511.05958)
*Stepan Vanecek,Manuel Walter Mussbacher,Dominik Groessler,Urvij Saroliya,Martin Schulz*

Main category: cs.DC

TL;DR: MT4G是一个开源、与厂商无关的工具，用于自动发现GPU的计算和内存拓扑结构及配置，通过结合API和50多个微基准测试，应用统计方法可靠识别拓扑属性，并在多种GPU上验证其通用性，应用于性能建模、瓶颈分析和资源动态分区。


<details>
  <summary>Details</summary>
Motivation: GPU拓扑信息对于HPC和AI性能优化至关重要，但目前缺乏像CPU领域hwloc这样的通用工具，且现有信息不完整、依赖厂商。

Challenges: GPU拓扑信息难以获取、不完整且厂商特定，缺乏自动化、跨厂商的拓扑发现工具。

Contributions: 提出了MT4G，一个开源、跨厂商的GPU拓扑自动发现工具，结合API与50多个微基准测试及统计方法（如Kolmogorov-Smirnov检验），实现对缓存大小、带宽和物理布局等属性的自动识别。

Results: 在十种不同GPU上验证了MT4G的通用性，并成功集成到GPU性能建模、GPUscout瓶颈分析和动态资源分区三个工作流中，展示了其在NVIDIA和AMD GPU上的有效性。

Conclusion: MT4G提供了一种自动化、可移植的解决方案，能够跨厂商理解现代HPC和AI系统中的GPU性能特征，填补了当前工具链的空白。

Related Work: CPU领域有hwloc等成熟工具用于拓扑发现，但在GPU领域尚缺乏类似通用、自动化且不依赖厂商的解决方案。

Abstract: Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.
  In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.
  We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems.

</details>


### [15] [Inductive Loop Analysis for Practical HPC Application Optimization](https://arxiv.org/abs/2511.06052)
*Philipp Schaad,Tal Ben-Nun,Patrick Iff,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文提出了一种名为符号归纳循环优化（SILO）的新技术，通过将数据访问和依赖建模为循环步幅的函数，实现了对顺序依赖循环的自动并行化以及数据移动优化，在科学计算核心算法上实现了最高12倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中的编译器难以有效分析跨步数据访问和循环携带依赖等常见模式，导致细粒度数据移动优化不足，因此需要一种更高层次的抽象方法来提升优化能力。

Challenges: 主要挑战包括如何对具有复杂数据访问模式（如跨步访问）的多层循环进行建模，以及如何在存在循环间依赖的情况下实现自动并行化和高效的数据预取与寄存器使用。

Contributions: 提出了SILO技术，引入符号化、归纳式的循环优化框架，能够建模数据访问和依赖关系，并支持自动并行化、软件预取和指针递增以减少寄存器溢出。

Results: 在大气模型和数值求解器等科学应用的核心内核上验证了SILO的有效性，相比现有最先进技术实现了最高12倍的加速。

Conclusion: SILO通过引入基于循环步幅的函数式抽象，显著提升了对多维数组循环嵌套的分析与优化能力，为科学计算中的性能瓶颈提供了有效的编译时解决方案。

Related Work: 相关工作包括传统编译器优化技术（如循环展开、分块）、自动并行化方法以及现有HPC框架中的数据局部性优化，但这些方法在高层模式分析方面存在局限。

Abstract: Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\times$ speedup over the state of the art.

</details>


### [16] [LiteCast: A Lightweight Forecaster for Carbon Optimizations](https://arxiv.org/abs/2511.06187)
*Mathew Joseph,Tanush Savadi,Abel Souza*

Main category: cs.DC

TL;DR: LiteCast是一种轻量级的时间序列预测方法，用于快速建模区域能源结构并估算碳强度，仅需几天的历史数据即可实现高效、快速适应电网变化的预测，在50个全球区域的评估中表现出优于现有最先进模型的性能，节省了20%以上的成本，并达到最大可实现平均节省的97%。


<details>
  <summary>Details</summary>
Motivation: 随着电气化和人工智能的发展，电力需求持续增长，电网通过扩大发电能力和引入可再生能源来应对负荷激增，但这影响了区域碳强度。为减少消费带来的环境影响，现有的碳感知优化依赖高精度、长时域的碳强度预测模型，但这些模型计算复杂、扩展性差，且精度提升并不直接转化为节能效果的线性增长。因此需要更高效的预测策略。

Challenges: 如何在不牺牲节能效果的前提下降低碳强度预测模型的复杂度和数据需求；如何使模型能够快速适应电网中的突发变化；如何在多种实际工作负载和不同区域条件下保持高性能和广泛适用性。

Contributions: 提出LiteCast，一种轻量级、高效的碳强度时间序列预测方法，仅需数天历史数据即可运行；证明预测排序的保真度比绝对精度更能决定节能效果；在50个全球区域的实验中验证了LiteCast在节能方面的优越表现，达到了接近最优的性能。

Results: LiteCast在多个真实工作负载下的全球50个区域评估中，相比最先进的预测模型实现了20%更高的节能效果，并达到了理论最大平均节能的97%，同时具备快速推理和快速适应新数据的能力。

Conclusion: 碳强度预测不必依赖复杂的高精度模型来实现显著的节能效果，LiteCast通过保持预测排名的准确性，在极低的数据和计算需求下实现了近最优的节能表现，展示了高效、可扩展的碳感知调度的可行性。

Related Work: 本文与碳感知计算、时间序列预测、能源系统建模以及基于需求响应的节能策略相关，特别是与使用深度学习模型进行长期碳强度预测的工作形成对比，强调了模型效率与实际节能效果之间的权衡。

Abstract: Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.

</details>


### [17] [PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization](https://arxiv.org/abs/2511.06345)
*Kelun Lei,Hailong Yang,Huaitao Zhang,Xin You,Kaige Zhang,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: PRAGMA是一个基于性能分析的AI内核生成框架，通过结合执行反馈和细粒度硬件分析，帮助大语言模型识别性能瓶颈并迭代优化代码，显著提升CPU和GPU上的内核性能。


<details>
  <summary>Details</summary>
Motivation: 现有的AI内核生成系统大多仅依赖正确性或执行时间反馈，缺乏对底层性能瓶颈的推理能力，限制了生成代码的性能优化潜力。

Challenges: 如何让大语言模型有效识别和理解硬件层面的性能瓶颈；如何在迭代生成过程中保留历史最优版本并指导代码改进；如何在不同硬件平台（如CPU和GPU）上实现一致的性能提升。

Contributions: 提出了PRAGMA框架，首次将细粒度硬件性能分析引入AI内核生成的推理循环；设计了保留最佳历史版本和迭代优化的机制；在CPU和GPU上验证了框架的有效性，显著优于基线方法。

Results: 在KernelBench基准上，PRAGMA相比无分析功能的基线AIKG表现更优，相较于Torch在CPU和GPU上分别实现了平均2.81倍和2.30倍的加速。

Conclusion: PRAGMA通过引入性能分析反馈，显著提升了AI生成内核的性能，证明了将硬件分析与语言模型推理结合的有效性，为自动化高性能内核生成提供了新方向。

Related Work: 相关工作包括自动内核优化工具（如TVM、Halide）、大语言模型在代码生成中的应用（如CodeGen、PaLM Code），以及基于强化学习或搜索的AI内核生成（AIKG）系统。

Abstract: Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.

</details>


### [18] [Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads](https://arxiv.org/abs/2511.06599)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种名为Saarthi的新型端到端无服务器框架，通过输入感知和多目标优化模型，动态管理函数资源需求，提升系统吞吐量并降低成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决FaaS平台中存在的启动延迟、静态资源配置、资源分配与调度次优等问题，提升函数性能的一致性和成本效益。

Challenges: 主要包括冷启动延迟、静态资源分配导致的资源浪费、工作负载无关的通用调度策略以及运行成本不可预测等挑战。

Contributions: 1. 提出Saarthi框架，实现输入感知的资源预测；2. 设计基于ILP的多目标优化模型以平衡吞吐量与成本；3. 引入主动容错冗余机制；4. 实现动态请求编排与函数配置管理。

Results: 在OpenFaaS上实现Saarthi后，相比基线实现了最高1.45倍的吞吐量提升，成本降低1.84倍，并能维持高达98.3%的服务水平目标，额外开销最多为0.2秒。

Conclusion: Saarthi通过智能、动态的资源管理，显著提升了无服务器平台的性能与成本效率，推动了自驱式（self-driving）无服务器计算的发展。

Related Work: 相关工作包括基于静态配置的FaaS平台如OpenFaaS、Knative，以及一些动态资源调整机制的研究，但缺乏对输入特征的感知和多目标联合优化。

Abstract: FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.

</details>


### [19] [DMA Collectives for Efficient ML Communication Offloads](https://arxiv.org/abs/2511.06605)
*Suchita Pati,Mahzabeen Islam,Shaizeen Aga,Mohamed Assem Ibrahim*

Main category: cs.DC

TL;DR: 本文研究了在AMD Instinct MI300X GPU上将机器学习通信规约（如all-gather、all-to-all）卸载到DMA引擎的性能、功耗和同步开销，发现DMA在大尺寸传输中表现优异，但在小尺寸场景下因调度与同步开销而性能较差；通过利用现有DMA架构创新优化后，显著缩小了性能差距，推动DMA规约向主流库实用化迈进。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器学习训练和推理中通信与计算的重叠效率，降低GPU核心和内存子系统的负担，研究将通信规约卸载到DMA引擎的潜力，但现有工作仅限于带宽密集型大传输场景，缺乏对功耗、能效和小数据量延迟的全面分析。

Challenges: DMA集体通信在小数据量（latency-bound）场景下性能显著落后，主要受限于DMA命令调度和同步开销；同时缺乏对性能、功耗和能效的综合评估。

Contributions: 1）首次对DMA集体通信在先进GPU上的性能、功耗和同步成本进行全面分析；2）揭示了DMA在大小数据量下的表现差异及瓶颈；3）利用未被充分利用的DMA架构创新优化集体通信实现；4）在真实硬件上验证了优化方案的有效性，显著提升小数据量性能并进一步改善大数据量效率。

Results: 相比RCCL，DMA集体在大尺寸（数十MB至GB）下性能提升16%、功耗降低32%；但在小尺寸下all-gather慢4.5倍，all-to-all慢2.5倍。经优化后，小尺寸下all-gather仅慢30%，all-to-all反而快20%，大尺寸性能再提升7%，功耗节省增加3-10%。

Conclusion: DMA集体通信在大尺寸传输中已具备优势，但小尺寸性能受限于调度与同步开销；通过架构创新优化可显著缩小差距，使DMA集体更接近主流通信库的实用要求。

Related Work: 此前的研究主要集中在将通信任务卸载到DMA以提升并发性能，但仅评估了带宽受限的大传输场景，且多关注性能而忽视功耗与同步成本。

Abstract: Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).
  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.

</details>


### [20] [A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump](https://arxiv.org/abs/2511.06824)
*Xin Yao,Yang Liu,Jin Jiang,Yesen Chen,Zhilong Chen,Hongkang Dong,Xiaofeng Wei,Teng Zhang,Dongyun Wang*

Main category: cs.DC

TL;DR: 提出了一种基于GPU加速的高性能多工况联合分析框架（GMAF），用于快速求解轴向柱塞泵（APP）在多周期内的动力学问题，显著提升了压力场和油液力矩的计算效率，并揭示了表面织构对压力承载能力和抗扭性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 传统CPU和迭代方法在处理具有精细网格的复杂表面织构问题时计算效率低，难以实现多周期动力学仿真，因此需要一种高效、可扩展的加速方法。

Challenges: 复杂表面织构需要精细网格，导致计算规模大；多周期仿真对计算资源需求高；传统求解器效率低，难以实现全局收敛。

Contributions: 设计了基于GPU的GMAF框架，采用PCG与ASSOR预条件子，实现了高并行度和高计算强度；提出同步收敛策略确保全局收敛；首次高效实现了光滑与织构表面APP在多周期内的联合动力学分析。

Results: GMAF显著加速了压力场的建立与求解以及油液力和力矩的数值积分；结果显示轴向油压力和周向力矩直接响应输入压力，其他分量呈正弦变化；法向压力相关的力和力矩迅速达到稳态，而粘性剪应力相关的则随周期演化；织构表面在压力场中形成‘台阶’，提升了压力承载能力和抗扭性能。

Conclusion: GMAF框架能高效、准确地模拟轴向柱塞泵在多周期内的动力学行为，尤其适用于包含表面织构的复杂工况，为APP的设计与优化提供了强有力的仿真工具。

Related Work: 已有研究多采用传统CPU迭代方法进行APP动力学仿真，受限于计算效率，难以处理精细网格和多周期问题；部分工作使用GPU加速单物理场求解，但缺乏多工况联合分析框架与全局收敛策略的设计。

Abstract: Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.

</details>


### [21] [LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure](https://arxiv.org/abs/2511.07229)
*Jaehong Cho,Hyunmin Choi,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim2.0是一个用于大规模大语言模型（LLM）服务系统中异构硬件探索的系统级模拟器，通过引入基于轨迹的性能建模和算子级延迟分析器，显著简化了新加速器的集成，并支持广泛的现代LLM服务技术，实验证明其在TPU案例中代码量减少18.5倍，GPU模拟误差仅为1.9%，具备高精度与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统模拟器在硬件模型集成方面缺乏清晰的抽象，且仅支持有限的服务技术，难以覆盖现代LLM服务的多样性，因此需要一个更灵活、易扩展的模拟平台。

Challenges: 1) 硬件模型难以集成到系统级模拟器中，缺乏统一抽象；2) 现有模拟器支持的服务技术范围狭窄，无法反映现代LLM服务的全貌。

Contributions: 1) 提出LLMServingSim2.0，采用基于轨迹的性能建模与算子级延迟分析器，实现一键式加速器集成；2) 集成最新的LLM服务技术，并提供请求路由、缓存管理和调度策略的灵活接口；3) 在TPU案例中验证了低代码量和高性能的硬件扩展能力。

Results: 1) 在TPU案例中，新分析器相比前代减少18.5倍代码量，硬件集成更高效；2) 对GPU上LLM服务的模拟误差仅为1.9%，同时保持实用的模拟时间。

Conclusion: LLMServingSim2.0是一个高效、精确且易于扩展的LLM服务系统模拟平台，能够支持广泛的硬件和系统技术探索，适用于硬件开发者与LLM服务提供商。

Related Work: 本文的前身LLMServingSim是早期面向LLM服务的模拟器，但缺乏对异构硬件的良好支持和灵活的系统组件建模能力，其他相关工作多集中于特定硬件或特定服务策略的模拟，缺乏综合性与可扩展性。

Abstract: This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.

</details>
