<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT提出了一种基于观察的协调模式，利用CRDT实现多智能体LLM系统中的无锁、无冲突并发代码生成，实验表明其在部分任务上可加速，但某些任务中会变慢，且始终保证强最终一致性与零合并失败。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统由于协调成本高而难以实现并行加速，因此需要一种更高效的协调机制。

Challenges: 如何在保证一致性的同时避免显式消息传递带来的开销，并处理LLM智能体在并发生成中的语义冲突。

Contributions: 提出了CodeCRDT协调模式，形式化了面向LLM智能体的观察驱动协调机制，揭示了语义冲突率与性能质量权衡，并通过实验刻画了任务结构对并行协调成败的影响。

Results: 在600次实验中，部分任务获得最高21.1%的加速，部分任务最慢下降39.4%，所有运行均100%收敛且无合并失败，语义冲突率为5-10%。

Conclusion: 观察驱动协调在特定任务结构下可有效提升多智能体LLM系统的并行效率，但存在性能与生成质量的权衡，需根据任务特性进行设计。

Related Work: Conflict-Free Replicated Data Types (CRDTs) 在分布式系统中的一致性维护工作，以及多智能体系统中基于消息传递的协调机制。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [2] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 本文探讨了通过结合大语言模型的随机代码生成与特定领域模拟器中的确定性验证，来实现AI驱动的分布式系统策略设计。


<details>
  <summary>Details</summary>
Motivation: 为了提高分布式系统中调度策略的设计效率和效果，研究者们试图利用AI技术自动探索大的设计空间。

Challenges: 如何有效地结合生成模型与验证机制以确保生成的策略既创新又可靠；同时保持系统的可解释性是一个挑战。

Contributions: 提出了一个迭代的生成-验证循环框架，用于自动化地设计和优化分布式系统的调度策略，并在Function-as-a-Service运行时环境中进行了案例研究。

Results: 初步结果显示，在多个模型上实现了吞吐量的提升，证明了该方法的有效性。

Conclusion: AI将在扩展此方法论方面发挥关键作用，特别是在帮助启动新的模拟器方面。

Related Work: 相关工作包括使用机器学习优化分布式系统性能的研究，以及利用大语言模型进行软件开发辅助的工作。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [3] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS是一种针对MoE训练中通信瓶颈的分布式负载均衡框架，利用Rail拓扑的对称性实现局部调度下的全局均衡，显著提升带宽利用率并降低完成时间。


<details>
  <summary>Details</summary>
Motivation: MoE模型训练中的稀疏且高度不平衡的全对全通信成为迭代的主要开销，传统负载均衡方法未能充分利用Rail架构的确定性拓扑和多NIC带宽。

Challenges: 如何在不依赖全局协调的情况下实现高效的负载均衡；如何充分利用Rail架构的多路径并行能力以减少通信延迟。

Contributions: 提出了RailS框架，首次利用Rail拓扑的对称性证明均匀发送可保证均匀接收，将全局协调转化为局部调度；设计了基于LPT的喷洒调度器，并启用N条并行通道实现细粒度、拓扑感知的多路径传输。

Results: 在合成和真实MoE负载下，总线带宽提升20%–78%，完成时间减少17%–78%；在Mixtral负载上迭代时间缩短18%–40%，达到近似最优负载均衡。

Conclusion: RailS通过拓扑感知的局部调度有效解决了MoE训练中的通信瓶颈，充分挖掘了分布式架构的并行潜力，为大规模MoE模型训练提供了高效可扩展的通信支持。

Related Work: 现有工作主要集中在通用负载均衡策略，缺乏对Rail这类特定拓扑结构的优化利用；部分研究尝试动态路由但未结合拓扑对称性和本地化调度。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [4] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 本文比较了使用Java、Python和Scala在Apache Spark平台上处理大规模数据集的性能，重点分析了不同数据量和操作复杂度下各语言在完整ETL工作流中的表现，并利用Apache Iceberg进行数据存储。


<details>
  <summary>Details</summary>
Motivation: 为了优化不同数据规模和处理需求下的数据处理效率，需要系统比较Java、Python和Scala在Spark平台上的端到端ETL性能差异。

Challenges: 缺乏对多种编程语言在完整ETL流程中、尤其是在使用Apache Iceberg时的综合性能对比研究；不同语言在执行复杂操作和处理不同数据规模时的性能差异显著。

Contributions: 提供了Java、Python和Scala在Spark平台上执行完整ETL流程（包括读取CSV、转换、写入Iceberg表）的实证性能对比结果，揭示了语言选择与数据规模、操作复杂度之间的关系。

Results: 处理5MB小文件时，Python最快（6.71秒），优于Scala（9.13秒）和Java（9.62秒）；处理1.6GB大文件时，三者性能接近，Python最快（46.34秒），Java最慢（50.56秒）；在复杂操作（合并两个CSV文件）中，Scala最快（374.42秒），Java次之（379.8秒），Python最慢（398.32秒）。

Conclusion: 编程语言显著影响Spark的数据处理效率：Python在处理小数据量时更具优势，而Scala和Java在处理大数据量和复杂操作时更高效。该结果可为根据数据规模和性能需求选择合适的编程语言提供依据。

Related Work: 已有研究多关注Spark在单一语言下的特定阶段性能，或比较语言在内存计算方面的差异，但缺乏在使用Apache Iceberg的完整ETL流程中对多语言的系统性比较。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [5] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 该论文研究了正则图中匹配问题的局部性，提出了一种随机算法，证明了在正则图中(1+ε)-近似匹配是真正局部的，其局部性仅依赖于ε，且当度Δ足够大时，这种依赖仅为log(1/ε)。此外，论文还展示了最大匹配在节点平均复杂度和最坏情况复杂度之间的显著分离，并通过Luby算法的新颖鞅分析技术，证明了一轮Luby算法应用于Δ-正则图的线图后会产生几乎Δ/2-正则图。


<details>
  <summary>Details</summary>
Motivation: 理解分布式对称性打破问题中的局部性，即节点需要探索其邻域半径以达到全局解的一部分。

Challenges: 确定正则图中匹配问题的局部性，特别是近似匹配与最大匹配的局部性差异。

Contributions: 提出了新的随机算法来展示(1+ε)-近似匹配在正则图中的局部性；建立了最大匹配在节点平均复杂度和最坏情况复杂度之间的强分离；引入了基于鞅分析的技术来分析Luby算法。

Results: 证明了(1+ε)-近似匹配在正则图中的局部性仅依赖于ε，且当Δ≥poly(1/ε)时，这种依赖为log(1/ε)；最大匹配的节点平均复杂度仅为O(1)；一轮Luby算法应用于Δ-正则图的线图后产生几乎Δ/2-正则图。

Conclusion: 论文成功地揭示了正则图中近似匹配和最大匹配的局部性特性，为分布式对称性打破问题提供了新的见解和技术工具。

Related Work: 正则图作为建立对称性打破问题下界和分类结果的主要基准，已有大量研究关注于最大匹配的局部性。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [6] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost提出了一种成本高效的强化学习训练框架，通过利用抢占式GPU资源（如云上的竞价实例）来加速大语言模型的推理阶段（rollout），采用混合架构和三项关键技术：自适应rollout卸载、基于拉取的权重传输、以及token级响应收集与迁移，显著提升了训练吞吐量和成本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型中日益重要，但其训练流程中rollout和训练阶段存在资源需求不匹配的问题；同时，现有框架无法有效利用抢占式GPU资源，导致成本高且资源利用率低。

Challenges: 如何在频繁且不可预测的抢占式资源中断下，高效利用碎片化GPU资源进行大规模rollout；同时保持与训练阶段的协调，避免资源浪费和性能下降。

Contributions: 1) 提出RLBoost框架，首次系统性地将抢占式GPU资源用于LLM的RL训练；2) 设计混合架构与三项核心技术：自适应rollout卸载、pull-based权重同步、token级响应收集与迁移；3) 实现高吞吐与高成本效益的平衡。

Results: 实验表明，相比仅使用按需GPU资源，RLBoost将训练吞吐量提升了1.51倍至1.97倍，成本效率提高了28%到49%。

Conclusion: RLBoost通过有效整合抢占式GPU资源，解决了RL训练中资源利用不充分和成本高的问题，为大语言模型的高效强化学习提供了一种可扩展且经济的解决方案。

Related Work: 相关工作包括共置式与 disaggregated 架构的RL训练框架，以及抢占式资源在深度学习中的应用研究，但这些工作未充分解决rollout与训练阶段的资源错配及抢占带来的效率问题。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [7] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: 本文提出了FLASH Viterbi和FLASH-BS Viterbi两种新型Viterbi解码算法，通过非递归分治、剪枝和并行化技术提升了解码的时间和内存效率，并设计了基于FPGA的硬件加速器，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 随着工作负载越来越多地迁移到资源受限的边缘平台，标准Viterbi解码仍然内存密集且计算不灵活，现有方法通常以解码时间为代价换取空间效率，但往往带来显著的运行时开销并且缺乏对各种系统约束的适应性。

Challenges: 如何在保持解码精度的同时减少内存使用和计算时间，以及如何使算法适应不同的系统约束。

Contributions: 提出了一种快速、轻量、自适应且硬件友好的Viterbi解码算子FLASH Viterbi，以及基于动态束搜索的变体FLASH-BS Viterbi；开发了基于FPGA的硬件加速器；实验表明新算法在解码时间和内存效率方面均优于现有基线。

Results: 实验结果显示，所提出的算法在解码时间和内存效率方面始终优于现有的基线方法，同时保持了适应性和硬件友好特性。

Conclusion: FLASH Viterbi和FLASH-BS Viterbi算法及其硬件加速器为资源受限的数据系统提供了高效、灵活的Viterbi解码解决方案。

Related Work: 现有的Viterbi解码方法主要通过牺牲解码时间来提高空间效率，但通常会带来较大的运行时开销，且缺乏对不同系统约束的适应能力。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems. To further
decouple space complexity from the hidden state space size, we present FLASH-BS
Viterbi, a dynamic beam search variant built on a memory-efficient data
structure. Both proposed algorithms exhibit strong adaptivity to diverse
deployment scenarios by dynamically tuning internal parameters. To ensure
practical deployment on edge devices, we also develop FPGA-based hardware
accelerators for both algorithms, demonstrating high throughput and low
resource usage. Extensive experiments show that our algorithms consistently
outperform existing baselines in both decoding time and memory efficiency,
while preserving adaptability and hardware-friendly characteristics essential
for modern data systems. All codes are publicly available at
https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [8] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: 本文提出了HybridEP，一种在带宽受限环境下优化MoE模型专家并行（EP）的混合式框架，通过动态调整专家的分布位置并结合建模指导，显著降低了跨数据中心训练中的通信开销，提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型规模的快速增长，单数据中心已难以支撑训练需求，跨数据中心训练成为趋势。然而，受限于跨DC带宽，专家并行（EP）面临严重的通信瓶颈，现有方法在低带宽下难以有效重叠通信与计算，亟需新的优化方案。

Challenges: 在低带宽环境下，EP的通信时间远长于计算时间，导致通信重叠策略效果有限；同时，动态调整专家分布会引入额外的专家迁移开销，并改变原有的通信模式，使得通信优化更加复杂。

Contributions: 1）提出HybridEP框架，通过动态变换专家空间布局来减少通信开销；2）构建基于流的模型以确定最优传输比例；3）设计域划分和参数高效迁移技术，在GPU层级优化通信拓扑结构；4）实现了在受限带宽下更具可扩展性的EP训练方案。

Results: 实验表明，在带宽受限场景下，HybridEP相比现有最先进的MoE训练系统性能提升最高达5.6倍；在大规模仿真中（1000个数据中心），不同带宽条件下最高实现1.45倍的加速。

Conclusion: HybridEP通过建模指导的混合专家并行策略，有效缓解了跨数据中心训练中因带宽受限导致的通信瓶颈，显著提升了MoE模型的训练效率和可扩展性，为未来大规模分布式训练提供了可行路径。

Related Work: 相关工作主要包括MoE架构设计（如Switch Transformer）、专家并行策略（如EP、DP）、通信优化技术（如流水线并行、通信重叠）以及跨数据中心训练中的资源调度机制。

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [9] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: Propius是一个为协作机器学习设计的可扩展资源管理系统，通过控制平面和数据平面实现高效的资源管理和计算流控制，显著提升了资源利用率、吞吐量和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 协作机器学习面临数据隐私、通信开销和模型异构性等挑战，现有系统缺乏可扩展性和可重用性，亟需一个高效的多租户资源管理框架。

Challenges: 如何在异构客户端环境中实现高效的资源共享、可扩展的模型分发与结果收集，并支持多种资源分配策略。

Contributions: 提出了Propius系统，包含控制平面（支持多任务资源调度）和数据平面（提升通信效率），实现了对协作ML任务的高效、可扩展管理。

Results: 实验表明，Propius在资源利用率上最高提升1.88倍，吞吐量最高提升2.76倍，任务完成时间最高缩短1.26倍。

Conclusion: Propius为协作机器学习提供了一个高效、可扩展且支持多租户的资源管理解决方案，显著优于现有框架。

Related Work: 相关工作集中在协作机器学习的优化算法、通信设计和硬件支持，但在系统级基础设施特别是资源管理方面研究较少。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [10] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 本研究对现代云原生环境中关键的内存键值存储系统（Redis替代品Valkey、KeyDB和Garnet）进行了在Kubernetes环境下的性能与可行性评估，填补了当前文献的空白。


<details>
  <summary>Details</summary>
Motivation: 内存键值数据存储已成为现代云原生基础设施的关键组件，但其发展面临可扩展性、兼容性和可持续性限制，且缺乏对最新工具的实验性评估。

Challenges: 在Kubernetes环境中评估不同内存键值存储系统的性能、兼容性和长期可行性，同时衡量吞吐量、尾部延迟、资源效率和迁移复杂性之间的权衡。

Contributions: 系统性地比较了Valkey、KeyDB和Garnet在真实工作负载下的表现，提供了关于性能、资源利用和项目可持续性的全面评估，填补了当前研究的空白。

Results: 研究结果揭示了各系统在性能、兼容性和长期可行性之间的明显权衡；例如，某些系统在吞吐量上表现更优，而另一些则在延迟或资源效率方面更具优势。

Conclusion: 不同的Redis替代方案各有优劣，选择应基于具体应用场景对性能、兼容性和项目可持续性的需求。

Related Work: 现有研究多集中于传统数据库或单一系统的优化，缺乏对新兴内存键值存储系统的系统性实验比较，尤其是在云原生和容器化环境中的评估。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>
