{"id": "2602.15166", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15166", "abs": "https://arxiv.org/abs/2602.15166", "authors": ["Tanner Andrulis", "Michael Gilbert", "Vivienne Sze", "Joel S. Emer"], "title": "Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation", "comment": null, "summary": "The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.\n  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers."}
{"id": "2602.15172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15172", "abs": "https://arxiv.org/abs/2602.15172", "authors": ["Michael Gilbert", "Tanner Andrulis", "Vivienne Sze", "Joel S. Emer"], "title": "The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation", "comment": null, "summary": "The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.\n  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.\n  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\\%$ higher than optimal) even when given $1000\\times$ the runtime ($>10$ hours)."}
{"id": "2602.15336", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15336", "abs": "https://arxiv.org/abs/2602.15336", "authors": ["Yogeswar Reddy Thota", "Setareh Rafatirad", "Homayoun Houman", "Tooraj Nikoubin"], "title": "Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction."}
{"id": "2602.15388", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.15388", "abs": "https://arxiv.org/abs/2602.15388", "authors": ["Yonghao Wang", "Jiaxin Zhou", "Yang Yin", "Hongqin Lyu", "Zhiteng Chao", "Wenchao Ding", "Jing Ye", "Tiancheng Wang", "Huawei Li"], "title": "Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification", "comment": "6 pages, 6 figures", "summary": "While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage."}
{"id": "2602.15145", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.15145", "abs": "https://arxiv.org/abs/2602.15145", "authors": ["Sunjung Kang", "Vishrant Tripathi", "Christopher G. Brinton"], "title": "Exploring Performance Tradeoffs in Age-Aware Remote Monitoring with Satellites", "comment": null, "summary": "We investigate a remote monitoring framework with multiple sensing modalities including IoT sensors on the ground, mobile UAVs in the air, and a periodically available satellite constellation. While the IoT sensors cover small areas and remain fixed, the UAVs can move between locations and cover larger areas, and the satellites can observe the entire region but have high latency and low reliability. We divide the deployment region into cells and model it as a graph, with the nodes representing individual cells and edges representing possible UAV mobility patterns. To evaluate the freshness of collected information from this graph, we adopt the Age of Information (AoI) metric, measured separately for each cell. Under a given deployment of IoT nodes and UAV mobility patterns, our objective is to ascertain whether the system should actually utilize monitoring updates from satellites - a seemingly simple yet surprisingly elusive question. For stationary randomized scheduling policies, we develop closed-form expressions and lower bounds for the weighted-sum AoI and utilize this analysis to explore performance tradeoffs as system parameters vary. We also provide a Lyapunov style max-weight policy and detailed simulations that provide crucial insights for deploying such systems in practice."}
{"id": "2602.15204", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15204", "abs": "https://arxiv.org/abs/2602.15204", "authors": ["Kevin Garner", "Polykarpos Thomadakis", "Nikos Chrisochoides"], "title": "Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation", "comment": "52 pages, 19 figures, 13 tables", "summary": "This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software."}
{"id": "2602.15281", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15281", "abs": "https://arxiv.org/abs/2602.15281", "authors": ["Merve Saimler", "Mohaned Chraiti", "Ozgur Ercetin"], "title": "High-Fidelity Network Management for Federated AI-as-a-Service: Cross-Domain Orchestration", "comment": null, "summary": "To support the emergence of AI-as-a-Service (AIaaS), communication service providers (CSPs) are on the verge of a radical transformation-from pure connectivity providers to AIaaS a managed network service (control-and-orchestration plane that exposes AI models). In this model, the CSP is responsible not only for transport/communications, but also for intent-to-model resolution and joint network-compute orchestration, i.e., reliable and timely end-to-end delivery. The resulting end-to-end AIaaS service thus becomes governed by communications impairments (delay, loss) and inference impairments (latency, error). A central open problem is an operational AIaaS control-and-orchestration framework that enforces high fidelity, particularly under multi-domain federation. This paper introduces an assurance-oriented AIaaS management plane based on Tail-Risk Envelopes (TREs): signed, composable per-domain descriptors that combine deterministic guardrails with stochastic rate-latency-impairment models. Using stochastic network calculus, we derive bounds on end-to-end delay violation probabilities across tandem domains and obtain an optimization-ready risk-budget decomposition. We show that tenant-level reservations prevent bursty traffic from inflating tail latency under TRE contracts. An auditing layer then uses runtime telemetry to estimate extreme-percentile performance, quantify uncertainty, and attribute tail-risk to each domain for accountability. Packet-level Monte-Carlo simulations demonstrate improved p99.9 compliance under overload via admission control and robust tenant isolation under correlated burstiness."}
{"id": "2602.15356", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15356", "abs": "https://arxiv.org/abs/2602.15356", "authors": ["Patrick G. Bridges", "Derek Schafer", "Jack Lange", "James B. White", "Anthony Skjellum", "Evan Suggs", "Thomas Hines", "Purushotham Bangalore", "Matthew G. F. Dosanjh", "Whit Schonbein"], "title": "Co-Design and Evaluation of a CPU-Free MPI GPU Communication Abstraction and Implementation", "comment": null, "summary": "Removing the CPU from the communication fast path is essential to efficient GPU-based ML and HPC application performance. However, existing GPU communication APIs either continue to rely on the CPU for communication or rely on APIs that place significant synchronization burdens on programmers. In this paper we describe the design, implementation, and evaluation of an MPI-based GPU communication API enabling easy-to-use, high-performance, CPU-free communication. This API builds on previously proposed MPI extensions and leverages HPE Slingshot 11 network card capabilities. We demonstrate the utility and performance of the API by showing how the API naturally enables CPU-free gather/scatter halo exchange communication primitives in the Cabana/Kokkos performance portability framework, and through a performance comparison with Cray MPICH on the Frontier and Tuolumne supercomputers. Results from this evaluation show up to a 50% reduction in medium message latency in simple GPU ping-pong exchanges and a 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs of the Frontier supercomputer."}
{"id": "2602.15286", "categories": ["cs.NI", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15286", "abs": "https://arxiv.org/abs/2602.15286", "authors": ["Merve Saimler", "Mohaned Chraiti"], "title": "AI-Paging: Lease-Based Execution Anchoring for Network-Exposed AI-as-a-Service", "comment": null, "summary": "With AI-as-a-Service (AIaaS) now deployed across multiple providers and model tiers, selecting the appropriate model instance at run time is increasingly outside the end user's knowledge and operational control. Accordingly, the 6G service providers are envisioned to play a crucial role in exposing AIaaS in a setting where users submit only an intent while the network helps in the intent-to-model matching (resolution) and execution placement under policy, trust, and Quality of Service (QoS) constraints. The network role becomes to discover candidate execution endpoints and selects a suitable model/anchor under policy and QoS constraints in a process referred here to as AI-paging (by analogy to cellular call paging). In the proposed architecture, AI-paging is a control-plane transaction that resolves an intent into an AI service identity (AISI), a scoped session token (AIST), and an expiring admission lease (COMMIT) that authorizes user-plane steering to a selected AI execution anchor (AEXF) under a QoS binding. AI-Paging enforces two invariants: (i) lease-gated steering (without COMMIT, no steering state is installed) and (ii) make-before-break anchoring to support continuity and reliability of AIaaS services under dynamic network conditions. We prototype AI-Paging using existing control- and user-plane mechanisms (service-based control, QoS flows, and policy-based steering) with no new packet headers, ensuring compatibility with existing 3GPP-based exposure and management architectures, and evaluate transaction latency, relocation interruption, enforcement correctness under lease expiry, and audit-evidence overhead under mobility and failures."}
{"id": "2602.15379", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15379", "abs": "https://arxiv.org/abs/2602.15379", "authors": ["Zhihao Shu", "Md Musfiqur Rahman Sanim", "Hangyu Zheng", "Kunxiong Zhu", "Miao Yin", "Gagan Agrawal", "Wei Niu"], "title": "FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations", "comment": null, "summary": "The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs."}
{"id": "2602.15288", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.15288", "abs": "https://arxiv.org/abs/2602.15288", "authors": ["Merve Saimler", "Mohaned Chraiti"], "title": "AI Sessions for Network-Exposed AI-as-a-Service", "comment": null, "summary": "Cloud-based Artificial Intelligence (AI) inference is increasingly latency- and context-sensitive, yet today's AI-as-a-Service is typically consumed as an application-chosen endpoint, leaving the network to provide only best-effort transport. This decoupling prevents enforceable tail-latency guarantees, compute-aware admission control, and continuity under mobility. This paper proposes Network-Exposed AI-as-a-Service (NE-AIaaS) built around a new service primitive: the AI Session (AIS)-a contractual object that binds model identity, execution placement, transport Quality-of-Service (QoS), and consent/charging scope into a single lifecycle with explicit failure semantics. We introduce the AI Service Profile (ASP), a compact contract that expresses task modality and measurable service objectives (e.g., time-to-first-response/token, p99 latency, success probability) alongside privacy and mobility constraints. On this basis, we specify protocol-grade procedures for (i) DISCOVER (model/site discovery), (ii) AI PAGING (context-aware selection of execution anchor), (iii) two-phase PREPARE/COMMIT that atomically co-reserves compute and QoS resources, and (iv) make-before-break MIGRATION for session continuity. The design is standard-mappable to Common API Framework (CAPIF) style northbound exposure, ETSI Multi-access Edge Computing (MEC) execution substrates, 5G QoS flows for transport enforcement, and Network Data Analytics Function (NWDAF) style analytics for closed-loop paging/migration triggers."}
{"id": "2602.15794", "categories": ["cs.DC", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15794", "abs": "https://arxiv.org/abs/2602.15794", "authors": ["Boris Sedlak", "VÃ­ctor Casamayor Pujol", "Ildefons Magrans de Abril", "Praveen Kumar Donta", "Adel N. Toosi", "Schahram Dustdar"], "title": "Service Orchestration in the Computing Continuum: Structural Challenges and Vision", "comment": null, "summary": "The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC."}
