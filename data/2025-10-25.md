<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild提出的“通过自约简的轮次消除”技术，并利用该简化方法证明了随机LOCAL模型中最大b-匹配和边着色问题的轮次下界。


<details>
  <summary>Details</summary>
Motivation: 原证明技术虽然优美但过于复杂（超过25页），难以理解与推广。本文旨在简化该技术，推动对图问题复杂性的深入理解。

Challenges: 简化复杂的轮次下界证明技术，并将其推广到更广泛的问题（如b-匹配和边着色）中，同时保持证明的严谨性和有效性。

Contributions: 1. 提出了更简洁的轮次消除自约简技术；2. 给出了最大b-匹配问题的轮次下界；3. 证明了使用Δ+k种颜色进行边着色的随机算法的轮次下界。

Results: 1. 最大b-匹配问题在随机LOCAL模型中需要Ω(min{log_{1+b}Δ, log_Δ n})和Ω(√(log_{1+b} n))轮；2. 边着色问题需要Ω(min{log Δ, log_Δ n})和Ω(√(log n))轮。

Conclusion: 本文成功简化了轮次消除技术，并将其应用于多个经典问题，增强了我们对分布式图算法下界的理解。

Related Work: Khoury和Schild在FOCS 2025中首次提出通过自约简的轮次消除技术，证明了最大匹配问题的下界，本文在此基础上进行了简化和扩展。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [2] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 本文提出了AsyncHZP，一种新颖的异步ZeRO变体，通过自适应重分片和多流异步调度方法，在降低通信开销的同时优化内存利用，显著提升了大规模语言模型训练的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在训练效率和可扩展性方面面临瓶颈，主流并行化方法如ND并行复杂且不够灵活，而ZeRO等方法受限于通信开销。

Challenges: 如何在保持内存效率和系统简洁的同时，减少通信开销并有效重叠通信与计算。

Contributions: 1) 提出AsyncHZP，一种自适应重分片的异步ZeRO变体；2) 设计多流异步调度方法，实现通信与计算的有效重叠；3) 在Dense和MoE模型上验证了方法的高效性与稳定性。

Results: 实验表明，AsyncHZP在多种模型上优于经典ND并行，达到最先进的性能，且无需复杂调参，具备良好的可扩展性和稳定性。

Conclusion: AsyncHZP通过简化系统设计和优化通信效率，为高效的大规模语言模型训练提供了一条简洁有效的路径。

Related Work: 本文基于ZeRO和数据并行、模型并行等相关工作，提出改进方案，与传统ZeRO及ND并行方法密切相关。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [3] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: 本文提出了NCCLX，一个由Meta开发的高效集体通信框架，旨在优化大语言模型（LLM）全生命周期的性能，支持超过10万GPU的集群，显著提升了通信效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的扩大，传统通信方法在吞吐量和延迟方面面临严重限制，难以满足大规模训练和低延迟推理的需求。

Challenges: 在数十万GPU规模下，实现高吞吐、低延迟且可靠的通信极具挑战，尤其是在同步训练和实时推理的不同需求之间取得平衡。

Contributions: 提出并实现了NCCLX框架，专为超大规模LLM训练和推理优化，支持复杂工作负载下的高效数据交换。

Results: 在Llama4模型上的实证评估表明，NCCLX显著提升了通信效率，有效支持了超大规模模型的运行。

Conclusion: NCCLX为下一代大语言模型在前所未有的规模上运行提供了可靠且高效的通信解决方案。

Related Work: 相关工作包括NCCL、MPI等传统集体通信库，但这些方法在超大规模场景下性能受限，未能充分满足LLM训练与推理的双重需求。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [4] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 本文提出了一种HPC-QC全栈框架，实现了高性能计算与量子计算的集成，支持在传统HPC环境中调用量子内核，并展示了混合工作负载的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了满足高性能计算（HPC）与量子计算（QC）日益增长的融合需求，需要构建可扩展的统一计算框架。

Challenges: 如何实现HPC与QC的无缝集成，支持跨平台、设备无关的量子程序开发，并有效调度和执行混合工作负载。

Contributions: 提出了HPC-QC全栈框架，开发了量子编程接口库扩展、自适应电路拼接虚拟化层，以及基于Cray LLVM的可重定向编译架构。

Results: 在HPE EX超级计算机上成功演示了多个多节点、多CPU/GPU的混合HPC-QC工作负载，包括求解线性方程组、量子优化和量子相变模拟。

Conclusion: 该框架为构建统一的量子-经典编程环境提供了可行路径，基于现有HPC软件栈实现了量子计算的可移植和高效调用。

Related Work: 现有研究主要集中在独立的量子编程框架或简单HPC-QC接口，缺乏完整的全栈集成与跨平台支持。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [5] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: 本文提出了一种名为FLAS的自适应自动伸缩系统，结合了预测性和反应性方法的优点，用于分布式服务的弹性资源管理，特别适用于基于内容的发布-订阅中间件，并在多种测试场景下验证了其在99%以上的时间内满足性能需求的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于云计算的弹性特性支撑了大多数新兴技术的发展，自动伸缩系统成为保障服务质量的关键。然而，现有方法在应对动态负载变化时存在响应滞后或过度配置的问题，因此需要一种更智能、自适应的伸缩机制。

Challenges: 主要挑战包括如何准确预测高层性能指标（如响应时间、吞吐量）的变化趋势，以及如何在减少系统侵入性的同时实现对不同应用的通用适应能力。此外，在极端负载条件下仍需保证SLA合规性。

Contributions: 本文的主要贡献包括：(1) 提出FLAS自动伸缩框架，融合预测与反应机制；(2) 设计基于资源使用指标估算高层性能指标的预测模型，降低监控开销；(3) 实现对内容发布-订阅系统的首个专用自动伸缩方案，且具备通用性；(4) 采用边界值分析法进行全面评估。

Results: 实验结果表明，FLAS在多种测试场景下（包括预期使用环境和最坏情况）均能有效维持性能要求，在99%以上的时间内满足SLA指标，验证了其鲁棒性和有效性。

Conclusion: FLAS通过结合预测与反应机制，实现了高效、低侵入、自适应的自动伸缩，特别适用于事件驱动架构中的分布式服务，在保障性能的同时提升了资源利用率。

Related Work: 相关工作主要集中在基于阈值的反应式伸缩和基于负载预测的主动式伸缩方法。已有研究多单独采用其中一种策略，缺乏根据运行时情况动态切换或融合两者优势的机制，而FLAS正是在此基础上提出了一种混合自适应方案。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [6] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出了一种自动构建和评估多种性能预测器的方法，以在资源受限的动态边缘环境中实现高精度且低推理延迟的应用性能预测，特别适用于电子显微镜工作流。


<details>
  <summary>Details</summary>
Motivation: 在动态边缘环境中，由于应用共置和节点异构性，应用性能预测具有挑战性，而准确的预测对资源调度和管理至关重要。

Challenges: 应用之间的资源竞争、节点异构性以及动态共置环境导致性能预测困难，同时需要兼顾预测准确性和推理延迟。

Contributions: 提出了一种系统化的方法，能够为不同服务器选择最优的性能预测器，兼顾准确率和推理时间，并在真实电子显微镜工作流中验证了方法的有效性。

Results: 预测器最高达到90%的准确率，推理时间低于往返时间（RTT）的1%，显著优于传统方法。

Conclusion: 通过联合优化准确性和推理延迟，所提出的预测器能有效提升边缘环境中的资源利用率和性能可预测性。

Related Work: 相关工作包括基于监控指标的性能预测、边缘计算中的资源调度以及机器学习在系统性能建模中的应用。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [7] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文研究了利用往返时间（RTT）预测器来改进分布式应用中的请求路由，通过在Kubernetes管理的GPU集群上收集的时间序列监控数据训练轻量级且准确的RTT预测模型，实现了高达95%的预测精度，并将预测延迟控制在应用RTT的10%以内。


<details>
  <summary>Details</summary>
Motivation: 传统负载均衡策略通常是反应式的，依赖过时或粗粒度的指标，导致路由决策次优和尾部延迟增加。为了降低端到端延迟，特别是在资源受限的边缘和云环境中，需要更主动、精准的负载均衡方法。

Challenges: 如何在保持低开销的同时实现高精度的RTT预测；如何适应不同共置场景和异构硬件环境；确定有效部署预测器所需的最低预测精度阈值和关键系统因素。

Contributions: 提出了基于时间序列监控数据的轻量级RTT预测器，使用高度相关的少量指标实现高效预测；确定了确保预测器有效部署的关键系统级因素和最低精度要求；通过模拟验证了性能感知负载均衡可显著降低应用RTT并减少资源浪费。

Results: 预测器达到高达95%的准确性，预测延迟控制在应用RTT的10%以内；仿真表明性能感知负载均衡能显著降低应用往返时间并减少资源浪费。

Conclusion: 将预测性负载均衡集成到未来生产系统中是可行的，能够有效提升分布式应用的性能表现。

Related Work: 相关工作包括传统的基于负载的路由算法、反应式与主动式负载均衡机制、基于机器学习的性能预测模型以及在容器化和GPU集群环境下的资源调度优化研究。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 本文提出了一种新的近似加法器，相较于现有加法器在能效和面积效率上更优，同时保持了较高的计算精度，并通过图像处理任务验证了其在高质量图像重建中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提升计算密集型多媒体应用（如图像、音频、视频处理）中硬件的能效，研究者广泛探索了近似加法器的设计。

Challenges: 如何在高性能、高计算精度和高能效之间取得良好平衡是近似加法器设计的主要挑战。

Contributions: 提出了一种新型近似加法器，具有更高的能效和面积效率，同时精度优于或相当于现有加法器，并通过仿真和图像处理应用验证了其有效性。

Results: 仿真结果表明，所提出的加法器在能效和面积方面优于现有设计，同时保持了良好的计算精度；在图像处理任务中能够高质量地重建图像。

Conclusion: 该新型近似加法器在多媒体应用中具有良好的应用前景，兼顾了能效、面积和精度。

Related Work: 已有多种静态和动态近似加法器被提出，致力于在性能、精度和能效之间进行权衡。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [9] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本研究通过在96个商用DDR4 DRAM芯片上广泛表征，利用同时多行激活（SiMRA）技术，在COTS DRAM芯片中实现了高吞吐量、低延迟的真随机数生成。所有基于SiMRA的真随机数生成器（TRNG）设计均通过了NIST随机性统计测试，且在吞吐量上优于现有最先进的DRAM-based TRNG。熵值随同时激活的行数增加而增加，但受数据模式和温度等操作条件显著影响。研究还开源了相关基础设施以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 真随机数生成（TRNG）在密码学和安全领域至关重要，但现有方案在成本、吞吐量或可扩展性方面存在局限。本文旨在探索利用广泛使用的商用DRAM芯片中的物理现象（SiMRA）来构建高效、低成本的TRNG。

Challenges: 如何在商用DRAM中稳定激发并利用SiMRA效应生成高质量随机性；确保生成的随机数通过严格的统计测试；优化吞吐量与延迟；分析不同参数（如激活行数、温度、数据模式）对熵的影响。

Contributions: 1）首次在96个真实DDR4芯片上系统表征SiMRA用于TRNG的潜力；2）提出并验证了多种基于不同同时激活行数的TRNG设计；3）揭示了SiMRA熵与行数、温度、数据模式之间的关系；4）开源实验基础设施以促进后续研究。

Results: 1）所有SiMRA-TRNG设计均通过NIST随机性测试；2）8行激活设计吞吐量最高，较现有最优DRAM-TRNG提升1.99倍；3）熵值随同时激活行数增加而上升，32行激活的平均熵是2行的2.51倍；4）温度升高（50°C至90°C）使32行激活的熵降低1.53倍。

Conclusion: SiMRA是一种在商用DRAM中实现高性能、高熵真随机数生成的有效机制，具备良好的统计特性和可扩展性，其性能受操作条件影响显著，未来可通过优化参数进一步提升。

Related Work: 已有研究利用DRAM的行冲突、激活延迟变化等物理特性构建TRNG，但通常吞吐量较低或依赖特殊硬件。本文不同于以往工作，首次系统探索同时多行激活（SiMRA）作为熵源，并在大量真实芯片上验证其有效性与可扩展性。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [10] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire是一种通用加速器，旨在有效利用数据依赖性受限内核中的细粒度并行性，通过每个核心集成一个低功耗、顺序执行的核心组，实现快速通信和直接L2缓存访问，在动态规划内核中达到最高7.64倍的加速比，整体端到端应用加速3.66倍，并降低最多56%的能耗，仅增加10.5%的面积开销。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD和GPGPU）在处理复杂数据依赖模式时难以高效利用细粒度并行性，而FPGA和ASIC虽性能高但设计复杂且缺乏灵活性，因此需要一种兼顾性能、能效与可编程性的通用加速方案。

Challenges: 如何在保持低硬件和编程复杂度的同时，有效支持具有复杂数据依赖关系的计算密集型内核，并实现高效的细粒度并行执行与低开销的数据共享和同步。

Contributions: 提出Squire架构，其创新包括：每个多核系统核心配备一个专用Squire加速器，包含多个低功耗顺序核心，支持快速互连和直接L2缓存访问；可在最小软件改动下加速依赖受限内核；在五个复杂依赖内核上验证，并构建了端到端读映射工具进行评估。

Results: 在动态规划内核上实现最高7.64倍的速度提升；端到端应用加速达3.66倍；能耗降低最多56%；硬件面积仅增加10.5%（相对于Neoverse-N1基线）。

Conclusion: Squire在性能、能效和面积之间取得了良好平衡，能够有效加速具有复杂依赖关系的HPC应用，同时保持较高的可编程性和系统集成便利性，是一种有前景的通用加速器设计方案。

Related Work: 相关工作包括基于SIMD和GPGPU的通用加速器，以及针对特定领域定制的FPGA和ASIC解决方案；Squire结合了两者的优点，既避免了前者的并行性利用不足，又克服了后者的高设计成本和低灵活性问题。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>
