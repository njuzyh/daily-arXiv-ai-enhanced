<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor是一种高效的随机去中心化调度器，利用批处理缓存信息和新型负载评分机制，在减少通信开销的同时提升现代数据中心的任务调度性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心需要高效、低通信开销的去中心化任务调度方案，以应对异构集群中动态多维资源需求的挑战。

Challenges: 如何在不依赖实时探测的情况下准确评估服务器负载，并在异构环境中有效调度具有多维资源需求的任务。

Contributions: 提出了Dodoor调度器，引入基于批处理的缓存更新机制和新的负载评分方法，能够更好地反映任务与服务器之间的反亲和性。

Results: 在101节点异构集群上，Dodoor减少了55-66%的调度消息，吞吐量提升达33.2%和21.5%，平均makespan延迟降低12.1%和7.2%，尾部延迟改善21.9%和24.6%。

Conclusion: Dodoor通过批处理缓存和新型负载评分机制，在降低通信开销的同时显著提升了调度效率和系统性能，适用于大规模异构数据中心。

Related Work: 基于加权球盒模型和批量设置的去中心化调度研究，以及传统的基于待处理任务数的负载均衡方法。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [2] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 本文提出了一种新的分布式集成系统框架FDIRS，通过采用异构分布式数据库技术，提升了系统的性能、响应速度、效率和可靠性，并解决了以往框架的一些问题。


<details>
  <summary>Details</summary>
Motivation: 为了提高集成分布式系统的满意度和性能，解决现有框架存在的问题。

Challenges: 集成系统的复杂性、性能瓶颈、响应速度慢以及可靠性不足是主要挑战。

Contributions: 提出了FDIRS新框架，采用了异构分布式数据库技术，显著提升了系统性能和可靠性，并通过仿真验证了其优越性。

Results: 仿真结果表明，FDIRS框架在性能、响应速度、效率和可靠性方面均优于已有框架，有效解决了之前框架的问题。

Conclusion: FDIRS框架能够有效提升集成分布式系统的整体性能和可靠性，具有实际应用价值。

Related Work: 文中简要分析了集成系统的发展过程以及ERPSD和ERPDRT框架，作为新框架设计的基础。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [3] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe是一个针对解耦式大语言模型服务的动态编排框架，通过细粒度资源迁移和全局缓存共享，解决了静态分配、负载不均和缓存感知路由导致的热点问题，显著提升了吞吐量并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的解耦式LLM服务系统因静态资源分配、计算与内存负载不均衡以及缓存感知路由引发的负载倾斜，导致资源浪费或违反SLO，亟需一种动态、高效的资源调度机制。

Challenges: 1) 静态资源分配难以适应动态工作负载；2) 预填充（计算密集）与解码（内存密集）阶段存在固有的负载不平衡；3) 前缀缓存感知路由导致高命中节点过载，加剧负载不均。

Contributions: 提出了BanaServe框架，引入了层级别权重迁移、注意力级别KV缓存迁移以及带层间重叠传输的全局KV缓存共享机制，实现了粗粒度与细粒度的动态负载重平衡，并解耦了路由决策与缓存位置的依赖。

Results: 相比vLLM，BanaServe实现了1.2倍至3.9倍的更高吞吐量，总处理时间减少3.9%至78.4%；相比DistServe，吞吐量提升1.1倍至2.8倍，延迟降低1.4%至70.1%。

Conclusion: BanaServe通过动态资源编排和创新的缓存管理机制，有效解决了当前解耦式LLM服务中的资源利用率低和负载不均问题，显著提升了系统性能和效率。

Related Work: 相关工作包括vLLM和DistServe等解耦式LLM服务系统，它们虽实现了预填充与解码的分离，但在动态资源调度和缓存负载均衡方面存在局限，BanaServe在此基础上进行了改进。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [4] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出了首个用于寻找图中最大权重独立集的分布式内存并行化约算法，并设计了分布式reduce-and-greedy与reduce-and-peel启发式算法，能够在超大规模图上实现良好的可扩展性和较高的求解速度，支持处理超过十亿顶点和170亿边的图。


<details>
  <summary>Details</summary>
Motivation: 由于最大权重独立集问题是NP难问题，现有方法受限于顺序处理的规模和效率，难以应对超大规模图。因此，需要设计高效的分布式算法以提升处理能力和求解速度。

Challenges: 主要挑战包括如何在分布式环境下高效实现数据约简规则、保持良好的约简效果、协调多处理器间的异步操作，以及在提升速度的同时尽可能保证解的质量。

Contributions: 1) 首个用于最大权重独立集问题的分布式内存并行约简算法；2) 首个分布式reduce-and-greedy和reduce-and-peel启发式算法；3) 实现了对超过十亿顶点和170亿边的大规模图的处理能力；4) 实验显示良好的加速比和可扩展性。

Results: 在最多1024个处理器上的实验表明，异步reduce-and-peel方法相比顺序算法平均加速33倍，解质量接近；reduce-and-greedy方法最高加速达50倍，但解质量略低。算法具有良好可扩展性和约简效果。

Conclusion: 所提出的分布式算法显著提升了最大权重独立集问题的求解效率和规模，为处理超大规模图提供了有效工具，同时在速度与解质量之间提供了良好权衡。

Related Work: 相关工作主要包括针对该问题的各类顺序精确算法和启发式方法，这些方法广泛使用数据约简规则来缩小问题规模，但受限于单机处理能力，无法扩展到超大规模图。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [5] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: 本文提出了ARES，一种基于长度预测的自适应解码重调度系统，通过轻量级LLM原生预测方法和动态负载均衡机制，显著提升了长输出推理任务的性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 由于输出长度变化导致解码阶段工作负载不平衡，现有静态预填充-解码调度方法在面对动态解码负载时容易引发SLO违规和内存溢出问题。

Challenges: 准确预测LLM生成的剩余长度、在低开销下实现高精度预测、在解码阶段动态平衡工作负载以避免资源瓶颈。

Contributions: 1) 提出一种利用LLM隐藏状态的轻量级连续长度预测方法，显著降低预测误差和参数开销；2) 设计一种结合当前与预测负载的动态重调度机制，有效改善P99 TPOT和整体吞吐量。

Results: 相比现有方法，长度预测的MAE降低了49.42%，预测器参数减少93.28%；解码阶段P99 TPOT降低74.77%，最高可提升2.24倍goodput。

Conclusion: ARES通过精准的长度预测和动态重调度，在不增加显著开销的前提下，有效缓解了LLM推理中的解码负载不均问题，显著提升了服务质量和系统效率。

Related Work: 与PD分离架构及静态prefill-to-decode调度相关的研究，以及早期关于LLM推理调度和资源管理的工作。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [6] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST是一个在分布式高性能计算集群上实现推理即服务的框架，通过OpenAI兼容API提供对多种AI模型的安全、私有和可扩展的推理访问。


<details>
  <summary>Details</summary>
Motivation: 满足科学工作流中对私有、安全和可扩展AI推理日益增长的需求，避免依赖商业云基础设施。

Challenges: 如何在多个异构HPC集群上实现低延迟、高吞吐的联邦推理调度，并支持自动扩缩容与安全认证。

Contributions: 提出FIRST框架，集成Globus Auth与Globus Compute，实现跨集群的统一API访问、多后端支持、资源自动扩展和热节点维持。

Results: 系统支持每日数十亿token的本地推理，提供批量与交互两种模式，实现在私有环境中高效并行执行LLM推理任务。

Conclusion: FIRST成功实现了在现有HPC设施上安全、高效的联邦推理服务，为科研人员提供了类云体验的本地AI推理能力。

Related Work: 相关工作包括分布式推理系统、HPC上的AI服务部署以及基于API的模型访问框架，但缺乏对多集群联邦调度和安全私有化支持的整合。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [7] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文研究了在同步和异步分布式系统中，具有二元输出的任务可解性的充要条件，提出了基于输出集的统一分析框架，涵盖了二元共识和对称性破缺等问题，并给出了适用于更强任务定义的不可能性证明。


<details>
  <summary>Details</summary>
Motivation: 为了统一分析分布式系统中具有二元输出的任务的可解性，特别是当部分进程可能发生崩溃时，需要明确系统参数（如进程数n和容错数t）之间的关系。

Challenges: 在存在最多t个进程崩溃的情况下，确定在何种n和t条件下，所有二元输出任务在同步和异步系统中均可解；同时要处理输出集、有效性、值多重性等不同任务变体的复杂性。

Contributions: 提供了n和t的紧致条件，完全刻画了二元输出任务在同步和异步系统中的可解性；提出输出集视角，统一了多个经典问题（如二元共识、对称性破缺），并导出了适用于更强任务形式的不可能性结果。

Results: 给出了在同步和异步系统中，所有二元输出任务可解的充要条件；证明了在某些n和t条件下任务不可解，并展示了这些不可能性结果如何推广到更复杂的任务定义。

Conclusion: 通过引入输出集的概念，本文建立了二元输出任务可解性的完整理论框架，揭示了系统规模与容错能力之间的本质关系，并为更广泛的任务类提供了统一的分析工具。

Related Work: 与二元共识、共识问题的不可能性结果（如FLP）、对称性破缺问题以及分布式任务可解性的一般理论（如色单纯形法）密切相关。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文提出了一种名为D-com的加速器，通过采用渐进式分解算法、Lanczos算法和协同加速器架构，结合计算复制和输出形状保持计算方案，显著降低了大模型的端到端延迟，同时保持较高的模型精度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模的快速增长，计算和内存成本急剧上升，传统的低秩分解技术因运行时分解带来的额外延迟而受限，因此需要一种更高效的模型压缩方法来平衡性能与精度。

Challenges: 主要挑战包括：分解操作的内存瓶颈导致性能受限；连续层间的分解开销累积；模型压缩带来的精度损失；以及现有分解方法在实际硬件上的延迟过高。

Contributions: 1) 采用Lanczos算法与渐进式分解算法；2) 设计了面向分解算法的协同加速器架构；3) 提出计算复制方法，将内存密集型操作转为计算密集型，实现6.2倍加速；4) 开发输出形状保持的计算方案，消除连续层的分解开销；5) 提出多轨道分解方法，在保持高精度的同时控制计算成本。

Results: D-com加速器相比A100 GPU实现了22%的端到端延迟降低，在AI2推理挑战任务中仅带来3%的模型质量下降，并在Llama2-7b模型上验证了分解策略的有效性。

Conclusion: 通过算法-硬件协同设计，输入分解可以在实际系统中带来显著性能提升，D-com为大模型压缩提供了高效且实用的解决方案。

Related Work: 相关工作主要集中在权重分解和低秩近似技术，如SVD分解、LoRA等，但这些方法多避免运行时激活分解以减少延迟，本文则通过算法优化与硬件支持重新评估并提升了输入分解的可行性与效益。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [9] [Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks](https://arxiv.org/abs/2510.13362)
*Angelos Athanasiadis,Nikolaos Tampouratzis,Ioannis Papaefstathiou*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA的高效CNN计算框架，利用Darknet设计输入，在保持全精度的同时实现高性能和能效，适用于CPU-FPGA异构系统。


<details>
  <summary>Details</summary>
Motivation: 传统处理器在性能、功耗和延迟之间难以平衡，尤其在边缘计算和嵌入式系统中，而FPGA因其高能效和可重构性成为有前景的替代方案。

Challenges: 在FPGA上高效实现全精度CNN计算面临复杂性和资源消耗的挑战，同时需保持与量化方案相当的性能和能效。

Contributions: 提出了一种基于Darknet的FPGA框架，支持全精度CNN部署，可在CPU-FPGA异构系统中高效运行，并在不牺牲精度的前提下实现与量化方案相当的性能和能效。

Results: 该框架在保持全精度的同时，实现了与现有量化FPGA框架相似的性能和能效，验证了其在边缘AI应用中的可行性。

Conclusion: 所提框架为在FPGA上部署高精度CNN提供了一种有效解决方案，兼顾准确性、性能与能效，适用于实时AI应用。

Related Work: 已有研究多采用量化技术降低FPGA上CNN的计算开销，但可能牺牲模型精度；Darknet作为主流CNN设计工具，尚未充分用于FPGA异构系统优化。

Abstract: The growing demand for real-time processing in artificial intelligence
applications, particularly those involving Convolutional Neural Networks
(CNNs), has highlighted the need for efficient computational solutions.
Conventional processors, very often, fall short in balancing performance, power
consumption, and latency, especially in embedded systems and edge computing
platforms. Field-Programmable Gate Arrays (FPGAs) offer a promising
alternative, combining high performance with energy efficiency and
reconfigurability. The presented framework addresses the complex and demanding
computations of CNNs on FPGAs maintaining full precision in all neural network
parameters. Specifically, our framework is based on Darknet which is very
widely used for the design of CNNs and allows the designer, by using a similar
input to that given to Darknet, to efficiently implement a CNN in a
heterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA
frameworks that support quantization, our solution aims to offer similar
performance and/or energy efficiency without any degradation on the NN
accuracy.

</details>


### [10] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 本文提出了一种灵活的块浮点量化加速器F-BFQ，能够在不同BFP变体间动态切换，以加速边缘设备上量化大语言模型的推理，实验结果显示其在AMD Kria板上相比ARM NEON CPU平均提升1.4倍推理速度。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的边缘设备上高效部署大语言模型，需要利用量化技术降低模型计算和存储开销，而现有加速器难以支持混合BFP量化下的多变体动态切换需求。

Challenges: 支持混合BFP量化的LLM在不同层使用不同的BFP变体，传统加速器需频繁重新配置，导致效率低下；如何实现无需重配置即可动态切换BFP变体的硬件加速是一大挑战。

Contributions: 提出了F-BFQ加速器架构，支持两种BFP量化变体的动态切换，并集成于AMD Kria板实现高效矩阵乘法运算，提升了BFP量化LLM的推理效率。

Results: 在三个BFP量化LLM上的实验表明，F-BFQ加速器相比Arm NEON CPU平均推理速度提升1.4倍，达到每秒5.2个token（约3.9个单词）。

Conclusion: F-BFQ加速器有效解决了BFP量化LLM在边缘设备上跨层变体切换的效率问题，为低精度量化模型的高效硬件加速提供了可行方案。

Related Work: 相关工作包括llama.cpp等LLM推理框架，以及块浮点（BFP）量化技术在模型压缩与边缘部署中的应用。

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: 本文提出了一种新的概率性事件排序方法“likely-happened-before”（记作→p），通过统计模型利用带有噪声的时间戳来实现更公平的事件排序，解决了传统时钟同步限制下的公平排序难题。


<details>
  <summary>Details</summary>
Motivation: 由于传统时钟同步存在固有误差，难以实现真正公平的事件排序，而越来越多的应用需要“公平排序”，即一个客户端早生成的事件应优先于其他客户端的后续事件被处理。因此，需要一种能容忍时钟偏差的新排序机制。

Challenges: 主要挑战包括：如何在无真实墙钟时间的情况下比较两个带噪声的时间戳；所提出的→p关系不具备传递性，导致排序复杂化；维持高概率排序的同时保证系统效率与可扩展性。

Contributions: 提出了“likely-happened-before”（→p）这一新关系，基于统计模型对带噪声时间戳进行概率性比较；设计了Tommy系统，利用每台时钟的偏移分布建模来判断事件顺序；为解决并发与公平性问题提供了新的理论基础和研究方向。

Results: 初步统计模型能够在没有真实墙钟时间的情况下，计算一个事件先于另一个事件发生的概率，从而为基础的happened-before关系提供补充，有效识别并排序原本被视为并发的事件。

Conclusion: 通过引入概率性排序关系→p，本文为分布式系统中的公平事件排序提供了一种新思路，展示了利用时钟变异性而非消除它的可行性，并为未来研究如在线公平排序、随机公平全序等指明了方向。

Related Work: 与Lamport的happened-before关系、向量时钟、物理时钟同步（如NTP）以及因果一致性等研究密切相关，但不同于这些确定性方法，本文采用概率性建模处理时钟不确定性。

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


### [12] [Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error](https://arxiv.org/abs/2510.13732)
*Mohd Saif Ali Khan,Karthik RM,Samar Agnihotri*

Main category: cs.NI

TL;DR: 提出两种动态可扩展的导频分配策略，以减少分布式大规模MIMO系统中的导频污染，提升信道估计质量和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 导频污染是限制分布式大规模MIMO系统性能的主要瓶颈，现有方法难以兼顾复杂度、可扩展性和估计精度。

Challenges: 如何在降低导频污染的同时，实现低复杂度、无需全局协调的可扩展导频分配。

Contributions: 1）提出一种低复杂度的集中式导频分配算法，通过顺序分配最小化全局信道估计误差；2）设计一种完全分布式的优先级导频选择算法，仅依赖局部信息即可有效减少干扰和导频污染。

Results: 数值仿真表明，所提方案在系统吞吐量方面优于现有的先进基准方案。

Conclusion: 所提出的两种导频分配策略在降低导频污染、提升频谱效率方面具有显著优势，且分布式方案具备低信令开销和动态适应能力，适合实际部署。

Related Work: 现有研究主要集中在静态导频分配或需要全局信息的优化方法，缺乏对动态、可扩展和低复杂度方案的综合考虑。

Abstract: Pilot contamination remains a major bottleneck in realizing the full
potential of distributed massive MIMO systems. We propose two dynamic and
scalable pilot assignment strategies designed for practical deployment in such
networks. First, we present a low complexity centralized algorithm that
sequentially assigns pilots to user equipments (UEs) to minimize the global
channel estimation errors across serving access points (APs). This improves the
channel estimation quality and reduces interference among UEs, enhancing the
spectral efficiency. Second, we develop a fully distributed algorithm that uses
a priority-based pilot selection approach. In this algorithm, each selected AP
minimizes estimation error using only local information and offers candidate
pilots to the UEs. Every UE then selects a suitable pilot based on AP priority.
This approach ensures consistency and minimizes interference while
significantly reducing pilot contamination. The method requires no global
coordination, maintains low signaling overhead, and adapts dynamically to the
UE deployment. Numerical simulations demonstrate the superiority of our
proposed schemes in terms of network throughput when compared to other
state-of-the-art benchmark schemes.

</details>
