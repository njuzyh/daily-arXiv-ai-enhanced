{"id": "2510.14172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.14172", "abs": "https://arxiv.org/abs/2510.14172", "authors": ["Yuchao Su", "Srikar Chundury", "Jiajia Li", "Frank Mueller"], "title": "DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation", "comment": null, "summary": "Hamiltonian simulation is a key workload in quantum computing, enabling the\nstudy of complex quantum systems and serving as a critical tool for classical\nverification of quantum devices. However, it is computationally challenging\nbecause the Hilbert space dimension grows exponentially with the number of\nqubits. The growing dimensions make matrix exponentiation, the key kernel in\nHamiltonian simulations, increasingly expensive. Matrix exponentiation is\ntypically approximated by the Taylor series, which contains a series of matrix\nmultiplications. Since Hermitian operators are often sparse, sparse matrix\nmultiplication accelerators are essential for improving the scalability of\nclassical Hamiltonian simulation. Yet, existing accelerators are primarily\ndesigned for machine learning workloads and tuned to their characteristic\nsparsity patterns, which differ fundamentally from those in Hamiltonian\nsimulations that are often dominated by structured diagonals.\n  In this work, we present \\name, the first diagonal-optimized quantum\nsimulation accelerator. It exploits the diagonal structure commonly found in\nproblem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic\narray dataflow to transform diagonally sparse matrices into dense computations,\nenabling high utilization and performance. Through detailed cycle-level\nsimulation of diverse benchmarks in HamLib, \\name{} demonstrates average\nperformance improvements of $10.26\\times$, $33.58\\times$, and $53.15\\times$\nover SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak\nspeedups up to $127.03\\times$ while reducing energy consumption by an average\nof $471.55\\times$ and up to $4630.58\\times$ compared to SIGMA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u91cf\u5b50\u6a21\u62df\u4e2d\u54c8\u5bc6\u987f\u91cf\u77e9\u9635\u5bf9\u89d2\u7ebf\u7ed3\u6784\u4f18\u5316\u7684\u65b0\u578b\u52a0\u901f\u5668\\name\uff0c\u901a\u8fc7\u91cd\u6784\u8109\u52a8\u9635\u5217\u6570\u636e\u6d41\uff0c\u5c06\u7a00\u758f\u5bf9\u89d2\u77e9\u9635\u8f6c\u5316\u4e3a\u5bc6\u96c6\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ecf\u5178\u54c8\u5bc6\u987f\u6a21\u62df\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u3002", "motivation": "\u7531\u4e8e\u54c8\u5bc6\u987f\u6a21\u62df\u4e2d\u7684\u77e9\u9635\u7ef4\u5ea6\u968f\u91cf\u5b50\u6bd4\u7279\u6570\u6307\u6570\u589e\u957f\uff0c\u77e9\u9635\u6307\u6570\u8fd0\u7b97\u6210\u672c\u6781\u9ad8\uff1b\u73b0\u6709\u7a00\u758f\u77e9\u9635\u52a0\u901f\u5668\u4e3b\u8981\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u91cf\u5b50\u6a21\u62df\u4e2d\u5e38\u89c1\u7684\u7ed3\u6784\u5316\u5bf9\u89d2\u7a00\u758f\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u7528\u4f18\u5316\u67b6\u6784\u3002", "challenges": "\u5982\u4f55\u9ad8\u6548\u5904\u7406\u54c8\u5bc6\u987f\u77e9\u9635\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7ed3\u6784\u5316\u5bf9\u89d2\u7a00\u758f\u6027\uff1b\u5728\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u7a00\u758f\u77e9\u9635\u8fd0\u7b97\u8f6c\u5316\u4e3a\u9ad8\u5229\u7528\u7387\u7684\u5bc6\u96c6\u8ba1\u7b97\uff1b\u8bbe\u8ba1\u9002\u914d\u6b64\u7c7b\u7ed3\u6784\u7684\u9ad8\u6548\u6570\u636e\u6d41\u4e0e\u786c\u4ef6\u67b6\u6784\u3002", "contributions": "\\name\u662f\u9996\u4e2a\u9488\u5bf9\u5bf9\u89d2\u7ed3\u6784\u4f18\u5316\u7684\u91cf\u5b50\u6a21\u62df\u52a0\u901f\u5668\uff1b\u63d0\u51fa\u91cd\u6784\u7684\u8109\u52a8\u6570\u7ec4\u6570\u636e\u6d41\uff0c\u5c06\u5bf9\u89d2\u7a00\u758f\u77e9\u9635\u8f6c\u6362\u4e3a\u5bc6\u96c6\u8ba1\u7b97\uff1b\u5728HamLib\u57fa\u51c6\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe127\u500d\u7684\u6027\u80fd\u63d0\u5347\u548c\u8d85\u8fc74600\u500d\u7684\u80fd\u6548\u6539\u8fdb\u3002", "results": "\\name\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u8f83SIGMA\u3001\u5916\u79ef\u6cd5\u548cGustavson\u7b97\u6cd5\u5206\u522b\u63d0\u534710.26\u500d\u300133.58\u500d\u548c53.15\u500d\uff0c\u6700\u9ad8\u52a0\u901f\u8fbe127.03\u500d\uff1b\u80fd\u91cf\u6d88\u8017\u5e73\u5747\u964d\u4f4e471.55\u500d\uff0c\u6700\u9ad8\u8fbe4630.58\u500d\u3002", "conclusion": "\\name\u901a\u8fc7\u5229\u7528\u54c8\u5bc6\u987f\u77e9\u9635\u7684\u5bf9\u89d2\u7ed3\u6784\u7279\u5f81\u548c\u5b9a\u5236\u5316\u786c\u4ef6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ecf\u5178\u54c8\u5bc6\u987f\u6a21\u62df\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u8bbe\u5907\u7684\u9a8c\u8bc1\u4e0e\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u5de5\u5177\u3002", "related_work": "SIGMA\u3001\u5916\u79ef\u6cd5\u548cGustavson\u7b97\u6cd5\u662f\u5f53\u524d\u7528\u4e8e\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u672a\u9488\u5bf9\u54c8\u5bc6\u987f\u77e9\u9635\u7684\u5bf9\u89d2\u4e3b\u5bfc\u7279\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u5bfc\u81f4\u5728\u91cf\u5b50\u6a21\u62df\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002"}}
{"id": "2510.14379", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.14379", "abs": "https://arxiv.org/abs/2510.14379", "authors": ["Ming-Han Lin", "Tian-Sheuan Chang"], "title": "Computing-In-Memory Aware Model Adaption For Edge Devices", "comment": "9 pages", "summary": "Computing-in-Memory (CIM) macros have gained popularity for deep learning\nacceleration due to their highly parallel computation and low power\nconsumption. However, limited macro size and ADC precision introduce throughput\nand accuracy bottlenecks. This paper proposes a two-stage CIM-aware model\nadaptation process. The first stage compresses the model and reallocates\nresources based on layer importance and macro size constraints, reducing model\nweight loading latency while improving resource utilization and maintaining\naccuracy. The second stage performs quantization-aware training, incorporating\npartial sum quantization and ADC precision to mitigate quantization errors in\ninference. The proposed approach enhances CIM array utilization to 90\\%,\nenables concurrent activation of up to 256 word lines, and achieves up to 93\\%\ncompression, all while preserving accuracy comparable to previous methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u8ba1\u7b97\u5185\u5b58\uff08CIM\uff09\u611f\u77e5\u6a21\u578b\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u538b\u7f29\u3001\u8d44\u6e90\u91cd\u5206\u914d\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86CIM\u9635\u5217\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u7531\u4e8eCIM\u5b8f\u7684\u5c3a\u5bf8\u9650\u5236\u548cADC\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u4e2d\u5b58\u5728\u541e\u5410\u91cf\u548c\u7cbe\u5ea6\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ecCIM\u5b8f\u5c3a\u5bf8\u6709\u9650\u5bfc\u81f4\u7684\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3001ADC\u7cbe\u5ea6\u4e0d\u8db3\u5f15\u53d1\u7684\u91cf\u5316\u8bef\u5dee\uff0c\u4ee5\u53ca\u6a21\u578b\u538b\u7f29\u4e0e\u52a0\u901f\u4e4b\u95f4\u7684\u7cbe\u5ea6\u5e73\u8861\u95ee\u9898\u3002", "contributions": "1\uff09\u63d0\u51fa\u4e24\u9636\u6bb5CIM\u611f\u77e5\u6a21\u578b\u81ea\u9002\u5e94\u65b9\u6cd5\uff1b2\uff09\u7ed3\u5408\u5c42\u91cd\u8981\u6027\u4e0e\u786c\u4ef6\u7ea6\u675f\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u4e0e\u8d44\u6e90\u91cd\u5206\u914d\uff1b3\uff09\u5f15\u5165\u90e8\u5206\u548c\u91cf\u5316\u4e0eADC\u7cbe\u5ea6\u5efa\u6a21\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u51cf\u5c11\u63a8\u7406\u8bef\u5dee\u3002", "results": "\u5b9e\u73b0\u4e86\u9ad8\u8fbe93%\u7684\u6a21\u578b\u538b\u7f29\u7387\uff0cCIM\u9635\u5217\u5229\u7528\u7387\u63d0\u5347\u81f390%\uff0c\u652f\u6301\u6700\u591a256\u6761\u5b57\u7ebf\u5e76\u53d1\u6fc0\u6d3b\uff0c\u63a8\u7406\u7cbe\u5ea6\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86CIM\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u4e0e\u541e\u5410\u91cf\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u57fa\u4e8eCIM\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u8bbe\u8ba1\u3001\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5982\u526a\u679d\u4e0e\u91cf\u5316\uff09\u4ee5\u53ca\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.14750", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14750", "abs": "https://arxiv.org/abs/2510.14750", "authors": ["\u0130smail Emir Y\u00fcksel", "Ataberk Olgun", "F. Nisa Bostanc\u0131", "Haocong Luo", "A. Giray Ya\u011fl\u0131k\u00e7\u0131", "Onur Mutlu"], "title": "ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems", "comment": "Extended version of our publication at the 58th IEEE/ACM\n  International Symposium on Microarchitecture (MICRO-58), 2025", "summary": "We experimentally demonstrate a new widespread read disturbance phenomenon,\nColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a\nDRAM row (aggressor row) open, we show that it is possible to disturb DRAM\ncells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells\nsharing the same columns as the aggressor row (across multiple DRAM subarrays).\nWith ColumnDisturb, the activation of a single row concurrently disturbs cells\nacross as many as three subarrays (e.g., 3072 rows) as opposed to\nRowHammer/RowPress, which affect only a few neighboring rows of the aggressor\nrow in a single subarray. We rigorously characterize ColumnDisturb and its\ncharacteristics under various operational conditions using 216 DDR4 and 4 HBM2\nchips from three major manufacturers. Among our 27 key experimental\nobservations, we highlight two major results and their implications.\n  First, ColumnDisturb affects chips from all three major manufacturers and\nworsens as DRAM technology scales down to smaller node sizes (e.g., the minimum\ntime to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We\nobserve that, in existing DRAM chips, ColumnDisturb induces bitflips within a\nstandard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict\nthat, as DRAM technology node size reduces, ColumnDisturb would worsen in\nfuture DRAM chips, likely causing many more bitflips in the standard refresh\nwindow. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows\nthan retention failures. Therefore, ColumnDisturb has strong implications for\nretention-aware refresh mechanisms that leverage the heterogeneity in cell\nretention times: our detailed analyses show that ColumnDisturb greatly reduces\nthe benefits of such mechanisms.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u5546\u7528DRAM\u82af\u7247\u4e2d\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5217\u95f4\u5e72\u6270\u73b0\u8c61ColumnDisturb\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u591a\u5b50\u9635\u5217\u4e2d\u5f15\u53d1\u4f4d\u7ffb\u8f6c\u7684\u673a\u5236\uff0c\u5e76\u6307\u51fa\u5176\u5bf9\u73b0\u6709\u5237\u65b0\u673a\u5236\uff08\u5982\u4fdd\u7559\u65f6\u95f4\u611f\u77e5\u5237\u65b0\uff09\u7684\u91cd\u5927\u5f71\u54cd\uff0c\u968f\u7740DRAM\u5de5\u827a\u8282\u70b9\u7f29\u5c0f\uff0c\u8be5\u95ee\u9898\u5c06\u66f4\u52a0\u4e25\u91cd\u3002", "motivation": "\u968f\u7740DRAM\u5bc6\u5ea6\u589e\u52a0\u548c\u5de5\u827a\u5c3a\u5bf8\u7f29\u5c0f\uff0c\u4f20\u7edf\u884c\u7ea7\u5e72\u6270\uff08\u5982RowHammer\uff09\u4e4b\u5916\u7684\u65b0\u578b\u5e72\u6270\u73b0\u8c61\u53ef\u80fd\u6d6e\u73b0\uff0c\u4f46\u5217\u7ea7\u5e72\u6270\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u4e00\u79cd\u65b0\u7684\u3001\u8de8\u5b50\u9635\u5217\u7684\u5217\u95f4\u5e72\u6270\u673a\u5236\uff0c\u8bc4\u4f30\u5176\u5bf9\u53ef\u9760\u6027\u548c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u7684\u5f71\u54cd\u3002", "challenges": "1. \u53d1\u73b0\u5e76\u9a8c\u8bc1\u4e00\u79cd\u5168\u65b0\u7684DRAM\u5e72\u6270\u673a\u5236\uff08ColumnDisturb\uff09\uff0c\u5176\u901a\u8fc7\u5217\u7ebf\uff08\u4f4d\u7ebf\uff09\u800c\u975e\u884c\u7ebf\u5f15\u53d1\u8de8\u5b50\u9635\u5217\u7684\u4f4d\u7ffb\u8f6c\uff1b2. \u5728\u591a\u79cd\u5546\u7528DDR4\u548cHBM2\u82af\u7247\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u8868\u5f81\uff1b3. \u5206\u6790\u8be5\u73b0\u8c61\u5bf9\u73b0\u6709\u4fdd\u7559\u65f6\u95f4\u611f\u77e5\u5237\u65b0\u673a\u5236\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "contributions": "1. \u9996\u6b21\u63d0\u51fa\u5e76\u5b9e\u9a8c\u8bc1\u5b9eColumnDisturb\u73b0\u8c61\uff0c\u5c55\u793a\u5176\u53ef\u540c\u65f6\u5f71\u54cd\u591a\u8fbe\u4e09\u4e2a\u5b50\u9635\u5217\uff08\u59823072\u884c\uff09\uff1b2. \u5728220\u9897\u771f\u5b9eDRAM\u82af\u7247\u4e0a\u7cfb\u7edf\u8868\u5f81\u8be5\u73b0\u8c61\uff0c\u63ed\u793a27\u9879\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\uff1b3. \u8bc1\u660eColumnDisturb\u5728\u6807\u51c6\u5237\u65b0\u5468\u671f\u5185\u5373\u53ef\u5f15\u53d1\u591a\u4f4d\u7ffb\u8f6c\uff0c\u4e14\u5f71\u54cd\u8303\u56f4\u8fdc\u8d85\u4fdd\u7559\u65f6\u95f4\u5931\u6548\uff1b4. \u6307\u51fa\u8be5\u73b0\u8c61\u5c06\u524a\u5f31\u4fdd\u7559\u65f6\u95f4\u611f\u77e5\u5237\u65b0\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "results": "1. ColumnDisturb\u5b58\u5728\u4e8e\u4e09\u5927\u5382\u5546\u7684DDR4\u548cHBM2\u82af\u7247\u4e2d\uff0c\u4e14\u968f\u5de5\u827a\u8282\u70b9\u7f29\u5c0f\u800c\u52a0\u5267\uff08\u9996\u6b21\u4f4d\u7ffb\u8f6c\u65f6\u95f4\u6700\u591a\u7f29\u77ed5.06\u500d\uff09\uff1b2. \u5728\u6807\u51c6DDR4\u5237\u65b0\u7a97\u53e3\uff0863.6ms\uff09\u5185\u5373\u53ef\u8bf1\u53d1\u591a\u4f4d\u7ffb\u8f6c\uff1b3. \u53d7\u5f71\u54cd\u7684\u884c\u6570\u6700\u591a\u53ef\u8fbe\u4fdd\u7559\u65f6\u95f4\u5931\u6548\u7684198\u500d\uff1b4. \u9884\u6d4b\u672a\u6765\u66f4\u5c0f\u5de5\u827a\u8282\u70b9\u4e0b\u8be5\u95ee\u9898\u5c06\u66f4\u4e25\u91cd\u3002", "conclusion": "ColumnDisturb\u662f\u4e00\u79cd\u4e25\u91cd\u4e14\u666e\u904d\u5b58\u5728\u7684\u65b0\u578bDRAM\u5e72\u6270\u673a\u5236\uff0c\u5f71\u54cd\u8303\u56f4\u5e7f\u3001\u968f\u5de5\u827a\u6f14\u8fdb\u6076\u5316\uff0c\u4e14\u4f1a\u663e\u8457\u524a\u5f31\u5f53\u524d\u5148\u8fdb\u7684\u5237\u65b0\u4f18\u5316\u7b56\u7565\uff0c\u672a\u6765DRAM\u53ef\u9760\u6027\u8bbe\u8ba1\u9700\u8003\u8651\u6b64\u7c7b\u5217\u7ea7\u5e72\u6270\u3002", "related_work": "RowHammer\u3001RowPress\u7b49\u884c\u7ea7\u5e72\u6270\u7814\u7a76\uff1bDRAM\u4fdd\u7559\u65f6\u95f4\u5f02\u8d28\u6027\u53ca\u4fdd\u7559\u65f6\u95f4\u611f\u77e5\u5237\u65b0\u673a\u5236\uff08\u5982PARA\u3001RTA\uff09\uff1bDRAM\u7269\u7406\u7ed3\u6784\u4e0e\u4f4d\u7ebf\u8026\u5408\u6548\u5e94\u7684\u76f8\u5173\u5efa\u6a21\u5de5\u4f5c\u3002"}}
{"id": "2510.14024", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14024", "abs": "https://arxiv.org/abs/2510.14024", "authors": ["Thanh Son Phung", "Douglas Thain"], "title": "Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management", "comment": null, "summary": "The rise of Generative AI introduces a new class of HPC workloads that\nintegrates lightweight LLMs with traditional high-throughput applications to\naccelerate scientific discovery. The current design of HPC clusters is\ninadequate to support this new class however, either incurring long wait times\non static batch queues or repeatedly paying expensive LLM startup costs upon\nresource preemption. To circumvent both the long queues and high startup costs,\nwe propose to \"decouple\" the LLM initialization context from the actual LLM\ninferences, and retain the context in GPUs until it is no longer needed, a\ntechnique we term \"Pervasive Context Management\". We transform a fact\nverification application to enable this technique, allowing it to reduce its\nexecution time by 72.1% (from 3 hours to 48 minutes) using the same amount of\nGPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and\nfurther reduce the execution time to 13 minutes.", "AI": {"tldr": "\u63d0\u51fa\u201c\u666e\u904d\u4e0a\u4e0b\u6587\u7ba1\u7406\u201d\u6280\u672f\uff0c\u901a\u8fc7\u89e3\u8026\u5927\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\u4e0a\u4e0b\u6587\u4e0e\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u79d1\u5b66\u5e94\u7528\u7684\u6267\u884c\u65f6\u95f4\u5e76\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709HPC\u96c6\u7fa4\u8bbe\u8ba1\u65e0\u6cd5\u6709\u6548\u652f\u6301\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u9ad8\u6027\u80fd\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b58\u5728\u6392\u961f\u65f6\u95f4\u957f\u548c\u6a21\u578b\u542f\u52a8\u5f00\u9500\u9ad8\u7684\u95ee\u9898\u3002", "challenges": "\u5982\u4f55\u5728\u8d44\u6e90\u62a2\u5360\u9891\u7e41\u7684HPC\u73af\u5883\u4e2d\u907f\u514d\u91cd\u590d\u652f\u4ed8\u9ad8\u6602\u7684LLM\u542f\u52a8\u6210\u672c\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u788e\u7247\u5316GPU\u8d44\u6e90\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002", "contributions": "\u63d0\u51fa\u4e86\u201c\u666e\u904d\u4e0a\u4e0b\u6587\u7ba1\u7406\u201d\u6280\u672f\uff0c\u5b9e\u73b0\u4e86LLM\u4e0a\u4e0b\u6587\u7684\u6301\u4e45\u5316\u4fdd\u7559\uff0c\u5e76\u6539\u9020\u4e86\u4e8b\u5b9e\u9a8c\u8bc1\u5e94\u7528\u4ee5\u652f\u6301\u8be5\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6267\u884c\u65f6\u95f4\u5927\u5e45\u7f29\u77ed\u548c\u673a\u4f1a\u6027\u6269\u5c55\u3002", "results": "\u4f7f\u7528\u76f8\u540c\u6570\u91cf\u7684GPU\u5c06\u6267\u884c\u65f6\u95f4\u4ece3\u5c0f\u65f6\u51cf\u5c11\u523048\u5206\u949f\uff08\u63d0\u534772.1%\uff09\uff0c\u5e76\u5728\u5229\u7528\u96c6\u7fa4\u4e2d32.8%\u7684GPU\u8fdb\u884c\u673a\u4f1a\u6269\u5c55\u540e\u8fdb\u4e00\u6b65\u7f29\u77ed\u81f313\u5206\u949f\u3002", "conclusion": "\u666e\u904d\u4e0a\u4e0b\u6587\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86HPC\u4e2dLLM\u5de5\u4f5c\u8d1f\u8f7d\u7684\u542f\u52a8\u5f00\u9500\u4e0e\u8d44\u6e90\u5229\u7528\u95ee\u9898\uff0c\u4e3aAI\u589e\u5f3a\u578b\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8fd0\u884c\u6a21\u5f0f\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ecHPC\u8d44\u6e90\u8c03\u5ea6\u4f18\u5316\u3001LLM\u63a8\u7406\u52a0\u901f\u6280\u672f\u4ee5\u53ca\u4e0a\u4e0b\u6587\u590d\u7528\u673a\u5236\u7b49\u3002"}}
{"id": "2510.13819", "categories": ["cs.NI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.13819", "abs": "https://arxiv.org/abs/2510.13819", "authors": ["George Stamatelis", "Hui Chen", "Henk Wymeersch", "George C. Alexandropoulos"], "title": "Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach", "comment": "Submitted to an IEEE venue", "summary": "This paper studies user localization aided by a Reconfigurable Intelligent\nSurface (RIS). A feedback link from the Base Station (BS) to the user is\nadopted to enable dynamic power control of the user pilot transmissions in the\nuplink. A novel multi-agent algorithm for the joint control of the RIS phase\nconfiguration and the user transmit power is presented, which is based on a\nhybrid approach integrating NeuroEvolution (NE) and supervised learning. The\nproposed scheme requires only single-bit feedback messages for the uplink power\ncontrol, supports RIS elements with discrete responses, and is numerically\nshown to outperform fingerprinting, deep reinforcement learning baselines and\nbackpropagation-based position estimators.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u7684\u7528\u6237\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u8fdb\u5316\u548c\u76d1\u7763\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u63a7\u5236RIS\u76f8\u4f4d\u914d\u7f6e\u548c\u7528\u6237\u53d1\u5c04\u529f\u7387\u3002\u8be5\u65b9\u6848\u4ec5\u9700\u5355\u6bd4\u7279\u53cd\u9988\u4fe1\u606f\uff0c\u652f\u6301\u79bb\u6563\u54cd\u5e94\u7684RIS\u5143\u4ef6\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347RIS\u8f85\u52a9\u4e0b\u7684\u7528\u6237\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7cfb\u7edf\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u5f00\u9500\u7684\u4e0a\u884c\u94fe\u8def\u529f\u7387\u63a7\u5236\u4e0eRIS\u914d\u7f6e\u3002", "challenges": "\u5982\u4f55\u5728\u6709\u9650\u53cd\u9988\uff08\u5982\u5355\u6bd4\u7279\u53cd\u9988\uff09\u4e0b\u5b9e\u73b0RIS\u76f8\u4f4d\u4e0e\u7528\u6237\u53d1\u5c04\u529f\u7387\u7684\u8054\u5408\u4f18\u5316\uff1b\u5904\u7406RIS\u5143\u4ef6\u7684\u79bb\u6563\u54cd\u5e94\u7279\u6027\uff1b\u5728\u590d\u6742\u65e0\u7ebf\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "contributions": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8fdb\u5316\u4e0e\u76d1\u7763\u5b66\u4e60\u6df7\u5408\u6846\u67b6\u7684\u591a\u667a\u80fd\u4f53\u8054\u5408\u4f18\u5316\u7b97\u6cd5\uff1b\u5b9e\u73b0\u4e86\u4ec5\u9700\u5355\u6bd4\u7279\u53cd\u9988\u7684\u4e0a\u884c\u529f\u7387\u63a7\u5236\u673a\u5236\uff1b\u652f\u6301\u5177\u6709\u79bb\u6563\u76f8\u4f4d\u54cd\u5e94\u7684RIS\uff0c\u5e76\u5728\u5b9a\u4f4d\u6027\u80fd\u4e0a\u8d85\u8d8a\u6307\u7eb9\u8bc6\u522b\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u4f4d\u7f6e\u4f30\u8ba1\u5668\u3002", "results": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6307\u7eb9\u8bc6\u522b\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u548c\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u4f4d\u7f6e\u4f30\u8ba1\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u5907\u4f4e\u53cd\u9988\u5f00\u9500\u548c\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RIS\u8f85\u52a9\u5b9a\u4f4d\u4e2d\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5177\u5907\u4f4e\u901a\u4fe1\u5f00\u9500\u3001\u9ad8\u7cbe\u5ea6\u548c\u5bf9\u5b9e\u9645RIS\u786c\u4ef6\u7279\u6027\u7684\u826f\u597d\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765\u4f4e\u529f\u8017\u667a\u80fd\u53cd\u5c04\u9762\u7cfb\u7edf\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u57fa\u4e8eRIS\u7684\u5b9a\u4f4d\u6280\u672f\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u3001\u6307\u7eb9\u5b9a\u4f4d\u65b9\u6cd5\u4ee5\u53ca\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u53ef\u5fae\u5206RIS\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.14147", "categories": ["cs.DC", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.14147", "abs": "https://arxiv.org/abs/2510.14147", "authors": ["Gabriel Raulet", "Dmitriy Morozov", "Aydin Buluc", "Katherine Yelick"], "title": "Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction", "comment": "11 pages, 5 figures, 3 tables", "summary": "Computing fixed-radius near-neighbor graphs is an important first step for\nmany data analysis algorithms. Near-neighbor graphs connect points that are\nclose under some metric, endowing point clouds with a combinatorial structure.\nAs computing power and data acquisition methods advance, diverse sources of\nlarge scientific datasets would greatly benefit from scalable solutions to this\ncommon subroutine for downstream analysis. Prior work on parallel nearest\nneighbors has made great progress in problems like k-nearest and approximate\nnearest neighbor search problems, with particular attention on Euclidean\nspaces. Yet many applications need exact solutions and non-Euclidean metrics.\nThis paper presents a scalable sparsity-aware distributed memory algorithm\nusing cover trees to compute near-neighbor graphs in general metric spaces. We\nprovide a shared-memory algorithm for cover tree construction and demonstrate\nits competitiveness with state-of-the-art fixed-radius search data structures.\nWe then introduce two distributed-memory algorithms for the near-neighbor graph\nproblem, a simple point-partitioning strategy and a spatial-partitioning\nstrategy, which leverage the cover tree algorithm on each node. Our algorithms\nexhibit parallel scaling across a variety of real and synthetic datasets for\nboth traditional and non-traditional metrics. On real world high dimensional\ndatasets with one million points, we achieve speedups up to 678.34x over the\nstate-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on\naverage), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per\nvertex (on average).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u57fa\u4e8e\u8986\u76d6\u6811\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u56fa\u5b9a\u534a\u5f84\u8fd1\u90bb\u56fe\uff0c\u652f\u6301\u7a00\u758f\u6027\u611f\u77e5\uff0c\u5e76\u5728\u591a\u79cd\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u5e76\u884c\u6269\u5c55\u6027\u4e0e\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u548c\u6570\u636e\u83b7\u53d6\u624b\u6bb5\u7684\u8fdb\u6b65\uff0c\u8bb8\u591a\u79d1\u5b66\u6570\u636e\u96c6\u9700\u8981\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8fd1\u90bb\u56fe\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u9700\u8981\u7cbe\u786e\u89e3\u7684\u5e94\u7528\u573a\u666f\u3002", "challenges": "\u5728\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u9ad8\u6548\u6784\u5efa\u7cbe\u786e\u7684\u8fd1\u90bb\u56fe\uff0c\u540c\u65f6\u5b9e\u73b0\u826f\u597d\u7684\u5206\u5e03\u5f0f\u5e76\u884c\u6269\u5c55\u6027\uff0c\u5c24\u5176\u9762\u5bf9\u9ad8\u7ef4\u548c\u5927\u89c4\u6a21\u6570\u636e\u65f6\u5b58\u5728\u6027\u80fd\u548c\u5185\u5b58\u4f7f\u7528\u6311\u6218\u3002", "contributions": "1) \u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u4eab\u5185\u5b58\u7684\u8986\u76d6\u6811\u6784\u5efa\u7b97\u6cd5\uff1b2) \u8bbe\u8ba1\u4e86\u4e24\u79cd\u5206\u5e03\u5f0f\u5185\u5b58\u8fd1\u90bb\u56fe\u7b97\u6cd5\uff08\u70b9\u5212\u5206\u548c\u7a7a\u95f4\u5212\u5206\u7b56\u7565\uff09\uff1b3) \u5728\u591a\u79cd\u5ea6\u91cf\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u7684\u9ad8\u6548\u5904\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u663e\u8457\u7684\u52a0\u901f\u6bd4\u3002", "results": "\u5728\u767e\u4e07\u7ea7\u9ad8\u7ef4\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75281024\u6838\u8fbe\u5230\u6700\u9ad8678.34\u500d\u52a0\u901f\uff08\u5e73\u574770\u4e2a\u90bb\u5c45\uff09\uff0c\u4f7f\u75284096\u6838\u8fbe\u5230\u6700\u9ad81590.99\u500d\u52a0\u901f\uff08\u5e73\u5747500\u4e2a\u90bb\u5c45\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u8986\u76d6\u6811\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u5728\u4e00\u822c\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u80fd\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e2d\u7684\u8fd1\u90bb\u56fe\u6784\u5efa\u3002", "related_work": "\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728k\u8fd1\u90bb\u548c\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5c24\u5176\u662f\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u4f46\u5728\u7cbe\u786e\u89e3\u548c\u975e\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u65b9\u9762\u7684\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\u3002"}}
{"id": "2510.14111", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.14111", "abs": "https://arxiv.org/abs/2510.14111", "authors": ["Taekyun Lee", "Tommaso Balercia", "Heasung Kim", "Hyeji Kim", "Jeffrey G. Andrews"], "title": "DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks", "comment": null, "summary": "This paper introduces a novel framework for high-accuracy outdoor user\nequipment (UE) positioning that applies a conditional generative diffusion\nmodel directly to high-dimensional massive MIMO channel state information\n(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic\noutdoor environments and require dense, impractical data surveys. To overcome\nthese limitations, our approach learns a direct mapping from raw uplink\nSounding Reference Signal (SRS) fingerprints to continuous geographic\ncoordinates. We demonstrate that our DiffLoc framework achieves unprecedented\nsub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm\nfusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,\nray-traced Tokyo urban macro-cell environment. This represents an\norder-of-magnitude improvement over existing methods, including supervised\nregression approaches (over 10 m error) and grid-based fusion (3 m error). Our\nconsistency training approach reduces inference time from 200 steps to just 2\nsteps while maintaining exceptional accuracy even for high-speed users (15-25\nm/s) and unseen user trajectories, demonstrating the practical feasibility of\nour framework for real-time 6G applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u9ad8\u7cbe\u5ea6\u5ba4\u5916\u7528\u6237\u8bbe\u5907\u5b9a\u4f4d\u6846\u67b6DiffLoc\uff0c\u76f4\u63a5\u5229\u7528\u5927\u89c4\u6a21MIMO\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u5b9e\u73b0\u4e9a\u5398\u7c73\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u6027\u8bad\u7ec3\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5b9e\u65f66G\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u6307\u7eb9\u5b9a\u4f4d\u65b9\u6cd5\u5728\u5927\u8303\u56f4\u52a8\u6001\u5ba4\u5916\u73af\u5883\u4e2d\u6269\u5c55\u6027\u5dee\uff0c\u4f9d\u8d56\u5bc6\u96c6\u4e14\u4e0d\u5207\u5b9e\u9645\u7684\u6570\u636e\u91c7\u96c6\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u9700\u6c42\u3002", "challenges": "\u5982\u4f55\u4ece\u9ad8\u7ef4\u3001\u590d\u6742\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e2d\u76f4\u63a5\u5b66\u4e60\u5230\u9ad8\u7cbe\u5ea6\u7684\u8fde\u7eed\u5730\u7406\u5750\u6807\u6620\u5c04\uff1b\u5728\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u9ad8\u901f\u79fb\u52a8\u7528\u6237\u548c\u672a\u77e5\u8f68\u8ff9\u7684\u9c81\u68d2\u6027\u3002", "contributions": "\u63d0\u51fa\u4e86DiffLoc\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u57fa\u4e8eSRS\u6307\u7eb9\u7684\u5ba4\u5916\u5b9a\u4f4d\uff1b\u5b9e\u73b0\u4e86\u4e9a\u5398\u7c73\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6\uff1b\u5f15\u5165\u4e00\u81f4\u6027\u8bad\u7ec3\u5c06\u63a8\u7406\u6b65\u9aa4\u4ece200\u6b65\u964d\u81f32\u6b65\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "results": "\u5728\u4e1c\u4eac\u57ce\u5e02\u5b8f\u5c0f\u533a\u73af\u5883\u4e2d\uff0cDiffLoc-CT\u6a21\u578b\u5b9e\u73b00.5\u5398\u7c73\u878d\u5408\u5b9a\u4f4d\u7cbe\u5ea6\u548c1-2\u5398\u7c73\u5355\u57fa\u7ad9\u7cbe\u5ea6\uff0c\u8f83\u76d1\u7763\u56de\u5f52\u65b9\u6cd5\uff08>10\u7c73\u8bef\u5dee\uff09\u548c\u7f51\u683c\u878d\u5408\u65b9\u6cd5\uff083\u7c73\u8bef\u5dee\uff09\u63d0\u5347\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "DiffLoc\u6846\u67b6\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u5907\u57286G\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u6f5c\u529b\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u57fa\u4e8e\u6307\u7eb9\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3001\u76d1\u7763\u56de\u5f52\u6a21\u578b\u3001\u7f51\u683c\u5316\u878d\u5408\u6280\u672f\u4ee5\u53ca\u751f\u6210\u6a21\u578b\u5728\u5b9a\u4f4d\u4e2d\u7684\u521d\u6b65\u5e94\u7528\uff0c\u4f46\u5747\u672a\u8fbe\u5230\u672c\u6587\u6240\u5b9e\u73b0\u7684\u7cbe\u5ea6\u4e0e\u6548\u7387\u5e73\u8861\u3002"}}
{"id": "2510.14186", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14186", "abs": "https://arxiv.org/abs/2510.14186", "authors": ["Pengkun Ren", "Hai Dong", "Nasrin Sohrabi", "Zahir Tari", "Pengcheng Zhang"], "title": "Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs", "comment": "18 pages, 4 figures", "summary": "Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on\ntransaction ordering despite malicious actors, but unconstrained ordering power\nenables sophisticated value extraction attacks like front running and sandwich\nattacks - a critical threat to blockchain systems. Order-fair consensus curbs\nadversarial value extraction by constraining how leaders may order\ntransactions. While state-of-the-art protocols such as Themis attain strong\nguarantees through graph-based ordering, they ask every replica to re-run the\nleader's expensive ordering computation for validation - an inherently\nsymmetric and redundant paradigm. We present AUTIG, a high-performance,\npluggable order-fairness service that breaks this symmetry. Our key insight is\nthat verifying a fair order does not require re-computing it. Instead,\nverification can be reduced to a stateless audit of succinct, verifiable\nassertions about the ordering graph's properties. AUTIG realizes this via an\nasymmetric architecture: the leader maintains a persistent\nUnconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction\nacross rounds and emits a structured proof of fairness with each proposal;\nfollowers validate the proof without maintaining historical state. AUTIG\nintroduces three critical innovations: (i) incremental graph maintenance driven\nby threshold-crossing events and state changes; (ii) a decoupled pipeline that\noverlaps leader-side collection/update/extraction with follower-side stateless\nverification; and (iii) a proof design covering all internal pairs in the\nfinalized prefix plus a frontier completeness check to rule out hidden external\ndependencies. We implement AUTIG and evaluate it against symmetric graph-based\nbaselines under partial synchrony. Experiments show higher throughput and lower\nend-to-end latency while preserving gamma-batch-order-fairness.", "AI": {"tldr": "AUTIG\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u53ef\u63d2\u62d4\u7684\u987a\u5e8f\u516c\u5e73\u6027\u670d\u52a1\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u5bf9\u79f0\u67b6\u6784\u6253\u7834\u4f20\u7edf\u62dc\u5360\u5ead\u5bb9\u9519\u5171\u8bc6\u4e2d\u5bf9\u4ea4\u6613\u6392\u5e8f\u7684\u5197\u4f59\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u9ad8\u6548\u9a8c\u8bc1\u800c\u65e0\u9700\u91cd\u590d\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u969c\u6279\u6b21\u987a\u5e8f\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u987a\u5e8f\u516c\u5e73\u5171\u8bc6\u534f\u8bae\uff08\u5982Themis\uff09\u867d\u7136\u80fd\u9632\u8303\u6076\u610f\u4ea4\u6613\u6392\u5e8f\u653b\u51fb\uff08\u5982\u62a2\u8dd1\u3001\u5939\u5fc3\u653b\u51fb\uff09\uff0c\u4f46\u8981\u6c42\u6240\u6709\u526f\u672c\u91cd\u590d\u6267\u884c\u9886\u5bfc\u8005\u6602\u8d35\u7684\u6392\u5e8f\u8ba1\u7b97\uff0c\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e0b\u3002\u8fd9\u79cd\u5bf9\u79f0\u9a8c\u8bc1\u673a\u5236\u9020\u6210\u4e86\u8d44\u6e90\u6d6a\u8d39\u548c\u7cfb\u7edf\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5982\u4f55\u5728\u4e0d\u91cd\u65b0\u6267\u884c\u5b8c\u6574\u6392\u5e8f\u8ba1\u7b97\u7684\u524d\u63d0\u4e0b\uff0c\u786e\u4fdd\u4ea4\u6613\u987a\u5e8f\u7684\u516c\u5e73\u6027\u53ef\u88ab\u9ad8\u6548\u4e14\u5b89\u5168\u5730\u9a8c\u8bc1\uff1b\u540c\u65f6\u9700\u8bbe\u8ba1\u4e00\u79cd\u8f7b\u91cf\u3001\u65e0\u72b6\u6001\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u4f7f\u8ddf\u968f\u8005\u65e0\u9700\u7ef4\u62a4\u5386\u53f2\u72b6\u6001\u5373\u53ef\u5b8c\u6210\u9a8c\u8bc1\uff0c\u5e76\u4fdd\u8bc1\u7cfb\u7edf\u5728\u90e8\u5206\u540c\u6b65\u6761\u4ef6\u4e0b\u7684\u9ad8\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "contributions": "1. \u63d0\u51faAUTIG\uff0c\u9996\u4e2a\u57fa\u4e8e\u4e0d\u5bf9\u79f0\u67b6\u6784\u7684\u9ad8\u6548\u987a\u5e8f\u516c\u5e73\u6027\u670d\u52a1\uff1b2. \u8bbe\u8ba1\u589e\u91cf\u5f0f\u672a\u786e\u8ba4\u4ea4\u6613\u56fe\uff08UTIG\uff09\u7ed3\u6784\uff0c\u7531\u9886\u5bfc\u8005\u7ef4\u62a4\u4ee5\u644a\u9500\u8de8\u8f6e\u6b21\u7684\u56fe\u6784\u5efa\u5f00\u9500\uff1b3. \u6784\u5efa\u65e0\u9700\u72b6\u6001\u7684\u9a8c\u8bc1\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u9886\u5bfc\u8005\u7684\u6392\u5e8f\u4e0e\u8ddf\u968f\u8005\u7684\u9a8c\u8bc1\u5e76\u884c\u5316\uff1b4. \u63d0\u51fa\u5305\u542b\u5185\u90e8\u5bf9\u8986\u76d6\u548c\u524d\u6cbf\u5b8c\u6574\u6027\u68c0\u67e5\u7684\u7ed3\u6784\u5316\u516c\u5e73\u6027\u8bc1\u660e\u673a\u5236\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0cAUTIG\u5728\u4fdd\u6301gamma-batch-order-fairness\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u5bf9\u79f0\u5f0f\u7684\u56fe\u57fa\u57fa\u51c6\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u9a8c\u8bc1\u8fc7\u7a0b\u65e0\u9700\u5386\u53f2\u72b6\u6001\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ddf\u968f\u8005\u7684\u8ba1\u7b97\u4e0e\u5b58\u50a8\u5f00\u9500\u3002", "conclusion": "AUTIG\u901a\u8fc7\u5c06\u6392\u5e8f\u4e0e\u9a8c\u8bc1\u89e3\u8026\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u987a\u5e8f\u516c\u5e73\u5171\u8bc6\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5bf9\u79f0\u9a8c\u8bc1\u5e26\u6765\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u533a\u5757\u94fe\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u4ea4\u6613\u6392\u5e8f\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "related_work": "\u4e0eThemis\u7b49\u57fa\u4e8e\u56fe\u7684\u987a\u5e8f\u516c\u5e73\u5171\u8bc6\u534f\u8bae\u5bc6\u5207\u76f8\u5173\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u9996\u6b21\u5c06\u4ea4\u6613\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e3a\u56fe\u7ed3\u6784\u4ee5\u5b9e\u73b0\u516c\u5e73\u6392\u5e8f\uff0c\u4f46\u91c7\u7528\u5bf9\u79f0\u9a8c\u8bc1\u65b9\u5f0f\u5bfc\u81f4\u6240\u6709\u526f\u672c\u91cd\u590d\u9ad8\u6210\u672c\u8ba1\u7b97\uff0cAUTIG\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u975e\u5bf9\u79f0\u9a8c\u8bc1\u673a\u5236\uff0c\u907f\u514d\u4e86\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2510.14599", "categories": ["cs.DC", "D.4.1; C.4; C.1.4; D.1.3"], "pdf": "https://arxiv.org/pdf/2510.14599", "abs": "https://arxiv.org/abs/2510.14599", "authors": ["Michal Konopa", "Jan Fesl", "Ladislav Ber \u00e1nek"], "title": "JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization", "comment": "25 pages", "summary": "The increasing complexity and temporal variability of workloads on\nMIG-enabled GPUs challenge the scalability of traditional centralized\nscheduling. Building upon the SJA concept, this paper introduces JASDA-a novel\nparadigm that extends SJA from a largely centralized scheduling model toward a\nfully decentralized negotiation process. In JASDA, jobs actively generate and\nscore feasible subjobs in response to scheduler-announced execution windows,\nwhile the scheduler performs policy-driven clearing that balances utilization,\nfairness, and temporal responsiveness. This bidirectional, iterative\ninteraction embeds feedback, calibration, and probabilistic safety directly\ninto the scheduling loop, enabling adaptive and transparent decision-making. By\ncoupling principles from auction theory and online optimization with the\ntemporal granularity of GPU workloads, JASDA provides a scalable foundation for\nmarket-aware and fairness-driven resource management-bridging theoretical\nscheduling models with practical deployment in modern MIG-enabled environments\nrelevant to Artificial Intelligence and Agriculture 4.0.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86JASDA\uff0c\u4e00\u79cd\u9762\u5411MIG-GPU\u7684\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u62cd\u5356\u7406\u8bba\u4e0e\u5728\u7ebf\u4f18\u5316\uff0c\u5b9e\u73b0\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u9002\u5e94\u3001\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u8d44\u6e90\u7ba1\u7406\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u5728MIG-GPU\u4e0a\u9762\u5bf9\u590d\u6742\u4e14\u65f6\u53d8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u6269\u5c55\u6027\u53d7\u9650\uff0c\u4e9f\u9700\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u8c03\u5ea6\u673a\u5236\u3002", "challenges": "\u5982\u4f55\u5728\u4fdd\u8bc1\u8d44\u6e90\u5229\u7528\u7387\u3001\u516c\u5e73\u6027\u548c\u54cd\u5e94\u65f6\u6548\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "contributions": "\u63d0\u51fa\u4e86JASDA\u8c03\u5ea6\u6846\u67b6\uff0c\u5c06SJA\u6a21\u578b\u4ece\u96c6\u4e2d\u5f0f\u6269\u5c55\u4e3a\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u5546\u673a\u5236\uff0c\u5f15\u5165\u53cc\u5411\u8fed\u4ee3\u4ea4\u4e92\u4e0e\u7b56\u7565\u9a71\u52a8\u7684\u6e05\u7b97\u673a\u5236\uff0c\u5d4c\u5165\u53cd\u9988\u4e0e\u6982\u7387\u5b89\u5168\u6027\u3002", "results": "JASDA\u901a\u8fc7\u62cd\u5356\u673a\u5236\u4e0e\u5728\u7ebf\u4f18\u5316\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u8c03\u5ea6\u7684\u9002\u5e94\u6027\u4e0e\u900f\u660e\u5ea6\uff0c\u5728\u5229\u7528\u7387\u3001\u516c\u5e73\u6027\u548c\u54cd\u5e94\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "JASDA\u4e3aMIG-GPU\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u8c03\u5ea6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5e02\u573a\u611f\u77e5\u4e14\u516c\u5e73\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u8c03\u5ea6\u6a21\u578b\u4e0e\u5b9e\u9645AI\u548c\u519c\u4e1a4.0\u5e94\u7528\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "related_work": "SJA\uff08Scheduler-Job Alignment\uff09\u6a21\u578b\u662f\u672c\u6587\u7684\u57fa\u7840\uff0c\u76f8\u5173\u5de5\u4f5c\u8fd8\u5305\u62ec\u57fa\u4e8e\u62cd\u5356\u7406\u8bba\u7684\u8d44\u6e90\u5206\u914d\u548c\u5728\u7ebf\u4f18\u5316\u8c03\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2510.14622", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.14622", "abs": "https://arxiv.org/abs/2510.14622", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Hyein Woo", "Junhee Kim", "Jinwoo Baek", "Kyungkuk Nam", "Sangyoon Ji", "Jiseon Kim", "Hanyeoreum Bae", "Junhyeok Jang", "Hyunwoo You", "Junseok Moon", "Myoungsoo Jung"], "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems", "comment": null, "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCXL\u7684\u65b0\u578bMPI\u901a\u4fe1\u8303\u5f0fMPI-over-CXL\uff0c\u5229\u7528\u8de8\u4e3b\u673a\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5171\u4eab\u5185\u5b58\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u6570\u636e\u62f7\u8d1d\u65b9\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5ef6\u8fdf\u548c\u5185\u5b58\u5e26\u5bbd\u6d88\u8017\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21HPC\u73af\u5883\u4e2d\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edfMPI\u5b9e\u73b0\u4f9d\u8d56\u663e\u5f0f\u5185\u5b58\u62f7\u8d1d\u64cd\u4f5c\uff0c\u5bfc\u81f4\u5197\u4f59\u6570\u636e\u79fb\u52a8\u548c\u7f13\u51b2\u533a\u7ba1\u7406\u5f00\u9500\uff0c\u5c24\u5176\u5f71\u54cd\u9ad8\u5f3a\u5ea6\u8fdb\u7a0b\u95f4\u901a\u4fe1\u7684HPC\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u3002", "challenges": "\u5982\u4f55\u5728\u591a\u4e3b\u673a\u95f4\u9ad8\u6548\u5b9e\u73b0\u7f13\u5b58\u4e00\u81f4\u7684\u5171\u4eab\u5185\u5b58\u901a\u4fe1\uff0c\u907f\u514d\u4f20\u7edfMPI\u4e2d\u7684\u5197\u4f59\u6570\u636e\u62f7\u8d1d\u548c\u7ba1\u7406\u5f00\u9500\uff1b\u540c\u65f6\u6784\u5efa\u652f\u6301\u8be5\u673a\u5236\u7684\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u534f\u540c\u73af\u5883\u3002", "contributions": "\u63d0\u51fa\u4e86MPI-over-CXL\u901a\u4fe1\u8303\u5f0f\uff1b\u8bbe\u8ba1\u4e86\u652f\u6301CXL 3.2\u7684\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\uff1b\u5b9e\u73b0\u4e86\u57fa\u4e8eFPGA\u7684\u591a\u4e3b\u673a\u4eff\u771f\u73af\u5883\u548c\u4e13\u7528\u8f6f\u4ef6\u6808\uff1b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6848\u5728\u5178\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "results": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edfMPI\u7cfb\u7edf\uff0cMPI-over-CXL\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5ef6\u8fdf\u548c\u5185\u5b58\u5e26\u5bbd\u4f7f\u7528\uff0c\u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MPI-over-CXL\u901a\u8fc7\u5229\u7528CXL\u63d0\u4f9b\u7684\u8de8\u4e3b\u673a\u5171\u4eab\u5185\u5b58\u80fd\u529b\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5c55\u73b0\u51fa\u5728\u5927\u89c4\u6a21\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u63d0\u5347\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u5de8\u5927\u6f5c\u529b\u3002", "related_work": "\u76f8\u5173\u5de5\u4f5c\u5305\u62ec\u4f20\u7edf\u57fa\u4e8e\u6d88\u606f\u4f20\u9012\u7684MPI\u5b9e\u73b0\u3001\u5171\u4eab\u5185\u5b58\u7f16\u7a0b\u6a21\u578b\u3001\u4ee5\u53ca\u8fd1\u5e74\u6765\u5229\u7528\u65b0\u578b\u4e92\u8fde\u6280\u672f\uff08\u5982CXL\u3001Gen-Z\uff09\u4f18\u5316\u7cfb\u7edf\u95f4\u901a\u4fe1\u7684\u7814\u7a76\u3002"}}
{"id": "2510.14686", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14686", "abs": "https://arxiv.org/abs/2510.14686", "authors": ["Tongxuan Liu", "Tao Peng", "Peijun Yang", "Xiaoyang Zhao", "Xiusheng Lu", "Weizhe Huang", "Zirui Liu", "Xiaoyu Chen", "Zhiwei Liang", "Jun Xiong", "Donghe Jin", "Minchao Zhang", "Jinrong Guo", "Yingxu Deng", "Xu Zhang", "Xianzhe Dong", "Siqi Wang", "Siyu Wu", "Yu Wu", "Zihan Tang", "Yuting Zeng", "Yanshu Wang", "Jinguang Liu", "Meng Kang", "Menxin Li", "Yunlong Wang", "Yiming Liu", "Xiaolong Ma", "Yifan Wang", "Yichen Zhang", "Jinrun Yin", "Keyang Zheng", "Jiawei Yin", "Jun Zhang", "Ziyue Wang", "Xiaobo Lin", "Liangyu Liu", "Liwei Lan", "Yang Liu", "Chunhua Peng", "Han Liu", "Songcheng Ren", "Xuezhu Wang", "Yunheng Shen", "Yi Wang", "Guyue Liu", "Hui Chen", "Tong Yang", "Hailong Yang", "Jing Li", "Guiguang Ding", "Ke Zhang"], "title": "xLLM Technical Report", "comment": "39 pages", "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.", "AI": {"tldr": "xLLM\u662f\u4e00\u4e2a\u9762\u5411\u9ad8\u6027\u80fd\u3001\u5927\u89c4\u6a21\u4f01\u4e1a\u7ea7\u670d\u52a1\u7684\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7684\u670d\u52a1-\u5f15\u64ce\u67b6\u6784\u548c\u591a\u5c42\u6b21\u4f18\u5316\uff0c\u5728\u591a\u79cdAI\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u3001\u9ad8\u5e76\u53d1\u7684\u4f01\u4e1a\u7ea7LLM\u63a8\u7406\u9700\u6c42\uff0c\u73b0\u6709\u6846\u67b6\u5728\u8d44\u6e90\u5229\u7528\u7387\u3001\u8c03\u5ea6\u6548\u7387\u548c\u8de8\u52a0\u901f\u5668\u517c\u5bb9\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u4e2a\u66f4\u667a\u80fd\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "challenges": "\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5982\u4f55\u9ad8\u6548\u5904\u7406\u591a\u6a21\u6001\u8bf7\u6c42\u5e76\u5171\u7f6e\u5728\u7ebf\u4e0e\u79bb\u7ebf\u4efb\u52a1\u4ee5\u63d0\u5347\u96c6\u7fa4\u5229\u7528\u7387\uff1b\u5982\u4f55\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u7075\u6d3b\u7684Prefill-Decode\u9636\u6bb5\u62c6\u5206\uff1b\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u7684\u5168\u5c40KV\u7f13\u5b58\u7ba1\u7406\u548c\u5bb9\u9519\u673a\u5236\uff1b\u4ee5\u53ca\u5982\u4f55\u5728\u7cfb\u7edf\u4e0e\u7b97\u6cd5\u5c42\u9762\u534f\u540c\u4f18\u5316\u4ee5\u5145\u5206\u9971\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "contributions": "1\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u8026\u5f0f\u670d\u52a1-\u5f15\u64ce\u67b6\u6784\uff1b2\uff09\u8bbe\u8ba1\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u7684\u52a8\u6001PD\u62c6\u5206\u7b56\u7565\u548c\u9762\u5411\u591a\u6a21\u6001\u7684EPD\u62c6\u5206\u7b56\u7565\uff1b3\uff09\u5b9e\u73b0\u4e86\u652f\u6301\u5168\u5c40KV\u7f13\u5b58\u7ba1\u7406\u548c\u5bb9\u9519\u7684\u5206\u5e03\u5f0f\u670d\u52a1\u5c42\uff1b4\uff09\u5728\u5f15\u64ce\u5c42\u5b9e\u73b0\u4e86\u591a\u5c42\u6267\u884c\u6d41\u6c34\u7ebf\u4f18\u5316\u3001\u81ea\u9002\u5e94\u56fe\u6a21\u5f0f\u548cxTensor\u5185\u5b58\u7ba1\u7406\uff1b5\uff09\u96c6\u6210\u4e86\u4f18\u5316\u7684\u63a8\u6d4b\u89e3\u7801\u548c\u52a8\u6001EPLB\u7b49\u7b97\u6cd5\u589e\u5f3a\u3002", "results": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540cTPOT\u7ea6\u675f\u4e0b\uff0cxLLM\u5728Qwen\u7cfb\u5217\u6a21\u578b\u4e0a\u541e\u5410\u91cf\u8fbe\u5230MindIE\u76841.7\u500d\u3001vLLM-Ascend\u76842.2\u500d\uff0c\u5728Deepseek\u7cfb\u5217\u6a21\u578b\u4e0a\u5e73\u5747\u541e\u5410\u91cf\u8fbe\u5230MindIE\u76841.7\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "xLLM\u901a\u8fc7\u7cfb\u7edf\u4e0e\u7b97\u6cd5\u7684\u6df1\u5ea6\u534f\u540c\u4f18\u5316\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u9ad8\u53ef\u7528\u3001\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u7684\u4f01\u4e1a\u7ea7LLM\u63a8\u7406\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u6a21\u6001\u573a\u666f\uff0c\u4e14\u5df2\u5f00\u6e90\u3002", "related_work": "\u4e0evLLM\u3001MindIE\u7b49\u4e3b\u6d41LLM\u63a8\u7406\u6846\u67b6\u76f8\u5173\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u805a\u7126\u4e8ePagedAttention\u3001\u5185\u5b58\u7ba1\u7406\u6216\u7279\u5b9a\u786c\u4ef6\u4f18\u5316\uff0c\u800cxLLM\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u670d\u52a1\u4e0e\u5f15\u64ce\u89e3\u8026\u67b6\u6784\uff0c\u5e76\u5728\u591a\u6a21\u6001\u8c03\u5ea6\u3001\u52a8\u6001\u9636\u6bb5\u62c6\u5206\u548c\u5168\u5c40\u8d44\u6e90\u7ba1\u7406\u65b9\u9762\u8fdb\u884c\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u4f18\u5316\u3002"}}
{"id": "2510.14798", "categories": ["cs.DC", "cs.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.14798", "abs": "https://arxiv.org/abs/2510.14798", "authors": ["Petra Berenbrink", "Tom Friedetzky", "Peter Kling", "Lars Nagel"], "title": "Balls and Bins and the Infinite Process with Random Deletions", "comment": null, "summary": "We consider an infinite balls-into-bins process with deletions where in each\ndiscrete step $t$ a coin is tossed as to whether, with probability $\\beta(t)\n\\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places\nthe ball in the lower loaded of two bins sampled uniformly at random) or, with\nremaining probability $1-\\beta(t)$, a ball is deleted from a non-empty bin\nchosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total\nload at time $t$. We are interested in bounding the discrepancy $x_{\\max}(t) -\nm(t)/n$ (current maximum load relative to current average) and the overload\n$x_{\\max}(t) - m_{\\max}(t)/n$ (current maximum load relative to highest average\nobserved so far).\n  We prove that at an arbitrarily chosen time $t$ the total number of balls\nabove the average is $O(n)$ and that the discrepancy is $ O(\\log(n))$. For the\ndiscrepancy, we provide a matching lower bound. Furthermore we prove that at an\narbitrarily chosen time $t$ the overload is $\\log\\log(n)+O(1)$. For \"good\"\ninsertion probability sequences (in which the average load of time intervals\nwith polynomial length increases in expectation) we show that even the\ndiscrepancy is bounded by $\\log\\log(n)+O(1)$.\n  One of our main analytical tools is a layered induction, as per [ABKU99].\nSince our model allows for rather more general scenarios than what was\npreviously considered, the formal analysis requires some extra ingredients as\nwell, in particular a detailed potential analysis. Furthermore, we simplify the\nsetup by applying probabilistic couplings to obtain certain \"recovery\"\nproperties, which eliminate much of the need for intricate and careful\nconditioning elsewhere in the analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u5e26\u6709\u5220\u9664\u64cd\u4f5c\u7684\u65e0\u9650\u7403-\u7bb1\u8fc7\u7a0b\uff0c\u5206\u6790\u4e86\u5728\u52a8\u6001\u63d2\u5165\u548c\u5220\u9664\u60c5\u51b5\u4e0b\u7cfb\u7edf\u7684\u8d1f\u8f7d\u5dee\u5f02\u548c\u8fc7\u8f7d\u60c5\u51b5\uff0c\u8bc1\u660e\u4e86\u5728\u4efb\u610f\u65f6\u523b\u8d1f\u8f7d\u5dee\u5f02\u4e3aO(log n)\uff0c\u8fc7\u8f7d\u4e3alog log n + O(1)\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u63d2\u5165\u6982\u7387\u5e8f\u5217\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5dee\u5f02\u754c\u3002", "motivation": "\u7406\u89e3\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff08\u5305\u542b\u63d2\u5165\u548c\u5220\u9664\uff09\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u8fd0\u884c\u4e0b\u7684\u6700\u5927\u8d1f\u8f7d\u4e0e\u5e73\u5747\u8d1f\u8f7d\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "challenges": "\u7531\u4e8e\u7cfb\u7edf\u540c\u65f6\u5b58\u5728\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\uff0c\u4e14\u63d2\u5165\u6982\u7387\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u8d1f\u8f7d\u52a8\u6001\u590d\u6742\uff0c\u96be\u4ee5\u7cbe\u786e\u523b\u753b\u6700\u5927\u8d1f\u8f7d\u4e0e\u5e73\u5747\u8d1f\u8f7d\u4e4b\u95f4\u7684\u5dee\u5f02\u548c\u8fc7\u8f7d\u884c\u4e3a\u3002", "contributions": "1. \u8bc1\u660e\u4e86\u4efb\u610f\u65f6\u523b\u603b\u8d1f\u8f7d\u8d85\u8fc7\u5e73\u5747\u503c\u7684\u90e8\u5206\u4e3aO(n)\uff0c\u5dee\u5f02\u4e3aO(log n)\uff0c\u4e14\u7ed9\u51fa\u4e86\u5339\u914d\u7684\u4e0b\u754c\uff1b2. \u9996\u6b21\u8bc1\u660e\u8fc7\u8f7d\u4e3alog log n + O(1)\uff1b3. \u5bf9\u2018\u826f\u597d\u2019\u63d2\u5165\u5e8f\u5217\uff0c\u5c06\u5dee\u5f02\u4e5f\u538b\u7f29\u81f3log log n + O(1)\uff1b4. \u4f7f\u7528\u5206\u5c42\u5f52\u7eb3\u548c\u52bf\u80fd\u5206\u6790\u7ed3\u5408\u6982\u7387\u8026\u5408\u6280\u672f\u7b80\u5316\u4e86\u590d\u6742\u6761\u4ef6\u5206\u6790\u3002", "results": "\u5728\u4efb\u610f\u65f6\u95f4t\uff0c\u5dee\u5f02\u4e3aO(log n)\uff0c\u8fc7\u8f7d\u4e3alog log n + O(1)\uff1b\u5bf9\u4e8e\u826f\u597d\u63d2\u5165\u5e8f\u5217\uff0c\u5dee\u5f02\u53ef\u8fdb\u4e00\u6b65\u964d\u81f3log log n + O(1)\uff1b\u5e76\u63d0\u4f9b\u4e86\u5dee\u5f02\u7684\u5339\u914d\u4e0b\u754c\u3002", "conclusion": "Greedy[2]\u7b56\u7565\u5728\u52a8\u6001\u5e26\u5220\u9664\u7684\u7403-\u7bb1\u8fc7\u7a0b\u4e2d\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u8d1f\u8f7d\u5747\u8861\u6027\u80fd\uff0c\u5c24\u5176\u5728\u957f\u671f\u8fd0\u884c\u4e2d\u6700\u5927\u8d1f\u8f7d\u4ec5\u7565\u9ad8\u4e8e\u5386\u53f2\u5e73\u5747\uff0c\u4e14\u901a\u8fc7\u7cbe\u7ec6\u5206\u6790\u53ef\u83b7\u5f97\u7d27\u786e\u754c\u3002", "related_work": "\u57fa\u4e8e\u7ecf\u5178\u7684ABKU99\u7684\u5206\u5c42\u5f52\u7eb3\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u4e86\u4ee5\u5f80\u4ec5\u8003\u8651\u63d2\u5165\u7684\u6a21\u578b\uff0c\u7eb3\u5165\u4e86\u5220\u9664\u64cd\u4f5c\u548c\u65f6\u53d8\u63d2\u5165\u6982\u7387\uff0c\u4e0e\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u3001\u968f\u673a\u5206\u914d\u548c\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u76f8\u5173\u5de5\u4f5c\u5bc6\u5207\u76f8\u5173\u3002"}}
