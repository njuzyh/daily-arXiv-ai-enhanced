<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: 本文提出了一种名为Grace的天地协同系统，用于遥感任务中的近实时大视觉语言模型推理。该系统通过在卫星上部署紧凑模型、地面站部署大模型，并结合异步检索增强生成（RAG）和任务调度算法，显著降低了平均延迟（76-95%），同时保持了推理精度。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）在低地球轨道卫星的遥感任务中展现出潜力，但受限于星上计算资源和短暂的星地通信窗口，其实际部署面临挑战。因此，需要一种高效的天地协同推理系统以实现近实时处理。

Challenges: 主要挑战包括：卫星上计算资源有限、星地通信时间短、需在保证推理性能的同时最小化延迟，以及如何有效协同星上与地面的大模型进行推理。

Contributions: 1) 提出了Grace系统，支持天地协同的近实时LVLM推理；2) 设计了适应性更新算法，用于在有限通信窗口内同步地面RAG知识库到卫星；3) 提出基于置信度的任务调度算法，动态决定任务在星上处理或卸载至地面站。

Results: 基于真实卫星轨道数据的实验表明，与现有最先进方法相比，Grace将平均延迟降低了76%-95%，且未牺牲推理准确性。

Conclusion: Grace通过星地协同架构有效解决了LVLM在LEO卫星系统中部署的延迟与资源限制问题，为遥感任务提供了高效、准确的近实时推理解决方案。

Related Work: 相关工作包括视觉语言模型在遥感中的应用、卫星边缘计算、检索增强生成（RAG）技术以及任务卸载机制。现有方法多集中于地面或纯星上处理，缺乏高效的天地协同推理框架。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [2] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种新的混合预编码方法，通过将数字和模拟预编码结合，优化多天线数据传输，利用角度与相位的相关性建模为双变量高斯分布，并首次定义了联合角度-相位熵来衡量无线信道中的不确定性，显著提升了和速率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了充分挖掘多用户大规模MIMO系统的潜力，需要高效的混合预编码技术来提升传输效率并抑制干扰。

Challenges: 如何准确建模无线信道中角度与相位的关联变化，并设计能适应这种变化的高效预编码方案。

Contributions: 1) 提出一种新的混合预编码框架；2) 将角度与相位建模为服从双变量高斯分布的关联变量；3) 首次定义联合角度-相位熵用于衡量信道不确定性；4) 所提方法显著提升了和速率与系统鲁棒性。

Results: 仿真结果显示，与现有最先进方法相比，所提方法实现了18.31%的和速率提升和11.47%的鲁棒性改善。

Conclusion: 该方法通过联合建模角度与相位的相关性及其不确定性，有效提升了MU-MMIMO系统中混合预编码的性能。

Related Work: 现有研究多独立处理角度或相位信息，缺乏对二者相关性的联合建模与利用，本文填补了这一空白。

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


### [3] [Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives](https://arxiv.org/abs/2510.24611)
*Azadeh Pourkabirian,Amir Masoud Rahmani,Kai Li,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种基于经济供需模型和VCG拍卖机制的面向延迟敏感型物联网应用的任务卸载方法，通过博弈论框架和激励机制实现用户与边缘服务器之间的市场平衡，最大化社会福利并保证低延迟。


<details>
  <summary>Details</summary>
Motivation: 由于物联网设备计算资源有限且需要实时响应，传统方法难以满足延迟敏感型应用的需求，因此需要一种能够有效分配边缘计算资源、平衡供需并激励多方参与的任务卸载机制。

Challenges: 主要挑战包括如何在多智能体环境中协调用户设备与边缘服务器的不同目标，避免资源供应不足或过剩，并在保证低延迟的同时实现公平、可信和高效的资源分配。

Contributions: 1）将任务卸载建模为经济供需模型以实现市场平衡；2）设计基于VCG拍卖的博弈论框架用于分析多智能体决策；3）提出激励机制以促进用户卸载任务和边缘服务器共享资源；4）确保真实性、社会福利最大化和延迟保障。

Results: 仿真结果表明，所提方法能够有效最大化社会福利，保持市场供需平衡，激励各方诚实参与，并为延迟敏感型物联网应用提供可靠的延迟保障。

Conclusion: 本文提出的基于经济模型和拍卖机制的任务卸载框架能够有效协调边缘计算环境中多方利益，在满足延迟要求的同时实现资源高效利用和激励相容。

Related Work: 相关工作主要集中在基于优化和博弈论的任务卸载策略，以及利用拍卖机制进行资源分配的研究，但较少从经济供需平衡角度出发同时考虑用户与边缘服务器的激励问题。

Abstract: Delay-sensitive Internet of Things (IoT) applications have drawn significant
attention. Running many of these applications on IoT devices is challenging due
to the limited processing resources of these devices and the need for real-time
responses. Task offloading can minimize latency by transferring computationally
intensive tasks from IoT devices to resource-rich edge servers, ensuring delay
and performance guarantees. In this paper, we develop a task-offloading
approach for delay-sensitive IoT applications in edge computing environments.
Unlike existing schemes, we model the task offloading problem as an economic
demand and supply model to achieve market balance. The proposed model avoids
under- and over-supply, ensuring the computational resources at edge servers
(supply) are allocated in a manner that best meets the processing and
computational needs of user devices (demand). Given the multi-agent nature of
task offloading involving users and service providers with different
preferences and objectives, we design a game-theoretic framework using a
Vickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions
and decision-making processes. Additionally, we develop an incentive mechanism
to encourage both parties to participate in the auction. The mechanism
maximizes user task offloading to edge servers and motivates edge servers to
share their computational resources, achieving profitability for both IoT users
and edge servers. Simulations demonstrate our method maximizes social welfare,
ensures truthfulness, maintains market balance, and provides latency guarantees
for delay-sensitive IoT applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: 本研究基于SAP云平台中约1,800台虚拟机管理程序和48,000台虚拟机的30天观测数据，分析了虚拟机调度与部署问题，揭示了资源争用、利用率不均和过度配置等关键挑战，并提出了改进调度算法的设计要求，同时公开了包含细粒度时间序列遥测数据的完整数据集，以支持未来大规模云基础设施的资源调度研究。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境中有效分配资源是核心挑战，尤其是在企业级云平台中。SAP作为全球最大的企业资源规划软件供应商，其云平台承载着大量关键业务工作负载，因此优化其虚拟机调度与资源利用率具有重要意义。通过真实世界的大规模数据识别当前调度策略中的低效问题，为设计更智能的资源管理机制提供依据。

Challenges: 研究发现存在多个调度问题：CPU资源争用超过40%，CPU就绪时间高达220秒，计算节点负载严重不均衡（最高CPU利用率达99%），以及普遍的资源过度配置（超过80%的虚拟机使用不到70%所分配的资源）。此外，如何在保障性能的同时提升资源利用率，是实际部署中的主要挑战。

Contributions: 1）提供了来自企业级、全虚拟化生产环境的大规模、细粒度时间序列遥测数据集，涵盖SAP S/4HANA和通用应用；2）识别并量化了当前VM调度中的多种低效现象；3）基于实证分析提出了新型调度与部署算法的设计需求；4）公开发布数据集以促进未来研究。

Results: 分析结果显示：存在严重的CPU资源争用（>40%）和就绪延迟（最高220秒）；主机间负载极不均衡，部分主机CPU利用率接近饱和（99%）；超过80%的虚拟机资源使用率低于其分配量的70%，表明普遍存在过度配置问题。这些结果揭示了当前调度策略在资源平衡与效率方面的不足。

Conclusion: 通过对SAP大规模生产环境的真实数据分析，本文揭示了现有虚拟机调度机制中的显著低效问题。研究结果强调了设计新型、数据驱动的调度与部署算法的必要性，以提升资源利用率、降低争用并实现负载均衡。公开的数据集将为后续研究提供重要支持。

Related Work: 已有研究多基于模拟或小规模测试环境分析资源调度问题，部分公开数据集缺乏企业级应用的代表性或时间分辨率。相比之下，本文的数据集在规模、真实性和细粒度方面具有独特优势，填补了企业级云工作负载实证研究的空白。

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [5] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: 本文提出了一种基于AMReX框架的高性能可压缩反应流求解器，针对多GPU环境进行了优化，解决了内存访问、计算负载不均和多GPU负载分配等关键性能瓶颈，在氢气-空气爆震和超声速横流喷射等典型燃烧问题中实现了2-5倍的性能提升，并在1-96个NVIDIA H100 GPU上表现出接近理想的弱扩展性。


<details>
  <summary>Details</summary>
Motivation: 高速化学反应流因存在多尺度特征且化学反应刚性强，计算挑战大；现有GPU求解器在内存管理、负载均衡及局部反应处理方面存在局限，亟需高效并行求解器以充分利用现代超算能力。

Challenges: 主要挑战包括：1）化学反应的强刚性导致计算耗时；2）反应区域高度局部化带来的负载不均衡；3）多GPU环境下自适应网格加密（AMR）的负载分配难题；4）GPU内存访问效率低下。

Contributions: 1）基于AMReX框架开发了面向多GPU的高性能可压缩反应流求解器；2）采用列主序存储优化内存访问模式；3）提出“块稀疏”积分策略以高效处理化学动力学计算；4）实现适用于AMR的矩阵化化学动力学方法与多GPU负载均衡方案。

Results: 在氢气-空气爆震和超声速横流喷射等算例中，相比初始GPU实现获得2-5倍性能提升；在1-96个NVIDIA H100 GPU上实现接近理想的弱扩展性；Roofline分析显示对流和化学反应模块的计算强度分别提升约10倍和4倍，显著提高了GPU资源利用率。

Conclusion: 本文提出的求解器有效克服了多尺度反应流在多GPU平台上的关键性能瓶颈，通过内存、算法和负载均衡的协同优化，实现了高可扩展性和高效能，为大规模燃烧模拟提供了有力工具。

Related Work: 相关工作主要集中在GPU加速的燃烧模拟、AMR框架（如AMReX、Chombo）的应用以及化学动力学的稀疏性利用等方面。本文在这些基础上进一步优化了多GPU扩展性和计算效率。

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [6] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: 本文介绍了SPACE卓越中心（SPACE-CoE）为应对天体物理、宇宙学和空间等离子体数值模拟在新一代加速器上的挑战，推动gPLUTO、OpenGadget3和iPIC3D三个旗舰代码在CINECA的Leonardo系统上的优化工作。通过性能分析工具，初步测试显示这三个代码在多达1,024个GPU上实现了高达80%的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了在现有和下一代加速器上实现大规模等离子体模拟，亟需对相关数值代码进行开发和重构。

Challenges: 主要挑战包括将复杂的科学计算代码适配到异构的高性能计算架构上，并确保其在单节点和多节点环境下的高效可扩展性。

Contributions: 本文提出了针对gPLUTO、OpenGadget3和iPIC3D三个代码的优化策略，并在Leonardo系统上展示了初步的性能分析和可扩展性结果，推动了科学应用向E级计算时代的过渡。

Results: 初步测试结果表明，三个代码在最多1,024个GPU上均实现了约80%的可扩展效率，表现出良好的并行性能。

Conclusion: 通过跨学科合作与系统级优化，gPLUTO、OpenGadget3和iPIC3D已初步适配于新一代加速器架构，为未来大规模等离子体模拟奠定了基础。

Related Work: 本文工作与高性能计算在天体物理和等离子体模拟中的应用密切相关，特别是针对大规模并行架构的科学代码优化研究。

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [7] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: 本文提出了一种名为CoMPSeT的开源工具，用于分析和比较多种多参与者会话类型（MPST）的特性，支持在浏览器中直接运行，有助于研究人员和教师理解MPST的语义与设计。


<details>
  <summary>Details</summary>
Motivation: 由于并发系统设计复杂，且现有的多参与者会话类型（MPST）变体繁多、特性各异，缺乏统一的理解和比较手段，因此需要一个工具来帮助研究者和教育者更清晰地掌握不同MPST的特点。

Challenges: 不同MPST变体具有各自独特的语义和特征，难以统一建模与比较；同时需要支持交互式动画展示和灵活组合特性，以增强理解与教学效果。

Contributions: 1) 提出CoMPSeT工具，支持多种MPST特性的组合与语义比较；2) 提供一组代表性MPST示例；3) 实现基于JavaScript的浏览器端执行，便于访问和使用。

Results: CoMPSeT成功实现了对多种MPST示例的建模、动画展示和语义比较，能够在浏览器中直观运行，验证了其在研究和教学中的实用性。

Conclusion: CoMPSeT为理解和比较不同MPST提供了有效工具，提升了对全局编排协议的可访问性，对研究和教学均有实际价值。

Related Work: 相关工作包括多种MPST理论模型及其语义扩展，如结构化通信、端点投影、死锁自由保证等，以及部分可视化工具，但缺乏支持跨变体比较与动画演示的综合平台。

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: SlowPoke是一种轻量级、硬件感知的框架，用于在多核芯片上实现高效的fail-slow故障检测，结合编译器插桩、实时轨迹压缩和拓扑感知排序算法，在极低内存开销下实现高检测准确率和低误报率。


<details>
  <summary>Details</summary>
Motivation: 多核架构中广泛存在的fail-slow故障严重影响性能，而传统分布式系统的检测方法因内存限制和无法跟踪硬件拓扑中的故障而难以适用。

Challenges: 在严格内存限制（仅数千字节）下实现实时故障监测；有效压缩检测轨迹；在复杂硬件拓扑中准确定位故障根源。

Contributions: 提出了SlowPoke框架，包含三方面创新：1）编译器辅助的低开销监控；2）实时轨迹压缩技术；3）拓扑感知的故障排序算法，实现精准根因定位。

Results: 在多种典型多核工作负载上评估显示，SlowPoke平均减少115.9倍的检测轨迹存储开销，平均检测准确率达86.77%，误报率仅为12.11%，且具有良好架构可扩展性。

Conclusion: SlowPoke在极低内存消耗下实现了高效、可扩展的片上fail-slow检测，适用于大规模多核系统部署。

Related Work: 本文借鉴了分布式系统中的fail-slow检测方法，但指出其在内存受限和硬件拓扑感知方面的不足，从而提出面向多核硬件的专用解决方案。

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>
